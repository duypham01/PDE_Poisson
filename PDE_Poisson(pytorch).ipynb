{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PDE_Poisson(pytorch).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMqnZYNZob4+oaDs4pa3FD4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duypham01/PDE_Poisson/blob/master/PDE_Poisson(pytorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJPZCBQUtcYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import grad\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "import itertools\n",
        "import time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK48bUkdt59J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PDENetLight(nn.Module):\n",
        "\n",
        "    # đọc paper để biết thêm chi tiết về tham số :))))\n",
        "    # hidden_sz = M trong paper\n",
        "    def __init__(self, dim_in, hidden_sz):\n",
        "        super().__init__()\n",
        "        self.hidden_sz = hidden_sz\n",
        "        self.W1 = nn.Parameter(torch.Tensor(hidden_sz, dim_in))\n",
        "        self.b1 = nn.Parameter(torch.Tensor(hidden_sz, 1))\n",
        "\n",
        "        # 1st layer\n",
        "        self.Uz1 = torch.nn.Parameter(torch.Tensor(hidden_sz, dim_in))\n",
        "        self.Wz1 = torch.nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
        "        self.bz1 = torch.nn.Parameter(torch.Tensor(hidden_sz, 1))\n",
        "\n",
        "        self.Ug1 = torch.nn.Parameter(torch.Tensor(hidden_sz, dim_in))\n",
        "        self.Wg1 = torch.nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
        "        self.bg1 = torch.nn.Parameter(torch.Tensor(hidden_sz, 1))\n",
        "\n",
        "        self.Ur1 = torch.nn.Parameter(torch.Tensor(hidden_sz, dim_in))\n",
        "        self.Wr1 = torch.nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
        "        self.br1 = torch.nn.Parameter(torch.Tensor(hidden_sz, 1))\n",
        "\n",
        "        self.Uh1 = torch.nn.Parameter(torch.Tensor(hidden_sz, dim_in))\n",
        "        self.Wh1 = torch.nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
        "        self.bh1 = torch.nn.Parameter(torch.Tensor(hidden_sz, 1))\n",
        "\n",
        "        # 2nd layer\n",
        "\n",
        "        self.Uz2 = torch.nn.Parameter(torch.Tensor(hidden_sz, dim_in))\n",
        "        self.Wz2 = torch.nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
        "        self.bz2 = torch.nn.Parameter(torch.Tensor(hidden_sz, 1))\n",
        "\n",
        "        self.Ug2 = torch.nn.Parameter(torch.Tensor(hidden_sz, dim_in))\n",
        "        self.Wg2 = torch.nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
        "        self.bg2 = torch.nn.Parameter(torch.Tensor(hidden_sz, 1))\n",
        "\n",
        "        self.Ur2 = torch.nn.Parameter(torch.Tensor(hidden_sz, dim_in))\n",
        "        self.Wr2 = torch.nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
        "        self.br2 = torch.nn.Parameter(torch.Tensor(hidden_sz, 1))\n",
        "\n",
        "        self.Uh2 = torch.nn.Parameter(torch.Tensor(hidden_sz, dim_in))\n",
        "        self.Wh2 = torch.nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
        "        self.bh2 = torch.nn.Parameter(torch.Tensor(hidden_sz, 1))\n",
        "\n",
        "        # 3rd layer\n",
        "\n",
        "\n",
        "        # output layer\n",
        "        self.W = torch.nn.Parameter(torch.Tensor(1, hidden_sz))\n",
        "        self.b = torch.nn.Parameter(torch.Tensor(1, 1))\n",
        "\n",
        "    def init_weights(self):\n",
        "        for p in self.parameters():\n",
        "            nn.init.xavier_uniform_(p.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "        Ones = torch.ones(self.hidden_sz, 1)\n",
        "\n",
        "        S1 = torch.tanh(torch.mm(self.W1, x) + self.b1)\n",
        "\n",
        "        # 1st layer\n",
        "        Z1 = torch.tanh(torch.mm(self.Uz1, x) + torch.mm(self.Wz1, S1) + self.bz1)\n",
        "        G1 = torch.tanh(torch.mm(self.Ug1, x) + torch.mm(self.Wg1, S1) + self.bg1)\n",
        "        R1 = torch.tanh(torch.mm(self.Ur1, x) + torch.mm(self.Wr1, S1) + self.br1)\n",
        "        # * is element - wise multiplication\n",
        "        H1 = torch.tanh(torch.mm(self.Uh1, x) + torch.mm(self.Wh1, S1 * R1) + self.bh1)\n",
        "\n",
        "        S2 = (Ones - G1) * H1 + Z1 * S1\n",
        "\n",
        "        # 2nd layer\n",
        "        Z2 = torch.tanh(torch.mm(self.Uz2, x) + torch.mm(self.Wz2, S2) + self.bz2)\n",
        "        G2 = torch.tanh(torch.mm(self.Ug2, x) + torch.mm(self.Wg2, S2) + self.bg2)\n",
        "        R2 = torch.tanh(torch.mm(self.Ur2, x) + torch.mm(self.Wr2, S2) + self.br2)\n",
        "        H2 = torch.tanh(torch.mm(self.Uh2, x) + torch.mm(self.Wh2, S2 * R2) + self.bh2)\n",
        "\n",
        "        S3 = (Ones - G2) * H2 + Z2 * S2\n",
        "\n",
        "\n",
        "        # output layer\n",
        "        out = torch.mm(self.W, S3) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def save(self, file):\n",
        "        torch.save(self, file)\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    total_param = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            num_param = np.prod(param.size())\n",
        "            if param.dim() > 1:\n",
        "                print(name, ':', 'x'.join(str(x) for x in list(param.size())), '=', num_param)\n",
        "            else:\n",
        "                print(name, ':', num_param)\n",
        "            total_param += num_param\n",
        "    return total_param\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAtQGKCMtp-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def laplacian(f, input_vector):\n",
        "    gradient = grad(f, input_vector, create_graph=True)[0]\n",
        "    ux = gradient.take(torch.tensor([0]))\n",
        "    uxx = grad(ux, input_vector, create_graph=True)[0].take(torch.tensor([0]))\n",
        "    uy = gradient.take(torch.tensor([1]))\n",
        "    uyy = grad(uy, input_vector, create_graph=True)[0].take(torch.tensor([1]))\n",
        "    return uxx + uyy\n",
        "\n",
        "\n",
        "def boundary_condition(input):\n",
        "    return input.take(torch.tensor([1])) + input.take(torch.tensor([1]))**2\n",
        "\n",
        "\n",
        "# -laplace(u) = f\n",
        "def right_hand_side(input):\n",
        "    return 0\n",
        "\n",
        "\n",
        "def exact_solution(spatial_time_point):\n",
        "    return 3 + spatial_time_point.take(torch.tensor([1])) +0.5*(spatial_time_point.take(torch.tensor([1]))**2 - spatial_time_point.take(torch.tensor([0]))**2)\n",
        "\n",
        "\n",
        "def exact_solution_scalar_value(x, y):\n",
        "    return 3 + y + 0.5*(y**2 - x**2)\n",
        "\n",
        "\n",
        "def random_data_points(batch_size):\n",
        "    Omegapoints = []\n",
        "    boundary_points = []\n",
        "\n",
        "    radius = math.sqrt(6)\n",
        "    for i in range(batch_size):\n",
        "        phi = 2 * math.pi * random.random()\n",
        "        r = radius * random.random()\n",
        "        Omegapoints.append([r*math.cos(phi), r*math.sin(phi)])\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        phi = 2 * math.pi * random.random()\n",
        "        boundary_points.append([radius * math.cos(phi), radius * math.sin(phi)])\n",
        "    return Omegapoints, boundary_points\n",
        "\n",
        "\n",
        "def batch_loss(net, datapoints):\n",
        "    G1 = G2 = 0\n",
        "    Omegapoints, boundary_points = datapoints\n",
        "    for Omegapoint in Omegapoints:\n",
        "        Omegapoint_input = Variable(torch.Tensor(Omegapoint).resize_(2, 1), requires_grad=True)\n",
        "        Omegapoint_output = net(Omegapoint_input)\n",
        "        G1 += (- laplacian(Omegapoint_output, Omegapoint_input) - right_hand_side(Omegapoint_input)) ** 2\n",
        "\n",
        "    for boundary_point in boundary_points:\n",
        "        boundary_point_input = Variable(torch.Tensor(boundary_point).resize_(2, 1), requires_grad=True)\n",
        "        boundary_point_output = net(boundary_point_input)\n",
        "        G2 += (boundary_point_output - boundary_condition(boundary_point_input))**2\n",
        "\n",
        "    G1 = G1 / len(Omegapoints)\n",
        "    G2 = G2 / len(boundary_points)\n",
        "    return G1 + G2\n",
        "\n",
        "\n",
        "def plot_estimation_and_exact_solution(file_name):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    x = y = np.arange(0, 3, 0.05)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    condition = X ** 2 + Y ** 2 <= 6\n",
        "    zs = np.array([net(torch.tensor([x, y]).resize_(2,1)).item() for x, y in zip(np.ravel(X), np.ravel(Y))])\n",
        "    Z = zs.reshape(X.shape)\n",
        "\n",
        "    zs = np.array([exact_solution_scalar_value(x, y) for x, y in zip(np.ravel(X), np.ravel(Y))])\n",
        "    Z1 = zs.reshape(X.shape)\n",
        "\n",
        "    ax.plot_surface(X, Y, Z, color='red')\n",
        "    ax.plot_surface(X, Y, Z1, color='blue')\n",
        "\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_zlabel('z')\n",
        "\n",
        "    # plt.show()\n",
        "    plt.savefig(file_name)\n",
        "    plt.close(\"all\")\n",
        "\n",
        "\n",
        "def random_points_in_circle():\n",
        "    points = []\n",
        "    radius = math.sqrt(6)\n",
        "    for i in range(2000):\n",
        "        phi = 2 * math.pi * random.random()\n",
        "        r = radius * random.random()\n",
        "        points.append([r*math.cos(phi), r*math.sin(phi)])\n",
        "    return points"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DuAKxfQtwiB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7981e3f7-5d16-4459-bb5c-458899d459f0"
      },
      "source": [
        "net = PDENetLight(2, 10)\n",
        "net.init_weights()\n",
        "\n",
        "# plot_estimation_and_exact_solution('./result/testnoBatch/Initial.png')\n",
        "l2_errors = []\n",
        "losses = []\n",
        "\n",
        "# net = torch.load('./model3')\n",
        "torch.set_printoptions(precision=10)\n",
        "learning_rate = 0.001\n",
        "iterations_count = 0\n",
        "start_training = time.time()\n",
        "for i in range(200):\n",
        "    print('Iteration number: ' + str(i + 1))\n",
        "    sample = random_data_points(1000)  # sample space time point\n",
        "    start_ite = time.time()\n",
        "    # for j in range(4):\n",
        "    #     # print('Batch iteration number: ' + str(j + 1))\n",
        "    #     net.zero_grad()\n",
        "    #     square_error = batch_loss(net, sample)  # calculate square error loss\n",
        "    #     square_error.backward()  # calculate gradient of square loss w.r.t the parameters\n",
        "    #     print('Batch loss: ' + str(square_error))\n",
        "    #     for param in net.parameters():\n",
        "    #         param.data -= learning_rate*param.grad.data\n",
        "    #     if j == 0:\n",
        "    #         losses.append(square_error.item())\n",
        "    net.zero_grad()\n",
        "    square_error = batch_loss(net, sample)  # calculate square error loss\n",
        "    square_error.backward()  # calculate gradient of square loss w.r.t the parameters\n",
        "    print('Batch loss: ' + str(square_error))\n",
        "    for param in net.parameters():\n",
        "        param.data -= learning_rate * param.grad.data\n",
        "    losses.append(square_error.item())\n",
        "    end_ite = time.time()\n",
        "    ite_time = end_ite - start_ite\n",
        "    total_time = end_ite - start_training\n",
        "    print('This iteration took ' + str(ite_time) + ' seconds')\n",
        "    print('Total training time elapsed ' + str(total_time/60) + ' minutes')\n",
        "    L2_error = 0\n",
        "    for point in random_points_in_circle():\n",
        "        point_input = torch.Tensor(point).resize_(2,1)\n",
        "        L2_error += (net(point_input) - exact_solution(point_input))**2\n",
        "    L2_error /= 121\n",
        "    l2_errors.append(L2_error.item())\n",
        "\n",
        "    print('L2error = ' + str(L2_error))\n",
        "    # print('Loss function = ' + str(square_error))\n",
        "\n",
        "    print('Finished iteration number ' + str(i + 1) + ', saving model')\n",
        "    model_path = './result/testnoBatch/model' + str(i+1) + 'th_ite'\n",
        "   # net.save(model_path)\n",
        "    # fig_path = './result/testnoBatch/' + str(i+1) + 'th_ite.png'\n",
        "    # plot_estimation_and_exact_solution(fig_path)\n",
        "\n",
        "print('Plotting and saving convergence history')\n",
        "epochs = [i + 1 for i in range(200)]\n",
        "plt.xticks(np.arange(0, 201, 20))\n",
        "\n",
        "plt.figure(0)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(epochs, losses, color = 'blue')\n",
        "# plt.savefig('./result/testnoBatch/loss.png')\n",
        "\n",
        "# with open('./result/testnoBatch/batchloss.txt', 'w+') as f:\n",
        "#     for loss in losses:\n",
        "#         f.write(str(loss) + '\\n')\n",
        "\n",
        "plt.figure(1)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('L2error')\n",
        "\n",
        "plt.plot(epochs, l2_errors, color = 'blue')\n",
        "# plt.savefig('./result/testnoBatch/l2error.png')\n",
        "# with open('./result/testnoBatch/L2error.txt', 'w+') as f:\n",
        "#     for l2_error in l2_errors:\n",
        "#         f.write(str(l2_error) + '\\n')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration number: 1\n",
            "Batch loss: tensor([[23.6359329224]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.556180238723755 seconds\n",
            "Total training time elapsed 0.2093027393023173 minutes\n",
            "L2error = tensor([[115.8604049683]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 1, saving model\n",
            "Iteration number: 2\n",
            "Batch loss: tensor([[21.8026390076]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.647074699401855 seconds\n",
            "Total training time elapsed 0.4206802447636922 minutes\n",
            "L2error = tensor([[111.9764022827]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 2, saving model\n",
            "Iteration number: 3\n",
            "Batch loss: tensor([[20.2805595398]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.675792217254639 seconds\n",
            "Total training time elapsed 0.6323347051938375 minutes\n",
            "L2error = tensor([[119.1888122559]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 3, saving model\n",
            "Iteration number: 4\n",
            "Batch loss: tensor([[19.4270267487]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.121941804885864 seconds\n",
            "Total training time elapsed 0.8515107870101929 minutes\n",
            "L2error = tensor([[106.0113525391]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 4, saving model\n",
            "Iteration number: 5\n",
            "Batch loss: tensor([[18.2932395935]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.976531744003296 seconds\n",
            "Total training time elapsed 1.068983507156372 minutes\n",
            "L2error = tensor([[103.4821319580]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 5, saving model\n",
            "Iteration number: 6\n",
            "Batch loss: tensor([[17.4333648682]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.75651741027832 seconds\n",
            "Total training time elapsed 1.2819881280263266 minutes\n",
            "L2error = tensor([[104.2557296753]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 6, saving model\n",
            "Iteration number: 7\n",
            "Batch loss: tensor([[15.2627124786]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.888864278793335 seconds\n",
            "Total training time elapsed 1.497446024417877 minutes\n",
            "L2error = tensor([[110.4278106689]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 7, saving model\n",
            "Iteration number: 8\n",
            "Batch loss: tensor([[14.2703781128]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.965259075164795 seconds\n",
            "Total training time elapsed 1.7142146070798239 minutes\n",
            "L2error = tensor([[92.3919601440]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 8, saving model\n",
            "Iteration number: 9\n",
            "Batch loss: tensor([[12.7501020432]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.273721694946289 seconds\n",
            "Total training time elapsed 1.9360618789990742 minutes\n",
            "L2error = tensor([[95.8652420044]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 9, saving model\n",
            "Iteration number: 10\n",
            "Batch loss: tensor([[14.1473064423]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.357161521911621 seconds\n",
            "Total training time elapsed 2.159894295533498 minutes\n",
            "L2error = tensor([[92.0128555298]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 10, saving model\n",
            "Iteration number: 11\n",
            "Batch loss: tensor([[12.4276142120]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.099884510040283 seconds\n",
            "Total training time elapsed 2.3789599736531577 minutes\n",
            "L2error = tensor([[86.2829208374]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 11, saving model\n",
            "Iteration number: 12\n",
            "Batch loss: tensor([[12.6467514038]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.102558374404907 seconds\n",
            "Total training time elapsed 2.597833057244619 minutes\n",
            "L2error = tensor([[83.3040237427]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 12, saving model\n",
            "Iteration number: 13\n",
            "Batch loss: tensor([[12.3249216080]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.949926614761353 seconds\n",
            "Total training time elapsed 2.814575672149658 minutes\n",
            "L2error = tensor([[87.5913467407]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 13, saving model\n",
            "Iteration number: 14\n",
            "Batch loss: tensor([[11.4107894897]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.57022476196289 seconds\n",
            "Total training time elapsed 3.041230348745982 minutes\n",
            "L2error = tensor([[88.7171173096]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 14, saving model\n",
            "Iteration number: 15\n",
            "Batch loss: tensor([[11.3806762695]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.719260931015015 seconds\n",
            "Total training time elapsed 3.2538121978441876 minutes\n",
            "L2error = tensor([[83.6621246338]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 15, saving model\n",
            "Iteration number: 16\n",
            "Batch loss: tensor([[11.1178236008]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.851223468780518 seconds\n",
            "Total training time elapsed 3.468497025966644 minutes\n",
            "L2error = tensor([[77.6762237549]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 16, saving model\n",
            "Iteration number: 17\n",
            "Batch loss: tensor([[10.6297636032]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.808904886245728 seconds\n",
            "Total training time elapsed 3.682969307899475 minutes\n",
            "L2error = tensor([[81.9849014282]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 17, saving model\n",
            "Iteration number: 18\n",
            "Batch loss: tensor([[10.2777624130]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.459793329238892 seconds\n",
            "Total training time elapsed 3.9079411069552106 minutes\n",
            "L2error = tensor([[76.4687347412]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 18, saving model\n",
            "Iteration number: 19\n",
            "Batch loss: tensor([[10.0274772644]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.965108156204224 seconds\n",
            "Total training time elapsed 4.125002233187358 minutes\n",
            "L2error = tensor([[78.5328445435]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 19, saving model\n",
            "Iteration number: 20\n",
            "Batch loss: tensor([[9.4743690491]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.75632381439209 seconds\n",
            "Total training time elapsed 4.3382386247317 minutes\n",
            "L2error = tensor([[73.0292358398]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 20, saving model\n",
            "Iteration number: 21\n",
            "Batch loss: tensor([[9.4204874039]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.079393148422241 seconds\n",
            "Total training time elapsed 4.556680154800415 minutes\n",
            "L2error = tensor([[69.1807785034]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 21, saving model\n",
            "Iteration number: 22\n",
            "Batch loss: tensor([[8.2448606491]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.838013648986816 seconds\n",
            "Total training time elapsed 4.771633664766948 minutes\n",
            "L2error = tensor([[70.9793548584]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 22, saving model\n",
            "Iteration number: 23\n",
            "Batch loss: tensor([[8.5736398697]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.858943462371826 seconds\n",
            "Total training time elapsed 4.986563507715861 minutes\n",
            "L2error = tensor([[68.1708755493]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 23, saving model\n",
            "Iteration number: 24\n",
            "Batch loss: tensor([[8.2470140457]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.800260066986084 seconds\n",
            "Total training time elapsed 5.200529257456462 minutes\n",
            "L2error = tensor([[67.5668563843]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 24, saving model\n",
            "Iteration number: 25\n",
            "Batch loss: tensor([[7.8910608292]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.359870910644531 seconds\n",
            "Total training time elapsed 5.424439561367035 minutes\n",
            "L2error = tensor([[65.1912384033]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 25, saving model\n",
            "Iteration number: 26\n",
            "Batch loss: tensor([[8.0255079269]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.823335647583008 seconds\n",
            "Total training time elapsed 5.63863562742869 minutes\n",
            "L2error = tensor([[60.5312576294]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 26, saving model\n",
            "Iteration number: 27\n",
            "Batch loss: tensor([[7.7537927628]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.092609405517578 seconds\n",
            "Total training time elapsed 5.857422773043314 minutes\n",
            "L2error = tensor([[62.5585556030]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 27, saving model\n",
            "Iteration number: 28\n",
            "Batch loss: tensor([[7.6071820259]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.519997119903564 seconds\n",
            "Total training time elapsed 6.083284215132395 minutes\n",
            "L2error = tensor([[60.5823402405]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 28, saving model\n",
            "Iteration number: 29\n",
            "Batch loss: tensor([[7.6151337624]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.056490898132324 seconds\n",
            "Total training time elapsed 6.301519870758057 minutes\n",
            "L2error = tensor([[61.8618125916]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 29, saving model\n",
            "Iteration number: 30\n",
            "Batch loss: tensor([[7.6566848755]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.988269329071045 seconds\n",
            "Total training time elapsed 6.5184985836346945 minutes\n",
            "L2error = tensor([[60.7407531738]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 30, saving model\n",
            "Iteration number: 31\n",
            "Batch loss: tensor([[6.7157821655]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.267168283462524 seconds\n",
            "Total training time elapsed 6.740332579612732 minutes\n",
            "L2error = tensor([[56.3872222900]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 31, saving model\n",
            "Iteration number: 32\n",
            "Batch loss: tensor([[6.9626970291]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.822186470031738 seconds\n",
            "Total training time elapsed 6.955804224809011 minutes\n",
            "L2error = tensor([[57.4039802551]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 32, saving model\n",
            "Iteration number: 33\n",
            "Batch loss: tensor([[6.6356763840]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.775800466537476 seconds\n",
            "Total training time elapsed 7.186022357145945 minutes\n",
            "L2error = tensor([[56.4571113586]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 33, saving model\n",
            "Iteration number: 34\n",
            "Batch loss: tensor([[6.4538393021]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.94430947303772 seconds\n",
            "Total training time elapsed 7.403604578971863 minutes\n",
            "L2error = tensor([[56.2376403809]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 34, saving model\n",
            "Iteration number: 35\n",
            "Batch loss: tensor([[6.3166885376]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.936046600341797 seconds\n",
            "Total training time elapsed 7.619904494285583 minutes\n",
            "L2error = tensor([[54.2949447632]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 35, saving model\n",
            "Iteration number: 36\n",
            "Batch loss: tensor([[6.4842929840]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.011616706848145 seconds\n",
            "Total training time elapsed 7.837458372116089 minutes\n",
            "L2error = tensor([[51.8437042236]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 36, saving model\n",
            "Iteration number: 37\n",
            "Batch loss: tensor([[6.2785177231]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.044038534164429 seconds\n",
            "Total training time elapsed 8.055589671929678 minutes\n",
            "L2error = tensor([[51.3632087708]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 37, saving model\n",
            "Iteration number: 38\n",
            "Batch loss: tensor([[6.2446746826]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.174450635910034 seconds\n",
            "Total training time elapsed 8.276146546999614 minutes\n",
            "L2error = tensor([[51.1289367676]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 38, saving model\n",
            "Iteration number: 39\n",
            "Batch loss: tensor([[5.9555263519]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.334437847137451 seconds\n",
            "Total training time elapsed 8.499075202147166 minutes\n",
            "L2error = tensor([[51.7014541626]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 39, saving model\n",
            "Iteration number: 40\n",
            "Batch loss: tensor([[5.7685761452]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.438578844070435 seconds\n",
            "Total training time elapsed 8.723608736197153 minutes\n",
            "L2error = tensor([[48.9517326355]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 40, saving model\n",
            "Iteration number: 41\n",
            "Batch loss: tensor([[6.3225483894]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.804771661758423 seconds\n",
            "Total training time elapsed 8.938087932268779 minutes\n",
            "L2error = tensor([[49.0376586914]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 41, saving model\n",
            "Iteration number: 42\n",
            "Batch loss: tensor([[5.8556509018]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.015035629272461 seconds\n",
            "Total training time elapsed 9.160786279042561 minutes\n",
            "L2error = tensor([[49.9593200684]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 42, saving model\n",
            "Iteration number: 43\n",
            "Batch loss: tensor([[5.7585172653]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.178620100021362 seconds\n",
            "Total training time elapsed 9.3812242547671 minutes\n",
            "L2error = tensor([[46.9329147339]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 43, saving model\n",
            "Iteration number: 44\n",
            "Batch loss: tensor([[5.1099038124]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.273526430130005 seconds\n",
            "Total training time elapsed 9.603070402145386 minutes\n",
            "L2error = tensor([[44.7730903625]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 44, saving model\n",
            "Iteration number: 45\n",
            "Batch loss: tensor([[5.7027235031]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.063240051269531 seconds\n",
            "Total training time elapsed 9.822208535671233 minutes\n",
            "L2error = tensor([[43.7431068420]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 45, saving model\n",
            "Iteration number: 46\n",
            "Batch loss: tensor([[5.3082537651]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.742763757705688 seconds\n",
            "Total training time elapsed 10.035320154825847 minutes\n",
            "L2error = tensor([[45.2947273254]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 46, saving model\n",
            "Iteration number: 47\n",
            "Batch loss: tensor([[5.2205905914]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.05159306526184 seconds\n",
            "Total training time elapsed 10.253449519475302 minutes\n",
            "L2error = tensor([[45.1399116516]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 47, saving model\n",
            "Iteration number: 48\n",
            "Batch loss: tensor([[4.8798656464]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.180369138717651 seconds\n",
            "Total training time elapsed 10.475064281622569 minutes\n",
            "L2error = tensor([[43.3665695190]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 48, saving model\n",
            "Iteration number: 49\n",
            "Batch loss: tensor([[5.2916860580]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.924603700637817 seconds\n",
            "Total training time elapsed 10.691439151763916 minutes\n",
            "L2error = tensor([[43.2718048096]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 49, saving model\n",
            "Iteration number: 50\n",
            "Batch loss: tensor([[5.1911454201]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.656055212020874 seconds\n",
            "Total training time elapsed 10.925240055720012 minutes\n",
            "L2error = tensor([[42.3307228088]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 50, saving model\n",
            "Iteration number: 51\n",
            "Batch loss: tensor([[4.9804949760]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.889212846755981 seconds\n",
            "Total training time elapsed 11.14089101155599 minutes\n",
            "L2error = tensor([[40.9853324890]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 51, saving model\n",
            "Iteration number: 52\n",
            "Batch loss: tensor([[4.5615396500]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.83912706375122 seconds\n",
            "Total training time elapsed 11.356051039695739 minutes\n",
            "L2error = tensor([[39.9495964050]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 52, saving model\n",
            "Iteration number: 53\n",
            "Batch loss: tensor([[4.8042421341]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.81596565246582 seconds\n",
            "Total training time elapsed 11.570391043027241 minutes\n",
            "L2error = tensor([[39.6016540527]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 53, saving model\n",
            "Iteration number: 54\n",
            "Batch loss: tensor([[4.8307137489]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.236392736434937 seconds\n",
            "Total training time elapsed 11.792035563786824 minutes\n",
            "L2error = tensor([[37.8057441711]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 54, saving model\n",
            "Iteration number: 55\n",
            "L2error = tensor([[36.7254219055]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 55, saving model\n",
            "Iteration number: 56\n",
            "Batch loss: tensor([[4.5935630798]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.966625690460205 seconds\n",
            "Total training time elapsed 12.222534926732381 minutes\n",
            "L2error = tensor([[37.7617721558]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 56, saving model\n",
            "Iteration number: 57\n",
            "Batch loss: tensor([[4.4605569839]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.961365938186646 seconds\n",
            "Total training time elapsed 12.440600927670797 minutes\n",
            "L2error = tensor([[36.3202171326]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 57, saving model\n",
            "Iteration number: 58\n",
            "Batch loss: tensor([[4.5898356438]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.006794452667236 seconds\n",
            "Total training time elapsed 12.65873090426127 minutes\n",
            "L2error = tensor([[35.8659477234]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 58, saving model\n",
            "Iteration number: 59\n",
            "Batch loss: tensor([[4.3985166550]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.745432615280151 seconds\n",
            "Total training time elapsed 12.874897674719493 minutes\n",
            "L2error = tensor([[35.0381965637]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 59, saving model\n",
            "Iteration number: 60\n",
            "Batch loss: tensor([[4.1647396088]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.048157691955566 seconds\n",
            "Total training time elapsed 13.092938955624899 minutes\n",
            "L2error = tensor([[34.6796340942]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 60, saving model\n",
            "Iteration number: 61\n",
            "Batch loss: tensor([[4.2907562256]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.320775985717773 seconds\n",
            "Total training time elapsed 13.315990622838338 minutes\n",
            "L2error = tensor([[34.0989761353]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 61, saving model\n",
            "Iteration number: 62\n",
            "Batch loss: tensor([[3.9722232819]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.692049264907837 seconds\n",
            "Total training time elapsed 13.528255808353425 minutes\n",
            "L2error = tensor([[32.9269409180]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 62, saving model\n",
            "Iteration number: 63\n",
            "Batch loss: tensor([[3.9671301842]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.930638790130615 seconds\n",
            "Total training time elapsed 13.744300242265066 minutes\n",
            "L2error = tensor([[33.1286430359]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 63, saving model\n",
            "Iteration number: 64\n",
            "Batch loss: tensor([[4.0231847763]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.015805959701538 seconds\n",
            "Total training time elapsed 13.961737581094106 minutes\n",
            "L2error = tensor([[32.5114860535]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 64, saving model\n",
            "Iteration number: 65\n",
            "Batch loss: tensor([[3.8244266510]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.12600302696228 seconds\n",
            "Total training time elapsed 14.181434945265453 minutes\n",
            "L2error = tensor([[31.4958152771]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 65, saving model\n",
            "Iteration number: 66\n",
            "Batch loss: tensor([[3.7070827484]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.810047388076782 seconds\n",
            "Total training time elapsed 14.395576377709707 minutes\n",
            "L2error = tensor([[30.2606792450]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 66, saving model\n",
            "Iteration number: 67\n",
            "Batch loss: tensor([[3.5004954338]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.78394365310669 seconds\n",
            "Total training time elapsed 14.609268156687419 minutes\n",
            "L2error = tensor([[30.3988342285]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 67, saving model\n",
            "Iteration number: 68\n",
            "Batch loss: tensor([[3.6588597298]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.97880744934082 seconds\n",
            "Total training time elapsed 14.826596466700236 minutes\n",
            "L2error = tensor([[29.9785785675]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 68, saving model\n",
            "Iteration number: 69\n",
            "Batch loss: tensor([[3.8109369278]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.996828079223633 seconds\n",
            "Total training time elapsed 15.044006939729055 minutes\n",
            "L2error = tensor([[30.5551738739]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 69, saving model\n",
            "Iteration number: 70\n",
            "Batch loss: tensor([[3.8684508801]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.26326060295105 seconds\n",
            "Total training time elapsed 15.26682869195938 minutes\n",
            "L2error = tensor([[28.6955280304]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 70, saving model\n",
            "Iteration number: 71\n",
            "Batch loss: tensor([[3.7764692307]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.967381477355957 seconds\n",
            "Total training time elapsed 15.483580807844797 minutes\n",
            "L2error = tensor([[27.9837017059]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 71, saving model\n",
            "Iteration number: 72\n",
            "Batch loss: tensor([[3.6573870182]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.966719388961792 seconds\n",
            "Total training time elapsed 15.700744048754375 minutes\n",
            "L2error = tensor([[27.6966266632]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 72, saving model\n",
            "Iteration number: 73\n",
            "Batch loss: tensor([[3.4288039207]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.129782676696777 seconds\n",
            "Total training time elapsed 15.920454756418865 minutes\n",
            "L2error = tensor([[28.4090328217]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 73, saving model\n",
            "Iteration number: 74\n",
            "Batch loss: tensor([[3.5412249565]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.888699293136597 seconds\n",
            "Total training time elapsed 16.13686287800471 minutes\n",
            "L2error = tensor([[28.3817920685]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 74, saving model\n",
            "Iteration number: 75\n",
            "Batch loss: tensor([[3.3507308960]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.812997817993164 seconds\n",
            "Total training time elapsed 16.367521556218467 minutes\n",
            "L2error = tensor([[26.8904476166]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 75, saving model\n",
            "Iteration number: 76\n",
            "Batch loss: tensor([[3.3416681290]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.967394828796387 seconds\n",
            "Total training time elapsed 16.58411708275477 minutes\n",
            "L2error = tensor([[25.8277206421]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 76, saving model\n",
            "Iteration number: 77\n",
            "Batch loss: tensor([[3.4323506355]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.97970962524414 seconds\n",
            "Total training time elapsed 16.800987362861633 minutes\n",
            "L2error = tensor([[25.6328468323]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 77, saving model\n",
            "Iteration number: 78\n",
            "Batch loss: tensor([[3.4204857349]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.10512638092041 seconds\n",
            "Total training time elapsed 17.020213989416757 minutes\n",
            "L2error = tensor([[25.2721710205]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 78, saving model\n",
            "Iteration number: 79\n",
            "Batch loss: tensor([[3.2706325054]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.241422414779663 seconds\n",
            "Total training time elapsed 17.241401811440785 minutes\n",
            "L2error = tensor([[24.7806949615]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 79, saving model\n",
            "Iteration number: 80\n",
            "Batch loss: tensor([[3.3642671108]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.197635889053345 seconds\n",
            "Total training time elapsed 17.462078201770783 minutes\n",
            "L2error = tensor([[25.1470794678]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 80, saving model\n",
            "Iteration number: 81\n",
            "Batch loss: tensor([[3.1536931992]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.848966836929321 seconds\n",
            "Total training time elapsed 17.682338670889536 minutes\n",
            "L2error = tensor([[23.4911384583]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 81, saving model\n",
            "Iteration number: 82\n",
            "Batch loss: tensor([[3.0547907352]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.133834838867188 seconds\n",
            "Total training time elapsed 17.901692275206248 minutes\n",
            "L2error = tensor([[23.5496654510]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 82, saving model\n",
            "Iteration number: 83\n",
            "Batch loss: tensor([[3.2985024452]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.883004188537598 seconds\n",
            "Total training time elapsed 18.11704020102819 minutes\n",
            "L2error = tensor([[23.1622600555]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 83, saving model\n",
            "Iteration number: 84\n",
            "Batch loss: tensor([[3.1482005119]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.730869054794312 seconds\n",
            "Total training time elapsed 18.329682211081188 minutes\n",
            "L2error = tensor([[22.7314929962]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 84, saving model\n",
            "Iteration number: 85\n",
            "Batch loss: tensor([[3.0226340294]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.089310884475708 seconds\n",
            "Total training time elapsed 18.54879173437754 minutes\n",
            "L2error = tensor([[21.9338932037]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 85, saving model\n",
            "Iteration number: 86\n",
            "Batch loss: tensor([[3.0563540459]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.253118991851807 seconds\n",
            "Total training time elapsed 18.770104912916818 minutes\n",
            "L2error = tensor([[21.8482398987]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 86, saving model\n",
            "Iteration number: 87\n",
            "Batch loss: tensor([[2.9832317829]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.804975748062134 seconds\n",
            "Total training time elapsed 18.986283532778423 minutes\n",
            "L2error = tensor([[21.9237480164]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 87, saving model\n",
            "Iteration number: 88\n",
            "Batch loss: tensor([[2.9637799263]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.023855447769165 seconds\n",
            "Total training time elapsed 19.203868238131204 minutes\n",
            "L2error = tensor([[21.6905574799]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 88, saving model\n",
            "Iteration number: 89\n",
            "Batch loss: tensor([[2.9330124855]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.043970584869385 seconds\n",
            "Total training time elapsed 19.422226274013518 minutes\n",
            "L2error = tensor([[20.5673847198]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 89, saving model\n",
            "Iteration number: 90\n",
            "Batch loss: tensor([[2.9257421494]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.204497575759888 seconds\n",
            "Total training time elapsed 19.64287398258845 minutes\n",
            "L2error = tensor([[20.6386280060]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 90, saving model\n",
            "Iteration number: 91\n",
            "Batch loss: tensor([[2.7584080696]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.098960638046265 seconds\n",
            "Total training time elapsed 19.862066304683687 minutes\n",
            "L2error = tensor([[20.5207042694]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 91, saving model\n",
            "Iteration number: 92\n",
            "Batch loss: tensor([[2.8995318413]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.123464345932007 seconds\n",
            "Total training time elapsed 20.08530889749527 minutes\n",
            "L2error = tensor([[19.6939010620]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 92, saving model\n",
            "Iteration number: 93\n",
            "Batch loss: tensor([[2.7050027847]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.023831129074097 seconds\n",
            "Total training time elapsed 20.303190382321677 minutes\n",
            "L2error = tensor([[19.4509963989]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 93, saving model\n",
            "Iteration number: 94\n",
            "Batch loss: tensor([[2.6000490189]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.379355907440186 seconds\n",
            "Total training time elapsed 20.529743667443594 minutes\n",
            "L2error = tensor([[19.0123806000]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 94, saving model\n",
            "Iteration number: 95\n",
            "Batch loss: tensor([[2.7601590157]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.7999906539917 seconds\n",
            "Total training time elapsed 20.74371072848638 minutes\n",
            "L2error = tensor([[19.6913166046]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 95, saving model\n",
            "Iteration number: 96\n",
            "Batch loss: tensor([[2.7333912849]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.203469276428223 seconds\n",
            "Total training time elapsed 20.964619588851928 minutes\n",
            "L2error = tensor([[18.6606445312]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 96, saving model\n",
            "Iteration number: 97\n",
            "Batch loss: tensor([[2.7583000660]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.237565517425537 seconds\n",
            "Total training time elapsed 21.186745524406433 minutes\n",
            "L2error = tensor([[18.3729743958]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 97, saving model\n",
            "Iteration number: 98\n",
            "Batch loss: tensor([[2.6506838799]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.140940427780151 seconds\n",
            "Total training time elapsed 21.40628693898519 minutes\n",
            "L2error = tensor([[17.9710311890]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 98, saving model\n",
            "Iteration number: 99\n",
            "Batch loss: tensor([[2.5387837887]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.78287649154663 seconds\n",
            "Total training time elapsed 21.619821774959565 minutes\n",
            "L2error = tensor([[17.6491985321]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 99, saving model\n",
            "Iteration number: 100\n",
            "Batch loss: tensor([[2.6744749546]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.217005491256714 seconds\n",
            "Total training time elapsed 21.841721292336782 minutes\n",
            "L2error = tensor([[17.1302185059]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 100, saving model\n",
            "Iteration number: 101\n",
            "Batch loss: tensor([[2.5789504051]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.921660661697388 seconds\n",
            "Total training time elapsed 22.0583003004392 minutes\n",
            "L2error = tensor([[17.1109580994]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 101, saving model\n",
            "Iteration number: 102\n",
            "Batch loss: tensor([[2.4991707802]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.256668329238892 seconds\n",
            "Total training time elapsed 22.28061595360438 minutes\n",
            "L2error = tensor([[16.8545475006]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 102, saving model\n",
            "Iteration number: 103\n",
            "Batch loss: tensor([[2.4629588127]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.353103876113892 seconds\n",
            "Total training time elapsed 22.50366988579432 minutes\n",
            "L2error = tensor([[16.0482406616]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 103, saving model\n",
            "Iteration number: 104\n",
            "Batch loss: tensor([[2.5464265347]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.977347612380981 seconds\n",
            "Total training time elapsed 22.722724187374116 minutes\n",
            "L2error = tensor([[15.9029150009]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 104, saving model\n",
            "Iteration number: 105\n",
            "Batch loss: tensor([[2.3205409050]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.653205633163452 seconds\n",
            "Total training time elapsed 22.93402127424876 minutes\n",
            "L2error = tensor([[15.3297815323]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 105, saving model\n",
            "Iteration number: 106\n",
            "Batch loss: tensor([[2.3895373344]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.29164171218872 seconds\n",
            "Total training time elapsed 23.15621147155762 minutes\n",
            "L2error = tensor([[15.5938587189]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 106, saving model\n",
            "Iteration number: 107\n",
            "Batch loss: tensor([[2.3914904594]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.790882587432861 seconds\n",
            "Total training time elapsed 23.370237147808076 minutes\n",
            "L2error = tensor([[15.3945674896]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 107, saving model\n",
            "Iteration number: 108\n",
            "Batch loss: tensor([[2.3829381466]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.739449501037598 seconds\n",
            "Total training time elapsed 23.583084074656167 minutes\n",
            "L2error = tensor([[14.9835777283]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 108, saving model\n",
            "Iteration number: 109\n",
            "Batch loss: tensor([[2.3707208633]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.943511247634888 seconds\n",
            "Total training time elapsed 23.799607368310294 minutes\n",
            "L2error = tensor([[14.8977212906]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 109, saving model\n",
            "Iteration number: 110\n",
            "Batch loss: tensor([[2.3696861267]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.960203647613525 seconds\n",
            "Total training time elapsed 24.01603387594223 minutes\n",
            "L2error = tensor([[14.3115472794]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 110, saving model\n",
            "Iteration number: 111\n",
            "Batch loss: tensor([[2.2321758270]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.787504434585571 seconds\n",
            "Total training time elapsed 24.229680720965067 minutes\n",
            "L2error = tensor([[14.4467096329]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 111, saving model\n",
            "Iteration number: 112\n",
            "Batch loss: tensor([[2.2442924976]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.209502220153809 seconds\n",
            "Total training time elapsed 24.450290616353353 minutes\n",
            "L2error = tensor([[14.0369796753]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 112, saving model\n",
            "Iteration number: 113\n",
            "Batch loss: tensor([[2.2223062515]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.761606693267822 seconds\n",
            "Total training time elapsed 24.66371682484945 minutes\n",
            "L2error = tensor([[14.0619449615]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 113, saving model\n",
            "Iteration number: 114\n",
            "Batch loss: tensor([[2.2933096886]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.978180885314941 seconds\n",
            "Total training time elapsed 24.881698854764302 minutes\n",
            "L2error = tensor([[13.7182836533]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 114, saving model\n",
            "Iteration number: 115\n",
            "Batch loss: tensor([[2.2182674408]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.84152889251709 seconds\n",
            "Total training time elapsed 25.097357193628948 minutes\n",
            "L2error = tensor([[12.9175920486]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 115, saving model\n",
            "Iteration number: 116\n",
            "Batch loss: tensor([[2.2291133404]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.013782024383545 seconds\n",
            "Total training time elapsed 25.314887018998466 minutes\n",
            "L2error = tensor([[12.9022493362]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 116, saving model\n",
            "Iteration number: 117\n",
            "Batch loss: tensor([[2.1661334038]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.11164665222168 seconds\n",
            "Total training time elapsed 25.53413159449895 minutes\n",
            "L2error = tensor([[12.4738807678]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 117, saving model\n",
            "Iteration number: 118\n",
            "Batch loss: tensor([[2.2073435783]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.207210540771484 seconds\n",
            "Total training time elapsed 25.75467507839203 minutes\n",
            "L2error = tensor([[12.4164094925]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 118, saving model\n",
            "Iteration number: 119\n",
            "Batch loss: tensor([[2.0711712837]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.266437530517578 seconds\n",
            "Total training time elapsed 25.976349778970082 minutes\n",
            "L2error = tensor([[12.2320346832]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 119, saving model\n",
            "Iteration number: 120\n",
            "Batch loss: tensor([[2.0911941528]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.506849527359009 seconds\n",
            "Total training time elapsed 26.20210735797882 minutes\n",
            "L2error = tensor([[12.1442899704]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 120, saving model\n",
            "Iteration number: 121\n",
            "Batch loss: tensor([[2.0421984196]], grad_fn=<AddBackward0>)\n",
            "This iteration took 14.489204406738281 seconds\n",
            "Total training time elapsed 26.46064500808716 minutes\n",
            "L2error = tensor([[11.9201650620]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 121, saving model\n",
            "Iteration number: 122\n",
            "Batch loss: tensor([[2.1273665428]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.966688632965088 seconds\n",
            "Total training time elapsed 26.68775288263957 minutes\n",
            "L2error = tensor([[11.7323923111]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 122, saving model\n",
            "Iteration number: 123\n",
            "Batch loss: tensor([[2.1127772331]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.076016426086426 seconds\n",
            "Total training time elapsed 26.906876969337464 minutes\n",
            "L2error = tensor([[11.4699382782]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 123, saving model\n",
            "Iteration number: 124\n",
            "Batch loss: tensor([[1.9445589781]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.30616044998169 seconds\n",
            "Total training time elapsed 27.129112243652344 minutes\n",
            "L2error = tensor([[11.4291238785]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 124, saving model\n",
            "Iteration number: 125\n",
            "Batch loss: tensor([[2.0335352421]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.983027696609497 seconds\n",
            "Total training time elapsed 27.346084078152973 minutes\n",
            "L2error = tensor([[11.1893148422]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 125, saving model\n",
            "Iteration number: 126\n",
            "Batch loss: tensor([[1.9470454454]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.21645212173462 seconds\n",
            "Total training time elapsed 27.567851376533508 minutes\n",
            "L2error = tensor([[11.0633296967]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 126, saving model\n",
            "Iteration number: 127\n",
            "Batch loss: tensor([[1.9689941406]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.849454164505005 seconds\n",
            "Total training time elapsed 27.782470337549846 minutes\n",
            "L2error = tensor([[10.8733806610]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 127, saving model\n",
            "Iteration number: 128\n",
            "Batch loss: tensor([[1.9400629997]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.632638692855835 seconds\n",
            "Total training time elapsed 27.993585932254792 minutes\n",
            "L2error = tensor([[10.8468856812]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 128, saving model\n",
            "Iteration number: 129\n",
            "Batch loss: tensor([[1.8995919228]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.081673383712769 seconds\n",
            "Total training time elapsed 28.212829168637594 minutes\n",
            "L2error = tensor([[10.3663988113]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 129, saving model\n",
            "Iteration number: 130\n",
            "Batch loss: tensor([[1.9083642960]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.44936203956604 seconds\n",
            "Total training time elapsed 28.43750564257304 minutes\n",
            "L2error = tensor([[10.3104429245]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 130, saving model\n",
            "Iteration number: 131\n",
            "Batch loss: tensor([[1.8084571362]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.960400819778442 seconds\n",
            "Total training time elapsed 28.654039998849232 minutes\n",
            "L2error = tensor([[10.0604677200]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 131, saving model\n",
            "Iteration number: 132\n",
            "Batch loss: tensor([[1.8161305189]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.720589399337769 seconds\n",
            "Total training time elapsed 28.866615506013236 minutes\n",
            "L2error = tensor([[9.9119586945]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 132, saving model\n",
            "Iteration number: 133\n",
            "Batch loss: tensor([[1.8346487284]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.299310684204102 seconds\n",
            "Total training time elapsed 29.088829127947488 minutes\n",
            "L2error = tensor([[10.1031217575]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 133, saving model\n",
            "Iteration number: 134\n",
            "Batch loss: tensor([[1.8513393402]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.873563766479492 seconds\n",
            "Total training time elapsed 29.304274725914002 minutes\n",
            "L2error = tensor([[9.7690973282]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 134, saving model\n",
            "Iteration number: 135\n",
            "Batch loss: tensor([[1.8030035496]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.242164611816406 seconds\n",
            "Total training time elapsed 29.52655135790507 minutes\n",
            "L2error = tensor([[9.7185163498]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 135, saving model\n",
            "Iteration number: 136\n",
            "Batch loss: tensor([[1.7760953903]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.899142503738403 seconds\n",
            "Total training time elapsed 29.742083617051442 minutes\n",
            "L2error = tensor([[9.4941215515]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 136, saving model\n",
            "Iteration number: 137\n",
            "Batch loss: tensor([[1.7886396646]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.247967958450317 seconds\n",
            "Total training time elapsed 29.964786871274313 minutes\n",
            "L2error = tensor([[9.4349412918]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 137, saving model\n",
            "Iteration number: 138\n",
            "Batch loss: tensor([[1.8201653957]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.791613101959229 seconds\n",
            "Total training time elapsed 30.178487559159596 minutes\n",
            "L2error = tensor([[9.2681703568]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 138, saving model\n",
            "Iteration number: 139\n",
            "Batch loss: tensor([[1.8239939213]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.750462055206299 seconds\n",
            "Total training time elapsed 30.391506206989288 minutes\n",
            "L2error = tensor([[8.9774065018]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 139, saving model\n",
            "Iteration number: 140\n",
            "Batch loss: tensor([[1.8006231785]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.01630711555481 seconds\n",
            "Total training time elapsed 30.608925994237264 minutes\n",
            "L2error = tensor([[8.7034320831]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 140, saving model\n",
            "Iteration number: 141\n",
            "Batch loss: tensor([[1.7621341944]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.975366115570068 seconds\n",
            "Total training time elapsed 30.825632572174072 minutes\n",
            "L2error = tensor([[8.6781063080]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 141, saving model\n",
            "Iteration number: 142\n",
            "Batch loss: tensor([[1.6629977226]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.23474383354187 seconds\n",
            "Total training time elapsed 31.04995927810669 minutes\n",
            "L2error = tensor([[8.6459217072]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 142, saving model\n",
            "Iteration number: 143\n",
            "Batch loss: tensor([[1.7384634018]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.468770742416382 seconds\n",
            "Total training time elapsed 31.27497253417969 minutes\n",
            "L2error = tensor([[8.4991025925]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 143, saving model\n",
            "Iteration number: 144\n",
            "Batch loss: tensor([[1.6574556828]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.098130226135254 seconds\n",
            "Total training time elapsed 31.494073685010274 minutes\n",
            "L2error = tensor([[8.3403701782]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 144, saving model\n",
            "Iteration number: 145\n",
            "Batch loss: tensor([[1.7670562267]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.870982646942139 seconds\n",
            "Total training time elapsed 31.709536735216776 minutes\n",
            "L2error = tensor([[8.2901878357]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 145, saving model\n",
            "Iteration number: 146\n",
            "Batch loss: tensor([[1.6895860434]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.009663820266724 seconds\n",
            "Total training time elapsed 31.927066230773924 minutes\n",
            "L2error = tensor([[8.1267395020]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 146, saving model\n",
            "Iteration number: 147\n",
            "Batch loss: tensor([[1.6273604631]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.855045318603516 seconds\n",
            "Total training time elapsed 32.14186269839605 minutes\n",
            "L2error = tensor([[8.0740785599]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 147, saving model\n",
            "Iteration number: 148\n",
            "Batch loss: tensor([[1.7211117744]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.302106857299805 seconds\n",
            "Total training time elapsed 32.36414799690247 minutes\n",
            "L2error = tensor([[7.7184004784]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 148, saving model\n",
            "Iteration number: 149\n",
            "Batch loss: tensor([[1.6388691664]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.015811681747437 seconds\n",
            "Total training time elapsed 32.58163777987162 minutes\n",
            "L2error = tensor([[7.7942523956]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 149, saving model\n",
            "Iteration number: 150\n",
            "Batch loss: tensor([[1.5570375919]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.87638521194458 seconds\n",
            "Total training time elapsed 32.79734461307525 minutes\n",
            "L2error = tensor([[7.6086702347]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 150, saving model\n",
            "Iteration number: 151\n",
            "Batch loss: tensor([[1.5376151800]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.214434146881104 seconds\n",
            "Total training time elapsed 33.01878128449122 minutes\n",
            "L2error = tensor([[7.3484745026]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 151, saving model\n",
            "Iteration number: 152\n",
            "Batch loss: tensor([[1.6504689455]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.024443864822388 seconds\n",
            "Total training time elapsed 33.2375439286232 minutes\n",
            "L2error = tensor([[7.2185163498]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 152, saving model\n",
            "Iteration number: 153\n",
            "Batch loss: tensor([[1.6377973557]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.790161371231079 seconds\n",
            "Total training time elapsed 33.451171255111696 minutes\n",
            "L2error = tensor([[7.0562982559]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 153, saving model\n",
            "Iteration number: 154\n",
            "Batch loss: tensor([[1.5689597130]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.867002725601196 seconds\n",
            "Total training time elapsed 33.66629552443822 minutes\n",
            "L2error = tensor([[7.0978803635]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 154, saving model\n",
            "Iteration number: 155\n",
            "Batch loss: tensor([[1.5973687172]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.863504648208618 seconds\n",
            "Total training time elapsed 33.88214528958003 minutes\n",
            "L2error = tensor([[6.9403471947]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 155, saving model\n",
            "Iteration number: 156\n",
            "Batch loss: tensor([[1.5215697289]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.895433187484741 seconds\n",
            "Total training time elapsed 34.0975700656573 minutes\n",
            "L2error = tensor([[7.0205984116]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 156, saving model\n",
            "Iteration number: 157\n",
            "Batch loss: tensor([[1.5125186443]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.072215795516968 seconds\n",
            "Total training time elapsed 34.315929126739505 minutes\n",
            "L2error = tensor([[6.7483019829]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 157, saving model\n",
            "Iteration number: 158\n",
            "Batch loss: tensor([[1.5445494652]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.77784252166748 seconds\n",
            "Total training time elapsed 34.52940526008606 minutes\n",
            "L2error = tensor([[6.7521042824]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 158, saving model\n",
            "Iteration number: 159\n",
            "Batch loss: tensor([[1.5489766598]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.653373956680298 seconds\n",
            "Total training time elapsed 34.758227403958635 minutes\n",
            "L2error = tensor([[6.6614971161]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 159, saving model\n",
            "Iteration number: 160\n",
            "Batch loss: tensor([[1.4936782122]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.717801094055176 seconds\n",
            "Total training time elapsed 34.97078503370285 minutes\n",
            "L2error = tensor([[6.5193586349]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 160, saving model\n",
            "Iteration number: 161\n",
            "Batch loss: tensor([[1.4887576103]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.02201247215271 seconds\n",
            "Total training time elapsed 35.188447717825575 minutes\n",
            "L2error = tensor([[6.5745534897]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 161, saving model\n",
            "Iteration number: 162\n",
            "Batch loss: tensor([[1.5546302795]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.979062795639038 seconds\n",
            "Total training time elapsed 35.40793003241221 minutes\n",
            "L2error = tensor([[6.5632853508]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 162, saving model\n",
            "Iteration number: 163\n",
            "Batch loss: tensor([[1.4454605579]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.753318071365356 seconds\n",
            "Total training time elapsed 35.6213670651118 minutes\n",
            "L2error = tensor([[6.2673430443]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 163, saving model\n",
            "Iteration number: 164\n",
            "Batch loss: tensor([[1.5013585091]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.746995449066162 seconds\n",
            "Total training time elapsed 35.83440883159638 minutes\n",
            "L2error = tensor([[6.2286705971]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 164, saving model\n",
            "Iteration number: 165\n",
            "Batch loss: tensor([[1.5587698221]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.814320802688599 seconds\n",
            "Total training time elapsed 36.04845326741536 minutes\n",
            "L2error = tensor([[5.9899821281]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 165, saving model\n",
            "Iteration number: 166\n",
            "Batch loss: tensor([[1.3413617611]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.333664178848267 seconds\n",
            "Total training time elapsed 36.27137793302536 minutes\n",
            "L2error = tensor([[6.1040501595]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 166, saving model\n",
            "Iteration number: 167\n",
            "Batch loss: tensor([[1.4504709244]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.284779071807861 seconds\n",
            "Total training time elapsed 36.493989555041 minutes\n",
            "L2error = tensor([[5.6871428490]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 167, saving model\n",
            "Iteration number: 168\n",
            "Batch loss: tensor([[1.3851311207]], grad_fn=<AddBackward0>)\n",
            "This iteration took 15.032033920288086 seconds\n",
            "Total training time elapsed 36.769190804163614 minutes\n",
            "L2error = tensor([[5.8196725845]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 168, saving model\n",
            "Iteration number: 169\n",
            "Batch loss: tensor([[1.3975512981]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.929989576339722 seconds\n",
            "Total training time elapsed 36.98536763191223 minutes\n",
            "L2error = tensor([[5.7869381905]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 169, saving model\n",
            "Iteration number: 170\n",
            "Batch loss: tensor([[1.3749053478]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.494340419769287 seconds\n",
            "Total training time elapsed 37.21154303153356 minutes\n",
            "L2error = tensor([[5.6487183571]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 170, saving model\n",
            "Iteration number: 171\n",
            "Batch loss: tensor([[1.3547825813]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.017287969589233 seconds\n",
            "Total training time elapsed 37.42924892107646 minutes\n",
            "L2error = tensor([[5.5218038559]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 171, saving model\n",
            "Iteration number: 172\n",
            "Batch loss: tensor([[1.4923317432]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.18985652923584 seconds\n",
            "Total training time elapsed 37.649713869889574 minutes\n",
            "L2error = tensor([[5.3512654305]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 172, saving model\n",
            "Iteration number: 173\n",
            "Batch loss: tensor([[1.4355664253]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.092418670654297 seconds\n",
            "Total training time elapsed 37.87003527482351 minutes\n",
            "L2error = tensor([[5.3876066208]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 173, saving model\n",
            "Iteration number: 174\n",
            "Batch loss: tensor([[1.3733695745]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.832256317138672 seconds\n",
            "Total training time elapsed 38.08452643156052 minutes\n",
            "L2error = tensor([[5.2730956078]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 174, saving model\n",
            "Iteration number: 175\n",
            "Batch loss: tensor([[1.3079661131]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.77527904510498 seconds\n",
            "Total training time elapsed 38.29901222785314 minutes\n",
            "L2error = tensor([[5.1316189766]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 175, saving model\n",
            "Iteration number: 176\n",
            "Batch loss: tensor([[1.3561420441]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.0919029712677 seconds\n",
            "Total training time elapsed 38.517708122730255 minutes\n",
            "L2error = tensor([[5.1928262711]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 176, saving model\n",
            "Iteration number: 177\n",
            "Batch loss: tensor([[1.3455018997]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.91029667854309 seconds\n",
            "Total training time elapsed 38.7335707505544 minutes\n",
            "L2error = tensor([[4.9366021156]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 177, saving model\n",
            "Iteration number: 178\n",
            "Batch loss: tensor([[1.4024827480]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.849523544311523 seconds\n",
            "Total training time elapsed 38.948326869805655 minutes\n",
            "L2error = tensor([[4.9451766014]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 178, saving model\n",
            "Iteration number: 179\n",
            "Batch loss: tensor([[1.3140594959]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.863239288330078 seconds\n",
            "Total training time elapsed 39.16347999970118 minutes\n",
            "L2error = tensor([[4.9507346153]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 179, saving model\n",
            "Iteration number: 180\n",
            "Batch loss: tensor([[1.3333836794]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.07181191444397 seconds\n",
            "Total training time elapsed 39.38202811082204 minutes\n",
            "L2error = tensor([[4.8596086502]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 180, saving model\n",
            "Iteration number: 181\n",
            "Batch loss: tensor([[1.3667838573]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.142490863800049 seconds\n",
            "Total training time elapsed 39.60202379226685 minutes\n",
            "L2error = tensor([[4.6563715935]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 181, saving model\n",
            "Iteration number: 182\n",
            "Batch loss: tensor([[1.2328383923]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.985996007919312 seconds\n",
            "Total training time elapsed 39.81926349798838 minutes\n",
            "L2error = tensor([[4.5851197243]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 182, saving model\n",
            "Iteration number: 183\n",
            "Batch loss: tensor([[1.2539173365]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.823889255523682 seconds\n",
            "Total training time elapsed 40.03373129367829 minutes\n",
            "L2error = tensor([[4.6710171700]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 183, saving model\n",
            "Iteration number: 184\n",
            "Batch loss: tensor([[1.3287888765]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.823441982269287 seconds\n",
            "Total training time elapsed 40.24961544275284 minutes\n",
            "L2error = tensor([[4.6426558495]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 184, saving model\n",
            "Iteration number: 185\n",
            "Batch loss: tensor([[1.3123250008]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.699447631835938 seconds\n",
            "Total training time elapsed 40.46209700504939 minutes\n",
            "L2error = tensor([[4.4878706932]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 185, saving model\n",
            "Iteration number: 186\n",
            "Batch loss: tensor([[1.2915449142]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.986932516098022 seconds\n",
            "Total training time elapsed 40.679133303960164 minutes\n",
            "L2error = tensor([[4.4095816612]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 186, saving model\n",
            "Iteration number: 187\n",
            "Batch loss: tensor([[1.2685613632]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.868356227874756 seconds\n",
            "Total training time elapsed 40.89441046317418 minutes\n",
            "L2error = tensor([[4.5298891068]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 187, saving model\n",
            "Iteration number: 188\n",
            "Batch loss: tensor([[1.2741613388]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.156602144241333 seconds\n",
            "Total training time elapsed 41.114203945795694 minutes\n",
            "L2error = tensor([[4.3166656494]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 188, saving model\n",
            "Iteration number: 189\n",
            "Batch loss: tensor([[1.3017936945]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.142343521118164 seconds\n",
            "Total training time elapsed 41.33380089998245 minutes\n",
            "L2error = tensor([[4.2722225189]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 189, saving model\n",
            "Iteration number: 190\n",
            "Batch loss: tensor([[1.2045958042]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.889435529708862 seconds\n",
            "Total training time elapsed 41.54921193520228 minutes\n",
            "L2error = tensor([[4.2228589058]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 190, saving model\n",
            "Iteration number: 191\n",
            "Batch loss: tensor([[1.2122194767]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.646230459213257 seconds\n",
            "Total training time elapsed 41.77789711157481 minutes\n",
            "L2error = tensor([[4.2341108322]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 191, saving model\n",
            "Iteration number: 192\n",
            "Batch loss: tensor([[1.2333661318]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.887407302856445 seconds\n",
            "Total training time elapsed 41.9966860294342 minutes\n",
            "L2error = tensor([[4.1382417679]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 192, saving model\n",
            "Iteration number: 193\n",
            "Batch loss: tensor([[1.2498316765]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.323002576828003 seconds\n",
            "Total training time elapsed 42.21929748853048 minutes\n",
            "L2error = tensor([[4.1497044563]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 193, saving model\n",
            "Iteration number: 194\n",
            "Batch loss: tensor([[1.2102603912]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.794299364089966 seconds\n",
            "Total training time elapsed 42.43404711484909 minutes\n",
            "L2error = tensor([[3.8517446518]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 194, saving model\n",
            "Iteration number: 195\n",
            "Batch loss: tensor([[1.1742026806]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.236106395721436 seconds\n",
            "Total training time elapsed 42.65522509415944 minutes\n",
            "L2error = tensor([[3.9052526951]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 195, saving model\n",
            "Iteration number: 196\n",
            "Batch loss: tensor([[1.1914894581]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.882415771484375 seconds\n",
            "Total training time elapsed 42.87057069937388 minutes\n",
            "L2error = tensor([[3.9355278015]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 196, saving model\n",
            "Iteration number: 197\n",
            "Batch loss: tensor([[1.2207710743]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.947421550750732 seconds\n",
            "Total training time elapsed 43.087911701202394 minutes\n",
            "L2error = tensor([[3.9181158543]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 197, saving model\n",
            "Iteration number: 198\n",
            "Batch loss: tensor([[1.1583169699]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.035054206848145 seconds\n",
            "Total training time elapsed 43.308598359425865 minutes\n",
            "L2error = tensor([[3.7140998840]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 198, saving model\n",
            "Iteration number: 199\n",
            "Batch loss: tensor([[1.2094104290]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.881471633911133 seconds\n",
            "Total training time elapsed 43.52418207724889 minutes\n",
            "L2error = tensor([[3.7407574654]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 199, saving model\n",
            "Iteration number: 200\n",
            "Batch loss: tensor([[1.1662951708]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.039889812469482 seconds\n",
            "Total training time elapsed 43.743531092007956 minutes\n",
            "L2error = tensor([[3.6535308361]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 200, saving model\n",
            "Plotting and saving convergence history\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1d8f6544a8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAfJElEQVR4nO3deZgV1ZnH8e/bTcuuLN1As6MBFUS2VkFEE8R1VDROooyjRJ0xxiXu+zMueTQajfuMcYlEYpSoMxKZQR0VdwW1QRBQCCoCLVsjJIAiIn3mj7fucGlpaIS6dfvW7/M896l7q2/3falufnXqVNU5FkJARETSoyjpAkREJLcU/CIiKaPgFxFJGQW/iEjKKPhFRFKmUdIF1EdpaWno3r170mWIiDQoU6dOXRFCKKu9vkEEf/fu3amsrEy6DBGRBsXMFmxpvbp6RERSRsEvIpIyCn4RkZRR8IuIpIyCX0QkZRT8IiIpo+AXEUmZgg7+iRPhlluSrkJEJL8UdPC/+CL8+tdJVyEikl8KOvg7dIA1a+DLL5OuREQkfxR08JeX+3LZsmTrEBHJJwUd/B06+HLJkmTrEBHJJ6kI/qVLk61DRCSfKPhFRFKmoIO/tBSKi9XVIyKSraCDv7gY2rVTi19EJFtBBz94d4+CX0Rkk4IP/vJydfWIiGQr+OBXi19EZHOpCP5ly6CmJulKRETyQ8EHf3k5bNwIK1YkXYmISH4o+ODXtfwiIptT8IuIpIyCX0QkZQo++DMjdC5enGwdIiL5ouCDv3lzaN0aFi1KuhIRkfxQ8MEP0KULLFyYdBUiIvkhNcGvFr+IiEtF8HftquAXEclIRfB36QIrV8JXXyVdiYhI8lIT/KBWv4gIpCz4dYJXRCRlwa8Wv4hISoK/UycwU/CLiEBKgr9xY2jfXsEvIgIpCX7QTVwiIhmpCn61+EVEYgx+M+tiZq+Y2YdmNtvMLojWtzGzF81sXrRsHVcN2bp29Ra/ZuISkbSLs8X/LXBJCKE3MBg418x6A1cCk0IIPYFJ0evY9enjN3B98kkuPk1EJH/FFvwhhCUhhGnR8zXAR0AnYCQwNnrbWOD4uGrItt9+vqyszMWniYjkr5z08ZtZd2AA8A7QPoSwJPrSUqB9Hd9zlplVmllldXX1DtfQuzc0aaLgFxGJPfjNrAXwX8CFIYTV2V8LIQQgbOn7QggPhhAqQggVZWVlO1xHSQn076/gFxGJNfjNrAQP/cdCCE9Hq5eZWXn09XJgeZw1ZKuogKlTYePGXH2iiEj+ifOqHgMeBj4KIdyR9aUJwOjo+WjgmbhqqG2//eDLL2Hu3Fx9oohI/omzxT8UOBUYbmbTo8fRwC3AYWY2DxgRvc6JigpfqrtHRNKsUVw/OITwJmB1fPnQuD53a/bcE5o2henT4bTTkqhARCR5qblzF6C4GHr1UlePiKRbqoIfvNWv4BeRNEtl8M+fD+vXJ12JiEgyUhn8NTUaukFE0it1wb/XXr6cMyfZOkREkpK64O/Vy5fq5xeRtEpd8LdsCR07KvhFJL1SF/ygK3tEJN1SHfxhi8PDiYgUtlQGf+/esGoVLF6cdCUiIrmXyuAfNMiXU6cmW4eISBJSGfz9+0NRkQZrE5F0SmXwN2vmc/Aq+EUkjVIZ/OBDNFdW6gSviKRPaoN/0CCoroZFi5KuREQkt1Ib/JlJWXSCV0TSJrXB368fNGqk4BeR9Elt8DdpAuXlUFWVdCUiIrmV2uAHKC2FFSuSrkJEJLdSHfxlZX6CV0QkTVId/Grxi0gapTr41eIXkTRKdfCXlsKaNZp/V0TSJdXBX1bmS3X3iEiapDr4S0t9qeAXkTRJdfBnWvzq5xeRNEl18KvFLyJplOrgV4tfRNIo1cHfpg2YqcUvIumS6uAvLvbwV4tfRNIk1cEP3t2jFr+IpEnqg7+0VC1+EUmX1Ae/WvwikjapD361+EUkbVIf/JkWvyZdF5G0iC34zWyMmS03s1lZ6643s8/NbHr0ODquz6+v0lLYuBFWrUq6EhGR3Iizxf8IcOQW1t8ZQugfPZ6N8fPrpUcPX86bl2wdIiK5ElvwhxBeB1bG9fN3lr59fTlzZrJ1iIjkShJ9/OeZ2QdRV1Drut5kZmeZWaWZVVbHePa1Rw9o3lzBLyLpkevg/x2wB9AfWALcXtcbQwgPhhAqQggVZZlBdWJQVAR9+ij4RSQ9chr8IYRlIYSNIYQa4CFg/1x+fl369oVZs7b9PhGRQpDT4Dez8qyXJwB5Ebd9+/q1/MuWJV2JiEj8GsX1g81sHPBDoNTMqoDrgB+aWX8gAJ8BP4/r87dH9gne9u2TrUVEJG6xBX8IYdQWVj8c1+ftiH328eXMmTBiRLK1iIjELfV37gK0a+eP2bOTrkREJH4K/kiPHrBggT8fORIuvjjZekRE4hJbV09D06XLpks633xTI3aKSOFSiz/StSssXAirV8PKlbBkSdIViYjEQ8Ef6dIF1q2DadP89eLFGrFTRAqTgj/SpYsv33jDl+vXa8ROESlMCv5I166+zAQ/eKtfRKTQKPgjmRb/5Mmb1in4RaQQKfgj7dpBSQmsXQvNmvk6Bb+IFKJ6Bb+ZNTezouh5LzM7zsxK4i0tt4qKoHNnf77ffr5U8ItIIapvi/91oImZdQJeAE7FZ9gqKJl+/r33hlatFPwiUpjqG/wWQvgK+DFwXwjhJ0Cf+MpKRqafv3t36NhRwS8ihanewW9mQ4BTgInRuuJ4SkpOJvi7dfPg101cIlKI6hv8FwJXAeNDCLPNbHfglfjKSkamq0ctfhEpZPUaqyeE8BrwGkB0kndFCOGXcRaWhGOOgenTYcCATS3+iy6Cli3hV79KujoRkZ2jvlf1PG5mu5pZc3zWrA/N7LJ4S8u9zp3h/vuhcWMP/g0b4K674MYbfYcgIlII6tvV0zuEsBo4HngO6IFf2VOwOnb05XHH+RU+V12VbD0iIjtLfYO/JLpu/3hgQghhAz59YsE64gi49VZ47DG45hp4/nl4772kqxIR2XH1Df4H8DlymwOvm1k3YHVcReWDFi3gsst8ecopvu7tt5OtSURkZ6jvyd17gHuyVi0wsx/FU1L+6dDBH++/n3QlIiI7rr4nd3czszvMrDJ63I63/lOjf3+d4BWRwlDfrp4xwBrgp9FjNfCHuIrKRwMG+GTs69cnXYmIyI6p75y7e4QQTsx6fYOZpar9278/fPuth//AgUlXIyLy/dW3xb/OzA7KvDCzocC6eErKTwMG+FL9/CLS0NW3xX828Ecz2y16vQoYHU9J+WmPPfwKH/Xzi0hDV68WfwhhRgihH7AvsG8IYQAwPNbK8kxRkXf3PPwwHHoo/PWvSVckIvL9bNcMXCGE1dEdvAAXx1BPXrvtNjjtNJg2DU491fv8RUQamh2ZetF2WhUNxODBPpbP734H774Ld96ZdEUiIttvR4K/oIds2JqTToJjj4WbboKvv066GhGR7bPV4DezNWa2eguPNUDHHNWYd8zgvPPg73+HiRO3/X4RkXyy1eAPIbQMIey6hUfLEEJ9rwgqSMOHQ/v2PoibiEhDsiNdPanWqBGMGuUt/lWrkq5GRKT+FPw74JRT4Jtv4E9/SroSEZH6U/DvgEGDYOhQH7dfY/iISEOh4N8BZnDddVBVBY88knQ1IiL1E1vwm9kYM1tuZrOy1rUxsxfNbF60bB3X5+fKiBF+ff+NN/pVPiIi+S7OFv8jwJG11l0JTAoh9AQmRa8bNDOfkH3xYrjggqSrERHZttiCP4TwOrCy1uqRwNjo+Vh8Dt8G74AD4OqrYexYePbZpKsREdm6XPfxtw8hLImeLwXa1/VGMzsrM+NXdXV1bqrbAdde69Mzqq9fRPJdYid3QwiBrQz7EEJ4MIRQEUKoKCsry2Fl309JCYwcCc89p2EcRCS/5Tr4l5lZOUC0XJ7jz4/V8cfD2rUwaVLSlYiI1C3XwT+BTRO4jAaeyfHnx2r4cNh1Vxg/PulKRETqFuflnOOAycCeZlZlZmcCtwCHmdk8YET0umDssgscfTQ8/jiceSZ89lnSFYmIfFdsA62FEEbV8aVD4/rMfHDzzb4cOxZatYLbb0+2HhGR2lI9wmYcuneHcePgww81PaOI5CcN2RCTXr0U/CKSnxT8MenZEz79VPPyikj+UfDHpFcvD32d4BWRfKPgj0mvXr5Ud4+I5BsFf0wU/CKSrxT8MWnbFlq3hhkzfCdwzTVJVyQi4hT8MTHzwH/0UZg3D377W1iwIOmqREQU/LHq1Qs2bvSJWszg+uuTrkhERMEfqz59fHnffXDeeX4375w5ydYkIqLgj9G550JlJQwYAFdcAU2bwq9/DTU1sHBh0tWJSFop+GPUogUMGuTPy8rg7LN9ALfhw6FbN3j++WTrE5F0UvDn0KWXQqNGMHkydOzoO4K1a5OuSkTSRsGfQ+Xl8MILMHUqPPGEX+Xzi1/Ahg1JVyYiaaLROXPs4IM3Pb/hBrjuOpg/308EZ0b0fPVV2HvvxEoUkQKnFn+Crr0WxoyBjz+GZ56Bdetg+XJN3Sgi8VKLP2Gnn+4PgBD8JPCMGcnWJCKFTS3+PGIG/fop+EUkXgr+PNO/P8ycqXH8RSQ+Cv48068ffP21j+8jIhIHBX+e6dfPl3/5i9/oNXVqsvWISOHRyd08s/feUFLiwziHABdcAG+84f3/IiI7g1r8eWaXXaB3bw/9Y46Bt96CiROTrkpEComCPw+dcw5cdRU8/TT84Adw0UVQXZ10VSJSKBT8eeiss3wUz5ISv8GrqgoOPxxWrvSxfS67TH3/IvL9Kfjz3LBhfqL3ww+hogIOO8xn8zrqKB/qobaNG+GTT3Jfp4g0HAr+BuCII+C11+Cbb3x8/9/+1gd2GzoURo+GuXM3vff++2HPPTXJu4jUTcHfQAweDB984Dd3XXIJPPusT/AyfjycfPKmG76efNJb/WPHJluviOQvBX8D0qYN7LWXPx8yxK/2GTMGpk/3ln51Nbz5JhQVwR//6DsAEZHaFPwN3IknwogRft3/Lbf4tI6XX+4nhF95JenqRCQfKfgbODP4/e99msc77vApHa+9Flq1gjPOgKeeSrpCEck3Cv4C0K2bz+zVrp2f7G3a1LuB2raFn/7Uzw0sXAgjR8LSpUlXKyJJ05ANBaJPH+/eKS721wce6OHfqZNP6r5hA0yY4HcF33xzsrWKSLLU4i8gJSV+YjejY0ffIbz44qZhHx54AL76Kpn6RCQ/KPgL3GGHweuvw5Qp/nzVKr8SKNvy5T42kIikQyLBb2afmdlMM5tuZpVJ1JAWI0b4jV8hwE03+WWg55/v1/6vXu1DP5SX+x3C77+fdLUikgtJ9vH/KISwIsHPT4VDDvEuoDZtYNAg7/K5/XYfC2iPPfy8QNOmPvHLsGEweTL07Zt01SISJ53cLXAtWsCZZ0Lnzt7/37o13Hijj+dz991+0vdf/xWuvhr22w+OOw7eew9KS5OuXETiYiGBzl0zmw+sAgLwQAjhwS285yzgLICuXbsOWrBgQW6LLHBz5/oVPjU1PgDc3nt74A8ZAhdfDLfe6u/LnAhu1iy5WkXk+zGzqSGEitrrkzq5e1AIYSBwFHCumR1c+w0hhAdDCBUhhIqysrLcV1jg9tzTA370aA998Bb/yJF+8vfzz/118+bQoQP87/8mW6+I7DyJBH8I4fNouRwYD+yfRB1pd9tt8Mgjm6875xz44otNg8Jdd52fC/iHf4B/+zf/WoauBBJpmHIe/GbW3MxaZp4DhwOzcl2HbNnw4X40UFXlY/9cf70PCX3iiX5uYPfd4YknfC7gTp00IYxIQ5TzPn4z2x1v5YOfXH48hHDT1r6noqIiVFbqqs9ceeklePllD/rsG8Jmz4bTT/dzAeDjAZWU+LzAPXsmU6uI1K2uPv5ETu5uLwV//li/3ieC6d/fw37oUB8j6N13/ZLQVq2ge/ekqxQRqDv4dTmnbJfGjX0I6Ixx43w+4KFD/ZxAz54wa5YfCYhIftKQDbJDMnMBzJgBP/qRT/n48MNJVyUiW6Pglx32q1/Bp5/6uYFhw3w+gIsvhkMP9ev/n3wy6QpFJJuCX3aYGfTo4cu77/a7hR98EJYs8UtBTz0V3njju9/3zTfwl7/4JaR//rPfTCYi8VPwy041YIC3/teu9TuCX33VT/aOGAG/+c2meYCrq+Hgg+GEE7xraNQo2H9/P08gIvFS8Eus2rb11v4xx8CVV8Ill8D8+X4yeMYM+NOfYM0aX1ZVQUWFt/5FJD66nFNyIgTv97/rLth1V58p7L//23cAGStW+BHA1Kl+XuCxx+CII+BnP0usbJEGTZdzSqLM/Pr/zz7zlv7EiZvGCMooLfXJ4fv3h2OP9XVPPAGNGvkO4JFH/GigWTPfQVx0ke8siot92GkRqR919UjOFBfD00/7jV61Qz+jQwc/4Xv22T6C6NChfnK4XTu4/HJo0gTWrfMuox/+ELp2hYED/ZwB+OQyl1/udxmLyJapq0fy2po18J//6VNGHnCA7whCgBtu8CElTjgB/ud/fCTRZ5/1K4QefRR2282/b8SIpP8FIsnRkA1ScL7+2o8Axo2Df/on7+5ZudLD//XX/aqiyy/3o4KaGr9ySF1CkiYKfilokyfDZZd5//+zz/pO4fzzNx92ukkTn3ymc2d/dOni5xOOPNKPItas8RPPIoVCwS+pNHOmd/v87W/whz/4kBKLFvmlo6tW+XuuusrXT5gAzzwDRx2VbM0iO4uCX6SWL7+EX/7SZxwrKvIjgBUr4PnnYZ99fHayuXP96KCqCnr1gjvv9LuRAZYt85POZsn+O0Tqkm9TL4okrnlzeOghH2bipZdgyhRo397vKO7bF557zielWbMG9trL70LeZx+/A/nqq/0KpIMOgjff3PznNoC2lKScruOXVCsq8lZ/xrRpPt3kk0/6DWZHHLHpa1VVft7gyiv99Y9/7OcWhg2DAw/0ew+qq32comuu2fQ+kXyjrh6R7TRxop88PvFE7y4aMwbuuw/mzPEdSa9e3kV0771+ZdGqVX4TWkkJNG3qJ6CbNfNhLCq+cxAusvOoj18kZitXwoYNPjrpAQf4TWTNm0PHjr7+m298BrOvvvKb0EpKfEjrzp19R9CoETz+uO88zjjDf067dj7PcU2Nn0vQ+QTZHhqyQSRm2fcIPP+830tw7LHQsuV337tqFfzzP/sVRdnatvURTMeN27Ru3339SqRWrXwIi4oK3xEUF3v30zvv+PcNGeIzpIlsi1r8IgmpqfFpKhs39qOAlSv9zuSNG2HSJD8CmD3b70vYfXdf9/nnvr642HcAb7/tRxPg3/vUU34nc9euPiWmjhDSTV09Ig3cF1/AzTd7mH/5Jbz1lp9UPv10H/junHN8Z5KZ0KZHDx+7qFUr30kMGuRHJWbwk59s+UhECouCX6TATZzo4xSdc46fXJ440S9PXbHCh7pesGDTe9u3h+OO8y6nFStg+XI/4hg4EA45xE9Qb9gACxf6UcmaNX6k0aOH3+08bJifu8gWgl/V1K5dbv/dUjcFv0jKffGFz4xWVeX3IcyZ4+cG2raFsjI/Anj7bfj4482/r7zcjxS+/tp3Ht9+611NP/+5H1G8+66PlfTqq/4YMgTOPdePMu6+299/4YU+XEbG+vXebdW2rd9ZLfFQ8ItIvaxaBZ984mMblZd7OGds2ODDYPz+9/DAA341Ur9+3u3UqhX8y7/A+PH+/QC77OI7iXXr4Pjj/W7omhq49FKfiQ18R3HIIb5TGToUTjvNr2hassSn8Vy/3ofxzhxh/O1v8NFHfuVUUZH/vClTfD6HXr1yu63ynYJfRHaqpUs9+Hfd1QO6VSs/Mqip8buZp0yBk07yS1rvvdcfmfGRevb0GdlWrPBpN+fN8+6nJUv86yUlm05aZ+y7rx+VTJ3qRx9Dh/pO5+mnvZa2baGy0o9qli71+Roa1bpucdGiTXddn3SSn/co5BPgCn4RSdSXX/pVSitXegu/aVNfH4JfydSoke8sXnrJzyl06uRDZuyyix9RvPWWHznss4+37G+4wV8ffTQcdhhccYXvZJYu9Z1P27b+vQDduvnj5Ze9jg0b/LHHHn5Z7YEHejfXunW+45g4Ebp3hwsu8K6qDz/0n3vppZvGapozxwf3Ky31rrLSUu+2KooGwlm71utJcsei4BeRgrJ+vQd8Zgfy3HPwj//oczQPH+5DbpSU+E5lwQJ/tG0LY8f60cX48X6/xKRJvvMpKvKdT1GRT+AzbRosXuw/28x3Ik2a+I5i1ix47bXv1lRc7J9RU+NHMwcd5DuLoiIf1K+qys9tdOgAgwd7d1VxsR8xzZ/v3VmDB/tOZcMGHzOqaAdGVFPwi0jBq6nZ/qBctMiDfMgQ767K/Iyvv/ajgNJSb+VXV/tJ7A8+8KubRo2CQw/ddGVU9qOmxq9ueughv2Iqw8yPDr74wndIW9KihR8tALRu7fNMH37499seunNXRAre92kdd+nij9o/o0kTP3LIaN7cdwQh1L/75oorYPp0/1nt2/vJ8pIS726aOtWviDLzG/R69PAd0Kuv+pFAkyZ+VNGz5/b/m7ZFLX4RkQKl8fhFRARQ8IuIpI6CX0QkZRT8IiIpo+AXEUkZBb+ISMoo+EVEUkbBLyKSMg3iBi4zqwYWbPON31UKrNjJ5ewMqmv75GtdkL+1qa7tk691wY7V1i2EUFZ7ZYMI/u/LzCq3dNda0lTX9snXuiB/a1Nd2ydf64J4alNXj4hIyij4RURSptCD/8GkC6iD6to++VoX5G9tqmv75GtdEENtBd3HLyIi31XoLX4REalFwS8ikjIFGfxmdqSZzTWzj83sygTr6GJmr5jZh2Y228wuiNZfb2afm9n06HF0QvV9ZmYzoxoqo3VtzOxFM5sXLVvnuKY9s7bLdDNbbWYXJrHNzGyMmS03s1lZ67a4fczdE/3NfWBmA3Nc121mNif67PFm1ipa393M1mVtt/vjqmsrtdX5uzOzq6JtNtfMjshxXU9k1fSZmU2P1udsm20lI+L9OwshFNQDKAY+AXYHdgFmAL0TqqUcGBg9bwn8FegNXA9cmgfb6jOgtNa6W4Ero+dXAr9J+He5FOiWxDYDDgYGArO2tX2Ao4HnAAMGA+/kuK7DgUbR899k1dU9+30JbbMt/u6i/wszgMZAj+j/bXGu6qr19duBa3O9zbaSEbH+nRVii39/4OMQwqchhG+APwMjkygkhLAkhDAter4G+AjolEQt22EkMDZ6PhY4PsFaDgU+CSF8n7u2d1gI4XVgZa3VdW2fkcAfg5sCtDKz8lzVFUJ4IYTwbfRyCtA5js/eljq2WV1GAn8OIawPIcwHPsb//+a0LjMz4KfAuDg+e2u2khGx/p0VYvB3AhZlva4iD8LWzLoDA4B3olXnRYdqY3LdnZIlAC+Y2VQzOyta1z6EsCR6vhRon0xpAJzM5v8Z82Gb1bV98unv7gy8VZjRw8zeN7PXzGxYQjVt6XeXL9tsGLAshDAva13Ot1mtjIj176wQgz/vmFkL4L+AC0MIq4HfAXsA/YEl+GFmEg4KIQwEjgLONbODs78Y/Ngyket9zWwX4DjgqWhVvmyz/5fk9qmLmV0DfAs8Fq1aAnQNIQwALgYeN7Ndc1xW3v3uahnF5g2MnG+zLWTE/4vj76wQg/9zoEvW687RukSYWQn+C30shPA0QAhhWQhhYwihBniImA5vtyWE8Hm0XA6Mj+pYljl0jJbLk6gN3xlNCyEsi2rMi21G3dsn8b87M/sZcAxwShQWRN0oX0TPp+L96L1yWddWfnf5sM0aAT8Gnsisy/U221JGEPPfWSEG/3tATzPrEbUaTwYmJFFI1Hf4MPBRCOGOrPXZfXInALNqf28OamtuZi0zz/GTg7PwbTU6etto4Jlc1xbZrBWWD9ssUtf2mQCcFl11MRj4e9aheuzM7EjgcuC4EMJXWevLzKw4er470BP4NFd1RZ9b1+9uAnCymTU2sx5Rbe/msjZgBDAnhFCVWZHLbVZXRhD331kuzlzn+oGf+f4rvqe+JsE6DsIP0T4ApkePo4FHgZnR+glAeQK17Y5fUTEDmJ3ZTkBbYBIwD3gJaJNAbc2BL4DdstblfJvhO54lwAa8L/XMurYPfpXFf0R/czOBihzX9THe95v5O7s/eu+J0e93OjANODaBbVbn7w64Jtpmc4GjcllXtP4R4Oxa783ZNttKRsT6d6YhG0REUqYQu3pERGQrFPwiIimj4BcRSRkFv4hIyij4RURSRsEvApjZRtt8VNCdNqprNNpjUvcdiHxHo6QLEMkT60II/ZMuQiQX1OIX2YponPZbzecteNfMfhCt725mL0cDj00ys67R+vbm4+HPiB4HRj+q2MweisZcf8HMmib2j5LUU/CLuKa1unpOyvra30MIfYF/B+6K1t0LjA0h7IsPiHZPtP4e4LUQQj98/PfZ0fqewH+EEPoAf8PvDhVJhO7cFQHMbG0IocUW1n8GDA8hfBoNprU0hNDWzFbgQw9siNYvCSGUmlk10DmEsD7rZ3QHXgwh9IxeXwGUhBBujP9fJvJdavGLbFuo4/n2WJ/1fCM6vyYJUvCLbNtJWcvJ0fO38ZFfAU4B3oieTwJ+AWBmxWa2W66KFKkvtTpEXFOLJtuOPB9CyFzS2drMPsBb7aOidecDfzCzy4Bq4PRo/QXAg2Z2Jt6y/wU+KqRI3lAfv8hWRH38FSGEFUnXIrKzqKtHRCRl1OIXEUkZtfhFRFJGwS8ikjIKfhGRlFHwi4ikjIJfRCRl/g8/oFmNDj21AQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}