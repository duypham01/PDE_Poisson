{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PDE_Poisson(pytorch).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO9BSy6qEN4QZ6qLqPONDCp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duypham01/PDE_Poisson/blob/master/PDE_Poisson(pytorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJPZCBQUtcYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import grad\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "import itertools\n",
        "import time\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK48bUkdt59J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PDENetLight(nn.Module):\n",
        "\n",
        "    # đọc paper để biết thêm chi tiết về tham số :))))\n",
        "    # hidden_sz = M trong paper\n",
        "    def __init__(self, dim_in, hidden_sz):\n",
        "        super().__init__()\n",
        "        self.hidden_sz = hidden_sz\n",
        "        self.W1 = nn.Parameter(torch.Tensor(hidden_sz, dim_in))\n",
        "        self.b1 = nn.Parameter(torch.Tensor(hidden_sz, 1))\n",
        "\n",
        "        # 1st layer\n",
        "        self.Uz1 = torch.nn.Parameter(torch.Tensor(hidden_sz, dim_in))\n",
        "        self.Wz1 = torch.nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
        "        self.bz1 = torch.nn.Parameter(torch.Tensor(hidden_sz, 1))\n",
        "\n",
        "        self.Ug1 = torch.nn.Parameter(torch.Tensor(hidden_sz, dim_in))\n",
        "        self.Wg1 = torch.nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
        "        self.bg1 = torch.nn.Parameter(torch.Tensor(hidden_sz, 1))\n",
        "\n",
        "        self.Ur1 = torch.nn.Parameter(torch.Tensor(hidden_sz, dim_in))\n",
        "        self.Wr1 = torch.nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
        "        self.br1 = torch.nn.Parameter(torch.Tensor(hidden_sz, 1))\n",
        "\n",
        "        self.Uh1 = torch.nn.Parameter(torch.Tensor(hidden_sz, dim_in))\n",
        "        self.Wh1 = torch.nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
        "        self.bh1 = torch.nn.Parameter(torch.Tensor(hidden_sz, 1))\n",
        "\n",
        "        # 2nd layer\n",
        "\n",
        "        self.Uz2 = torch.nn.Parameter(torch.Tensor(hidden_sz, dim_in))\n",
        "        self.Wz2 = torch.nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
        "        self.bz2 = torch.nn.Parameter(torch.Tensor(hidden_sz, 1))\n",
        "\n",
        "        self.Ug2 = torch.nn.Parameter(torch.Tensor(hidden_sz, dim_in))\n",
        "        self.Wg2 = torch.nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
        "        self.bg2 = torch.nn.Parameter(torch.Tensor(hidden_sz, 1))\n",
        "\n",
        "        self.Ur2 = torch.nn.Parameter(torch.Tensor(hidden_sz, dim_in))\n",
        "        self.Wr2 = torch.nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
        "        self.br2 = torch.nn.Parameter(torch.Tensor(hidden_sz, 1))\n",
        "\n",
        "        self.Uh2 = torch.nn.Parameter(torch.Tensor(hidden_sz, dim_in))\n",
        "        self.Wh2 = torch.nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
        "        self.bh2 = torch.nn.Parameter(torch.Tensor(hidden_sz, 1))\n",
        "\n",
        "        # 3rd layer\n",
        "\n",
        "\n",
        "        # output layer\n",
        "        self.W = torch.nn.Parameter(torch.Tensor(1, hidden_sz))\n",
        "        self.b = torch.nn.Parameter(torch.Tensor(1, 1))\n",
        "\n",
        "    def init_weights(self):\n",
        "        for p in self.parameters():\n",
        "            nn.init.xavier_uniform_(p.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "        Ones = torch.ones(self.hidden_sz, 1)\n",
        "\n",
        "        S1 = torch.tanh(torch.mm(self.W1, x) + self.b1)\n",
        "\n",
        "        # 1st layer\n",
        "        Z1 = torch.tanh(torch.mm(self.Uz1, x) + torch.mm(self.Wz1, S1) + self.bz1)\n",
        "        G1 = torch.tanh(torch.mm(self.Ug1, x) + torch.mm(self.Wg1, S1) + self.bg1)\n",
        "        R1 = torch.tanh(torch.mm(self.Ur1, x) + torch.mm(self.Wr1, S1) + self.br1)\n",
        "        # * is element - wise multiplication\n",
        "        H1 = torch.tanh(torch.mm(self.Uh1, x) + torch.mm(self.Wh1, S1 * R1) + self.bh1)\n",
        "\n",
        "        S2 = (Ones - G1) * H1 + Z1 * S1\n",
        "\n",
        "        # 2nd layer\n",
        "        Z2 = torch.tanh(torch.mm(self.Uz2, x) + torch.mm(self.Wz2, S2) + self.bz2)\n",
        "        G2 = torch.tanh(torch.mm(self.Ug2, x) + torch.mm(self.Wg2, S2) + self.bg2)\n",
        "        R2 = torch.tanh(torch.mm(self.Ur2, x) + torch.mm(self.Wr2, S2) + self.br2)\n",
        "        H2 = torch.tanh(torch.mm(self.Uh2, x) + torch.mm(self.Wh2, S2 * R2) + self.bh2)\n",
        "\n",
        "        S3 = (Ones - G2) * H2 + Z2 * S2\n",
        "\n",
        "\n",
        "        # output layer\n",
        "        out = torch.mm(self.W, S3) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def save(self, file):\n",
        "        torch.save(self, file)\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    total_param = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            num_param = np.prod(param.size())\n",
        "            if param.dim() > 1:\n",
        "                print(name, ':', 'x'.join(str(x) for x in list(param.size())), '=', num_param)\n",
        "            else:\n",
        "                print(name, ':', num_param)\n",
        "            total_param += num_param\n",
        "    return total_param\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAtQGKCMtp-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def laplacian(f, input_vector):\n",
        "    gradient = grad(f, input_vector, create_graph=True)[0]\n",
        "    ux = gradient.take(torch.tensor([0]))\n",
        "    uxx = grad(ux, input_vector, create_graph=True)[0].take(torch.tensor([0]))\n",
        "    uy = gradient.take(torch.tensor([1]))\n",
        "    uyy = grad(uy, input_vector, create_graph=True)[0].take(torch.tensor([1]))\n",
        "    return uxx + uyy\n",
        "\n",
        "\n",
        "def boundary_condition(input):\n",
        "    return 0\n",
        "\n",
        "\n",
        "# -laplace(u) = f\n",
        "def right_hand_side(input):\n",
        "    return 2*(math.pi)**2*math.sin(math.pi*input.take(torch.tensor([0])))*math.sin(math.pi*input.take(torch.tensor([1])))\n",
        "\n",
        "\n",
        "def exact_solution(input):\n",
        "    return math.sin(math.pi*input.take(torch.tensor([0])))*math.sin(math.pi*input.take(torch.tensor([1])))\n",
        "\n",
        "\n",
        "def random_data_points(batch_size):\n",
        "    Omegapoints = []\n",
        "    boundary_points = []\n",
        "    for i in range(batch_size):\n",
        "        Omegapoints.append([random.uniform(0, 1), random.uniform(0, 1)])\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        random_point = [random.uniform(0, 1), random.uniform(0, 1)]\n",
        "        random_index = random.randint(0,1)\n",
        "        random_value = random.randint(0,1)\n",
        "        random_point[random_index] = random_value\n",
        "        boundary_points.append(random_point)\n",
        "    return Omegapoints, boundary_points\n",
        "\n",
        "\n",
        "def batch_loss(net, datapoints):\n",
        "    G1 = G2 = 0\n",
        "    Omegapoints, boundary_points = datapoints\n",
        "    for Omegapoint in Omegapoints:\n",
        "        Omegapoint_input = Variable(torch.Tensor(Omegapoint).resize_(2, 1), requires_grad=True)\n",
        "        Omegapoint_output = net(Omegapoint_input)\n",
        "        G1 += (- laplacian(Omegapoint_output, Omegapoint_input) - right_hand_side(Omegapoint_input)) ** 2\n",
        "\n",
        "    for boundary_point in boundary_points:\n",
        "        boundary_point_input = Variable(torch.Tensor(boundary_point).resize_(2, 1), requires_grad=True)\n",
        "        boundary_point_output = net(boundary_point_input)\n",
        "        G2 += (boundary_point_output - boundary_condition(boundary_point_input))**2\n",
        "\n",
        "    G1 = G1 / len(Omegapoints)\n",
        "    G2 = G2 / len(boundary_points)\n",
        "    return G1 + G2\n",
        "\n",
        "\n",
        "def plot_estimation_and_exact_solution(file_name):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    x = y = np.arange(0, 3, 0.05)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    condition = X ** 2 + Y ** 2 <= 6\n",
        "    zs = np.array([net(torch.tensor([x, y]).resize_(2,1)).item() for x, y in zip(np.ravel(X), np.ravel(Y))])\n",
        "    Z = zs.reshape(X.shape)\n",
        "\n",
        "    zs = np.array([exact_solution_scalar_value(x, y) for x, y in zip(np.ravel(X), np.ravel(Y))])\n",
        "    Z1 = zs.reshape(X.shape)\n",
        "\n",
        "    ax.plot_surface(X, Y, Z, color='red')\n",
        "    ax.plot_surface(X, Y, Z1, color='blue')\n",
        "\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_zlabel('z')\n",
        "\n",
        "    # plt.show()\n",
        "    plt.savefig(file_name)\n",
        "    plt.close(\"all\")\n",
        "\n",
        "\n",
        "def random_points_test():\n",
        "    points = []\n",
        "    for i in range(2000):\n",
        "        points.append([random.uniform(0, 1), random.uniform(0, 1)])\n",
        "    return points"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DuAKxfQtwiB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "935bcdb3-7c53-4272-8887-374d19bdeb14"
      },
      "source": [
        "net = PDENetLight(2, 10)\n",
        "net.init_weights()\n",
        "\n",
        "# plot_estimation_and_exact_solution('./result/testnoBatch/Initial.png')\n",
        "l2_errors = []\n",
        "losses = []\n",
        "\n",
        "# net = torch.load('./model3')\n",
        "torch.set_printoptions(precision=10)\n",
        "learning_rate = 0.001\n",
        "iterations_count = 0\n",
        "start_training = time.time()\n",
        "for i in range(200):\n",
        "    print('Iteration number: ' + str(i + 1))\n",
        "    sample = random_data_points(1000)  # sample space time point\n",
        "    start_ite = time.time()\n",
        "    # for j in range(4):\n",
        "    #     # print('Batch iteration number: ' + str(j + 1))\n",
        "    #     net.zero_grad()\n",
        "    #     square_error = batch_loss(net, sample)  # calculate square error loss\n",
        "    #     square_error.backward()  # calculate gradient of square loss w.r.t the parameters\n",
        "    #     print('Batch loss: ' + str(square_error))\n",
        "    #     for param in net.parameters():\n",
        "    #         param.data -= learning_rate*param.grad.data\n",
        "    #     if j == 0:\n",
        "    #         losses.append(square_error.item())\n",
        "    net.zero_grad()\n",
        "    square_error = batch_loss(net, sample)  # calculate square error loss\n",
        "    square_error.backward()  # calculate gradient of square loss w.r.t the parameters\n",
        "    print('Batch loss: ' + str(square_error))\n",
        "    for param in net.parameters():\n",
        "        param.data -= learning_rate * param.grad.data\n",
        "    losses.append(square_error.item())\n",
        "    end_ite = time.time()\n",
        "    ite_time = end_ite - start_ite\n",
        "    total_time = end_ite - start_training\n",
        "    print('This iteration took ' + str(ite_time) + ' seconds')\n",
        "    print('Total training time elapsed ' + str(total_time/60) + ' minutes')\n",
        "    L2_error = 0\n",
        "    for point in random_points_test():\n",
        "        point_input = torch.Tensor(point).resize_(2,1)\n",
        "        L2_error += (net(point_input) - exact_solution(point_input))**2\n",
        "    L2_error /= 121\n",
        "    l2_errors.append(L2_error.item())\n",
        "\n",
        "    print('L2error = ' + str(L2_error))\n",
        "    # print('Loss function = ' + str(square_error))\n",
        "\n",
        "    print('Finished iteration number ' + str(i + 1) + ', saving model')\n",
        "    model_path = './result/testnoBatch/model' + str(i+1) + 'th_ite'\n",
        "   # net.save(model_path)\n",
        "    # fig_path = './result/testnoBatch/' + str(i+1) + 'th_ite.png'\n",
        "    # plot_estimation_and_exact_solution(fig_path)\n",
        "\n",
        "print('Plotting and saving convergence history')\n",
        "epochs = [i + 1 for i in range(200)]\n",
        "plt.xticks(np.arange(0, 201, 20))\n",
        "\n",
        "plt.figure(0)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(epochs, losses, color = 'blue')\n",
        "# plt.savefig('./result/testnoBatch/loss.png')\n",
        "\n",
        "# with open('./result/testnoBatch/batchloss.txt', 'w+') as f:\n",
        "#     for loss in losses:\n",
        "#         f.write(str(loss) + '\\n')\n",
        "\n",
        "plt.figure(1)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('L2error')\n",
        "\n",
        "plt.plot(epochs, l2_errors, color = 'blue')\n",
        "# plt.savefig('./result/testnoBatch/l2error.png')\n",
        "# with open('./result/testnoBatch/L2error.txt', 'w+') as f:\n",
        "#     for l2_error in l2_errors:\n",
        "#         f.write(str(l2_error) + '\\n')\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration number: 1\n",
            "Batch loss: tensor([[85.9664382935]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.514628410339355 seconds\n",
            "Total training time elapsed 0.1919846256573995 minutes\n",
            "L2error = tensor([[7.7562279701]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 1, saving model\n",
            "Iteration number: 2\n",
            "Batch loss: tensor([[58.9175834656]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.421607971191406 seconds\n",
            "Total training time elapsed 0.3997388521830241 minutes\n",
            "L2error = tensor([[5.1169109344]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 2, saving model\n",
            "Iteration number: 3\n",
            "Batch loss: tensor([[42.8937377930]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.459325075149536 seconds\n",
            "Total training time elapsed 0.6081506331761678 minutes\n",
            "L2error = tensor([[3.6003084183]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 3, saving model\n",
            "Iteration number: 4\n",
            "Batch loss: tensor([[27.6743450165]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.476379871368408 seconds\n",
            "Total training time elapsed 0.8175426602363587 minutes\n",
            "L2error = tensor([[3.1697361469]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 4, saving model\n",
            "Iteration number: 5\n",
            "Batch loss: tensor([[18.2081756592]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.555466175079346 seconds\n",
            "Total training time elapsed 1.0279003739356996 minutes\n",
            "L2error = tensor([[2.9228582382]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 5, saving model\n",
            "Iteration number: 6\n",
            "Batch loss: tensor([[14.5167446136]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.469271659851074 seconds\n",
            "Total training time elapsed 1.2560138384501138 minutes\n",
            "L2error = tensor([[2.9135708809]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 6, saving model\n",
            "Iteration number: 7\n",
            "Batch loss: tensor([[11.0719366074]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.418814420700073 seconds\n",
            "Total training time elapsed 1.463866110642751 minutes\n",
            "L2error = tensor([[3.0079593658]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 7, saving model\n",
            "Iteration number: 8\n",
            "Batch loss: tensor([[9.5917539597]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.492494344711304 seconds\n",
            "Total training time elapsed 1.6728195230166116 minutes\n",
            "L2error = tensor([[2.9640381336]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 8, saving model\n",
            "Iteration number: 9\n",
            "Batch loss: tensor([[7.9141469002]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.397902011871338 seconds\n",
            "Total training time elapsed 1.8803509950637818 minutes\n",
            "L2error = tensor([[3.2217228413]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 9, saving model\n",
            "Iteration number: 10\n",
            "Batch loss: tensor([[6.9487290382]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.493557453155518 seconds\n",
            "Total training time elapsed 2.089141837755839 minutes\n",
            "L2error = tensor([[3.2256002426]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 10, saving model\n",
            "Iteration number: 11\n",
            "Batch loss: tensor([[6.1146821976]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.35004448890686 seconds\n",
            "Total training time elapsed 2.2958193182945252 minutes\n",
            "L2error = tensor([[3.1227521896]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 11, saving model\n",
            "Iteration number: 12\n",
            "Batch loss: tensor([[4.8917760849]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.492201328277588 seconds\n",
            "Total training time elapsed 2.504598085085551 minutes\n",
            "L2error = tensor([[3.4693460464]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 12, saving model\n",
            "Iteration number: 13\n",
            "Batch loss: tensor([[4.3394236565]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.516723871231079 seconds\n",
            "Total training time elapsed 2.7138201236724853 minutes\n",
            "L2error = tensor([[3.2456853390]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 13, saving model\n",
            "Iteration number: 14\n",
            "Batch loss: tensor([[3.8419747353]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.499861478805542 seconds\n",
            "Total training time elapsed 2.922407909234365 minutes\n",
            "L2error = tensor([[3.7536935806]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 14, saving model\n",
            "Iteration number: 15\n",
            "Batch loss: tensor([[3.4060063362]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.076675415039062 seconds\n",
            "Total training time elapsed 3.142379911740621 minutes\n",
            "L2error = tensor([[3.6788671017]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 15, saving model\n",
            "Iteration number: 16\n",
            "Batch loss: tensor([[3.1181590557]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.393188714981079 seconds\n",
            "Total training time elapsed 3.3491469780604044 minutes\n",
            "L2error = tensor([[3.8912622929]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 16, saving model\n",
            "Iteration number: 17\n",
            "Batch loss: tensor([[2.7456417084]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.377122640609741 seconds\n",
            "Total training time elapsed 3.5559083024660745 minutes\n",
            "L2error = tensor([[4.3053417206]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 17, saving model\n",
            "Iteration number: 18\n",
            "Batch loss: tensor([[2.7237687111]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.589819431304932 seconds\n",
            "Total training time elapsed 3.767060732841492 minutes\n",
            "L2error = tensor([[3.9732360840]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 18, saving model\n",
            "Iteration number: 19\n",
            "Batch loss: tensor([[2.2852840424]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.617968797683716 seconds\n",
            "Total training time elapsed 3.977962617079417 minutes\n",
            "L2error = tensor([[4.2164125443]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 19, saving model\n",
            "Iteration number: 20\n",
            "Batch loss: tensor([[2.1084902287]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.454308271408081 seconds\n",
            "Total training time elapsed 4.186196112632752 minutes\n",
            "L2error = tensor([[3.8272707462]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 20, saving model\n",
            "Iteration number: 21\n",
            "Batch loss: tensor([[2.0963735580]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.463712215423584 seconds\n",
            "Total training time elapsed 4.3946528951327 minutes\n",
            "L2error = tensor([[5.0027742386]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 21, saving model\n",
            "Iteration number: 22\n",
            "Batch loss: tensor([[2.0425367355]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.243542432785034 seconds\n",
            "Total training time elapsed 4.5993553638458256 minutes\n",
            "L2error = tensor([[3.8841726780]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 22, saving model\n",
            "Iteration number: 23\n",
            "Batch loss: tensor([[1.9214675426]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.59772253036499 seconds\n",
            "Total training time elapsed 4.810210760434469 minutes\n",
            "L2error = tensor([[4.4246597290]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 23, saving model\n",
            "Iteration number: 24\n",
            "Batch loss: tensor([[1.7167007923]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.433596134185791 seconds\n",
            "Total training time elapsed 5.019173189004262 minutes\n",
            "L2error = tensor([[3.7526516914]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 24, saving model\n",
            "Iteration number: 25\n",
            "Batch loss: tensor([[1.7209687233]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.459308624267578 seconds\n",
            "Total training time elapsed 5.227530992031097 minutes\n",
            "L2error = tensor([[4.5438027382]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 25, saving model\n",
            "Iteration number: 26\n",
            "Batch loss: tensor([[1.8205194473]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.450878620147705 seconds\n",
            "Total training time elapsed 5.435926640033722 minutes\n",
            "L2error = tensor([[3.4021754265]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 26, saving model\n",
            "Iteration number: 27\n",
            "Batch loss: tensor([[1.7535326481]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.521365642547607 seconds\n",
            "Total training time elapsed 5.645202664534251 minutes\n",
            "L2error = tensor([[4.8294210434]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 27, saving model\n",
            "Iteration number: 28\n",
            "Batch loss: tensor([[1.7301710844]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.573001384735107 seconds\n",
            "Total training time elapsed 5.855833733081818 minutes\n",
            "L2error = tensor([[2.9261982441]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 28, saving model\n",
            "Iteration number: 29\n",
            "Batch loss: tensor([[1.8333137035]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.662357330322266 seconds\n",
            "Total training time elapsed 6.0678051273028055 minutes\n",
            "L2error = tensor([[5.1662006378]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 29, saving model\n",
            "Iteration number: 30\n",
            "Batch loss: tensor([[2.0808174610]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.584026336669922 seconds\n",
            "Total training time elapsed 6.278240931034088 minutes\n",
            "L2error = tensor([[2.2134523392]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 30, saving model\n",
            "Iteration number: 31\n",
            "Batch loss: tensor([[2.7543365955]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.112241983413696 seconds\n",
            "Total training time elapsed 6.498990194002787 minutes\n",
            "L2error = tensor([[5.0169258118]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 31, saving model\n",
            "Iteration number: 32\n",
            "Batch loss: tensor([[2.5481376648]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.484946727752686 seconds\n",
            "Total training time elapsed 6.708045033613841 minutes\n",
            "L2error = tensor([[2.0449838638]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 32, saving model\n",
            "Iteration number: 33\n",
            "Batch loss: tensor([[3.3383014202]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.615043640136719 seconds\n",
            "Total training time elapsed 6.91883712609609 minutes\n",
            "L2error = tensor([[5.4793181419]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 33, saving model\n",
            "Iteration number: 34\n",
            "Batch loss: tensor([[3.9790451527]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.692076206207275 seconds\n",
            "Total training time elapsed 7.131271183490753 minutes\n",
            "L2error = tensor([[1.2284041643]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 34, saving model\n",
            "Iteration number: 35\n",
            "Batch loss: tensor([[5.9491090775]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.608075380325317 seconds\n",
            "Total training time elapsed 7.342398166656494 minutes\n",
            "L2error = tensor([[5.1120882034]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 35, saving model\n",
            "Iteration number: 36\n",
            "Batch loss: tensor([[3.6578660011]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.52304720878601 seconds\n",
            "Total training time elapsed 7.551527734597524 minutes\n",
            "L2error = tensor([[1.1176418066]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 36, saving model\n",
            "Iteration number: 37\n",
            "Batch loss: tensor([[4.5108175278]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.727731704711914 seconds\n",
            "Total training time elapsed 7.764338250954946 minutes\n",
            "L2error = tensor([[4.5037574768]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 37, saving model\n",
            "Iteration number: 38\n",
            "Batch loss: tensor([[3.1778264046]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.725056648254395 seconds\n",
            "Total training time elapsed 7.977248120307922 minutes\n",
            "L2error = tensor([[1.2044227123]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 38, saving model\n",
            "Iteration number: 39\n",
            "Batch loss: tensor([[3.9278683662]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.285278081893921 seconds\n",
            "Total training time elapsed 8.199522721767426 minutes\n",
            "L2error = tensor([[4.6491007805]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 39, saving model\n",
            "Iteration number: 40\n",
            "Batch loss: tensor([[3.4391713142]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.780240774154663 seconds\n",
            "Total training time elapsed 8.413457703590392 minutes\n",
            "L2error = tensor([[0.9698837996]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 40, saving model\n",
            "Iteration number: 41\n",
            "Batch loss: tensor([[4.5234260559]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.420912742614746 seconds\n",
            "Total training time elapsed 8.621226354440052 minutes\n",
            "L2error = tensor([[4.1353840828]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 41, saving model\n",
            "Iteration number: 42\n",
            "Batch loss: tensor([[3.4001774788]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.45153260231018 seconds\n",
            "Total training time elapsed 8.83086065451304 minutes\n",
            "L2error = tensor([[0.8938184381]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 42, saving model\n",
            "Iteration number: 43\n",
            "Batch loss: tensor([[3.9629209042]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.361048221588135 seconds\n",
            "Total training time elapsed 9.037781639893849 minutes\n",
            "L2error = tensor([[3.6261773109]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 43, saving model\n",
            "Iteration number: 44\n",
            "Batch loss: tensor([[2.9200303555]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.384037971496582 seconds\n",
            "Total training time elapsed 9.245077590147654 minutes\n",
            "L2error = tensor([[0.8529376984]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 44, saving model\n",
            "Iteration number: 45\n",
            "Batch loss: tensor([[3.9731881618]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.516079425811768 seconds\n",
            "Total training time elapsed 9.454169619083405 minutes\n",
            "L2error = tensor([[3.7158091068]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 45, saving model\n",
            "Iteration number: 46\n",
            "Batch loss: tensor([[2.9279570580]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.500791311264038 seconds\n",
            "Total training time elapsed 9.66341526110967 minutes\n",
            "L2error = tensor([[0.8699980974]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 46, saving model\n",
            "Iteration number: 47\n",
            "Batch loss: tensor([[3.6185719967]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.446252584457397 seconds\n",
            "Total training time elapsed 9.8712912718455 minutes\n",
            "L2error = tensor([[3.6837668419]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 47, saving model\n",
            "Iteration number: 48\n",
            "Batch loss: tensor([[3.1792762280]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.4559965133667 seconds\n",
            "Total training time elapsed 10.079610578219096 minutes\n",
            "L2error = tensor([[0.7988617420]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 48, saving model\n",
            "Iteration number: 49\n",
            "Batch loss: tensor([[3.2001879215]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.357208251953125 seconds\n",
            "Total training time elapsed 10.286223169167837 minutes\n",
            "L2error = tensor([[2.9646079540]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 49, saving model\n",
            "Iteration number: 50\n",
            "Batch loss: tensor([[2.1020641327]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.39015531539917 seconds\n",
            "Total training time elapsed 10.493577297528585 minutes\n",
            "L2error = tensor([[0.8224105835]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 50, saving model\n",
            "Iteration number: 51\n",
            "Batch loss: tensor([[2.5558593273]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.611899375915527 seconds\n",
            "Total training time elapsed 10.704333305358887 minutes\n",
            "L2error = tensor([[2.6074666977]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 51, saving model\n",
            "Iteration number: 52\n",
            "Batch loss: tensor([[1.8563770056]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.588492393493652 seconds\n",
            "Total training time elapsed 10.914680163065592 minutes\n",
            "L2error = tensor([[0.8775006533]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 52, saving model\n",
            "Iteration number: 53\n",
            "Batch loss: tensor([[2.0831949711]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.437054634094238 seconds\n",
            "Total training time elapsed 11.122547086079916 minutes\n",
            "L2error = tensor([[2.5391750336]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 53, saving model\n",
            "Iteration number: 54\n",
            "Batch loss: tensor([[2.0898540020]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.320729970932007 seconds\n",
            "Total training time elapsed 11.328225151697795 minutes\n",
            "L2error = tensor([[0.7664635777]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 54, saving model\n",
            "Iteration number: 55\n",
            "Batch loss: tensor([[2.3634877205]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.20402216911316 seconds\n",
            "Total training time elapsed 11.54881416161855 minutes\n",
            "L2error = tensor([[2.5751855373]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 55, saving model\n",
            "Iteration number: 56\n",
            "Batch loss: tensor([[2.1548912525]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.70781922340393 seconds\n",
            "Total training time elapsed 11.779151844978333 minutes\n",
            "L2error = tensor([[0.6735275388]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 56, saving model\n",
            "Iteration number: 57\n",
            "Batch loss: tensor([[2.5646939278]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.519234657287598 seconds\n",
            "Total training time elapsed 11.98868538538615 minutes\n",
            "L2error = tensor([[2.4889271259]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 57, saving model\n",
            "Iteration number: 58\n",
            "Batch loss: tensor([[2.1931009293]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.36881971359253 seconds\n",
            "Total training time elapsed 12.195462103684743 minutes\n",
            "L2error = tensor([[0.6375374198]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 58, saving model\n",
            "Iteration number: 59\n",
            "Batch loss: tensor([[2.4330089092]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.2708899974823 seconds\n",
            "Total training time elapsed 12.400290981928508 minutes\n",
            "L2error = tensor([[2.1412713528]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 59, saving model\n",
            "Iteration number: 60\n",
            "Batch loss: tensor([[1.6314488649]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.275458574295044 seconds\n",
            "Total training time elapsed 12.605295423666636 minutes\n",
            "L2error = tensor([[0.6968412995]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 60, saving model\n",
            "Iteration number: 61\n",
            "Batch loss: tensor([[1.7804220915]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.467553615570068 seconds\n",
            "Total training time elapsed 12.813533274332682 minutes\n",
            "L2error = tensor([[2.0632135868]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 61, saving model\n",
            "Iteration number: 62\n",
            "Batch loss: tensor([[1.5951507092]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.32318639755249 seconds\n",
            "Total training time elapsed 13.01939725081126 minutes\n",
            "L2error = tensor([[0.6909686923]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 62, saving model\n",
            "Iteration number: 63\n",
            "Batch loss: tensor([[1.4530463219]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.514152765274048 seconds\n",
            "Total training time elapsed 13.228519674142202 minutes\n",
            "L2error = tensor([[1.6246148348]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 63, saving model\n",
            "Iteration number: 64\n",
            "Batch loss: tensor([[1.2090198994]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.443653345108032 seconds\n",
            "Total training time elapsed 13.437694076697031 minutes\n",
            "L2error = tensor([[0.7206040621]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 64, saving model\n",
            "Iteration number: 65\n",
            "Batch loss: tensor([[1.2762944698]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.324187517166138 seconds\n",
            "Total training time elapsed 13.643298017978669 minutes\n",
            "L2error = tensor([[1.6317504644]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 65, saving model\n",
            "Iteration number: 66\n",
            "Batch loss: tensor([[1.2711501122]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.409141302108765 seconds\n",
            "Total training time elapsed 13.850773819287618 minutes\n",
            "L2error = tensor([[0.5939787030]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 66, saving model\n",
            "Iteration number: 67\n",
            "Batch loss: tensor([[1.4394139051]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.338851928710938 seconds\n",
            "Total training time elapsed 14.05741533835729 minutes\n",
            "L2error = tensor([[1.5948963165]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 67, saving model\n",
            "Iteration number: 68\n",
            "Batch loss: tensor([[1.3891918659]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.428288698196411 seconds\n",
            "Total training time elapsed 14.265259313583375 minutes\n",
            "L2error = tensor([[0.5665796399]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 68, saving model\n",
            "Iteration number: 69\n",
            "Batch loss: tensor([[1.3772320747]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.31959319114685 seconds\n",
            "Total training time elapsed 14.47081876595815 minutes\n",
            "L2error = tensor([[1.3943823576]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 69, saving model\n",
            "Iteration number: 70\n",
            "Batch loss: tensor([[1.1398972273]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.285887002944946 seconds\n",
            "Total training time elapsed 14.675862121582032 minutes\n",
            "L2error = tensor([[0.5449679494]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 70, saving model\n",
            "Iteration number: 71\n",
            "Batch loss: tensor([[1.2135670185]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.31359052658081 seconds\n",
            "Total training time elapsed 14.881775311628978 minutes\n",
            "L2error = tensor([[1.4368103743]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 71, saving model\n",
            "Iteration number: 72\n",
            "Batch loss: tensor([[1.1610563993]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.275172710418701 seconds\n",
            "Total training time elapsed 15.086577053864797 minutes\n",
            "L2error = tensor([[0.4408636689]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 72, saving model\n",
            "Iteration number: 73\n",
            "Batch loss: tensor([[1.2708734274]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.30823040008545 seconds\n",
            "Total training time elapsed 15.292280141512553 minutes\n",
            "L2error = tensor([[1.2272809744]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 73, saving model\n",
            "Iteration number: 74\n",
            "Batch loss: tensor([[0.9620413780]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.369579315185547 seconds\n",
            "Total training time elapsed 15.498893996079763 minutes\n",
            "L2error = tensor([[0.5169560909]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 74, saving model\n",
            "Iteration number: 75\n",
            "Batch loss: tensor([[1.0714337826]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.325876712799072 seconds\n",
            "Total training time elapsed 15.704725567499796 minutes\n",
            "L2error = tensor([[1.1459116936]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 75, saving model\n",
            "Iteration number: 76\n",
            "Batch loss: tensor([[1.0575119257]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.33578610420227 seconds\n",
            "Total training time elapsed 15.9109405875206 minutes\n",
            "L2error = tensor([[0.4817639887]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 76, saving model\n",
            "Iteration number: 77\n",
            "Batch loss: tensor([[1.1691745520]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.400080919265747 seconds\n",
            "Total training time elapsed 16.117925878365835 minutes\n",
            "L2error = tensor([[1.1566803455]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 77, saving model\n",
            "Iteration number: 78\n",
            "Batch loss: tensor([[1.0905907154]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.235230445861816 seconds\n",
            "Total training time elapsed 16.322085837523144 minutes\n",
            "L2error = tensor([[0.4067956805]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 78, saving model\n",
            "Iteration number: 79\n",
            "Batch loss: tensor([[1.2719128132]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.39662480354309 seconds\n",
            "Total training time elapsed 16.529502308368684 minutes\n",
            "L2error = tensor([[1.1455291510]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 79, saving model\n",
            "Iteration number: 80\n",
            "Batch loss: tensor([[1.1638739109]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.333715915679932 seconds\n",
            "Total training time elapsed 16.735634326934814 minutes\n",
            "L2error = tensor([[0.3667268455]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 80, saving model\n",
            "Iteration number: 81\n",
            "Batch loss: tensor([[1.3714233637]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.389387130737305 seconds\n",
            "Total training time elapsed 16.959264679749808 minutes\n",
            "L2error = tensor([[1.0946277380]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 81, saving model\n",
            "Iteration number: 82\n",
            "Batch loss: tensor([[1.2949275970]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.274831056594849 seconds\n",
            "Total training time elapsed 17.18166907628377 minutes\n",
            "L2error = tensor([[0.3138608634]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 82, saving model\n",
            "Iteration number: 83\n",
            "Batch loss: tensor([[1.5763099194]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.306333065032959 seconds\n",
            "Total training time elapsed 17.38697613875071 minutes\n",
            "L2error = tensor([[1.1706281900]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 83, saving model\n",
            "Iteration number: 84\n",
            "Batch loss: tensor([[1.2570402622]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.381663799285889 seconds\n",
            "Total training time elapsed 17.59387229681015 minutes\n",
            "L2error = tensor([[0.3057190776]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 84, saving model\n",
            "Iteration number: 85\n",
            "Batch loss: tensor([[1.5247511864]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.494404792785645 seconds\n",
            "Total training time elapsed 17.80295855998993 minutes\n",
            "L2error = tensor([[1.1055082083]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 85, saving model\n",
            "Iteration number: 86\n",
            "Batch loss: tensor([[1.4352509975]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.366582870483398 seconds\n",
            "Total training time elapsed 18.010160899162294 minutes\n",
            "L2error = tensor([[0.2833269238]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 86, saving model\n",
            "Iteration number: 87\n",
            "Batch loss: tensor([[1.6827342510]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.364707231521606 seconds\n",
            "Total training time elapsed 18.217909955978392 minutes\n",
            "L2error = tensor([[1.0029301643]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 87, saving model\n",
            "Iteration number: 88\n",
            "Batch loss: tensor([[1.2970033884]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.79155445098877 seconds\n",
            "Total training time elapsed 18.431963153680165 minutes\n",
            "L2error = tensor([[0.2875964642]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 88, saving model\n",
            "Iteration number: 89\n",
            "Batch loss: tensor([[1.5544848442]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.321102142333984 seconds\n",
            "Total training time elapsed 18.637547691663105 minutes\n",
            "L2error = tensor([[0.9908321500]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 89, saving model\n",
            "Iteration number: 90\n",
            "Batch loss: tensor([[1.2701777220]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.4690260887146 seconds\n",
            "Total training time elapsed 18.845975335439046 minutes\n",
            "L2error = tensor([[0.2821952105]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 90, saving model\n",
            "Iteration number: 91\n",
            "Batch loss: tensor([[1.4282783270]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.601658344268799 seconds\n",
            "Total training time elapsed 19.056922443707784 minutes\n",
            "L2error = tensor([[0.9802920818]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 91, saving model\n",
            "Iteration number: 92\n",
            "Batch loss: tensor([[1.4584749937]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.415790319442749 seconds\n",
            "Total training time elapsed 19.264371367295585 minutes\n",
            "L2error = tensor([[0.2635182440]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 92, saving model\n",
            "Iteration number: 93\n",
            "Batch loss: tensor([[1.6848040819]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.460379600524902 seconds\n",
            "Total training time elapsed 19.472736263275145 minutes\n",
            "L2error = tensor([[0.8395437002]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 93, saving model\n",
            "Iteration number: 94\n",
            "Batch loss: tensor([[1.3400406837]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.534987449645996 seconds\n",
            "Total training time elapsed 19.682276459534965 minutes\n",
            "L2error = tensor([[0.2591337860]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 94, saving model\n",
            "Iteration number: 95\n",
            "Batch loss: tensor([[1.4007799625]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.37095022201538 seconds\n",
            "Total training time elapsed 19.88965121905009 minutes\n",
            "L2error = tensor([[0.7402125597]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 95, saving model\n",
            "Iteration number: 96\n",
            "Batch loss: tensor([[0.9764980674]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.519715070724487 seconds\n",
            "Total training time elapsed 20.098896980285645 minutes\n",
            "L2error = tensor([[0.2686850727]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 96, saving model\n",
            "Iteration number: 97\n",
            "Batch loss: tensor([[1.0791858435]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.375861883163452 seconds\n",
            "Total training time elapsed 20.305787726243338 minutes\n",
            "L2error = tensor([[0.6574860215]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 97, saving model\n",
            "Iteration number: 98\n",
            "Batch loss: tensor([[0.9341269135]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.491538286209106 seconds\n",
            "Total training time elapsed 20.51460084517797 minutes\n",
            "L2error = tensor([[0.2886748612]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 98, saving model\n",
            "Iteration number: 99\n",
            "Batch loss: tensor([[0.9026991725]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.536297798156738 seconds\n",
            "Total training time elapsed 20.724485051631927 minutes\n",
            "L2error = tensor([[0.6257479191]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 99, saving model\n",
            "Iteration number: 100\n",
            "Batch loss: tensor([[0.8169806600]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.400649785995483 seconds\n",
            "Total training time elapsed 20.93173214594523 minutes\n",
            "L2error = tensor([[0.2554055154]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 100, saving model\n",
            "Iteration number: 101\n",
            "Batch loss: tensor([[0.7693057060]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.469635486602783 seconds\n",
            "Total training time elapsed 21.14069486061732 minutes\n",
            "L2error = tensor([[0.5168738365]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 101, saving model\n",
            "Iteration number: 102\n",
            "Batch loss: tensor([[0.6402226686]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.368936538696289 seconds\n",
            "Total training time elapsed 21.347371912002565 minutes\n",
            "L2error = tensor([[0.2950891256]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 102, saving model\n",
            "Iteration number: 103\n",
            "Batch loss: tensor([[0.6958424449]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.401159524917603 seconds\n",
            "Total training time elapsed 21.554769309361777 minutes\n",
            "L2error = tensor([[0.5285445452]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 103, saving model\n",
            "Iteration number: 104\n",
            "Batch loss: tensor([[0.6464063525]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.378331899642944 seconds\n",
            "Total training time elapsed 21.762159299850463 minutes\n",
            "L2error = tensor([[0.2658147812]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 104, saving model\n",
            "Iteration number: 105\n",
            "Batch loss: tensor([[0.7141882777]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.430190563201904 seconds\n",
            "Total training time elapsed 21.969991437594096 minutes\n",
            "L2error = tensor([[0.4575358331]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 105, saving model\n",
            "Iteration number: 106\n",
            "Batch loss: tensor([[0.5906613469]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.347016334533691 seconds\n",
            "Total training time elapsed 22.17644611199697 minutes\n",
            "L2error = tensor([[0.2796068788]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 106, saving model\n",
            "Iteration number: 107\n",
            "Batch loss: tensor([[0.6376585960]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.509815692901611 seconds\n",
            "Total training time elapsed 22.40343539317449 minutes\n",
            "L2error = tensor([[0.4376735985]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 107, saving model\n",
            "Iteration number: 108\n",
            "Batch loss: tensor([[0.5995750427]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.376559019088745 seconds\n",
            "Total training time elapsed 22.610580865542094 minutes\n",
            "L2error = tensor([[0.2637854815]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 108, saving model\n",
            "Iteration number: 109\n",
            "Batch loss: tensor([[0.6125648022]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.408109903335571 seconds\n",
            "Total training time elapsed 22.81827085018158 minutes\n",
            "L2error = tensor([[0.4656622708]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 109, saving model\n",
            "Iteration number: 110\n",
            "Batch loss: tensor([[0.7150096297]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.29549527168274 seconds\n",
            "Total training time elapsed 23.024377342065176 minutes\n",
            "L2error = tensor([[0.2481394708]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 110, saving model\n",
            "Iteration number: 111\n",
            "Batch loss: tensor([[0.8157856464]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.342373609542847 seconds\n",
            "Total training time elapsed 23.230917302767434 minutes\n",
            "L2error = tensor([[0.5216743350]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 111, saving model\n",
            "Iteration number: 112\n",
            "Batch loss: tensor([[0.8067219257]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.457303524017334 seconds\n",
            "Total training time elapsed 23.43905050754547 minutes\n",
            "L2error = tensor([[0.2428475469]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 112, saving model\n",
            "Iteration number: 113\n",
            "Batch loss: tensor([[0.9026048183]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.098758935928345 seconds\n",
            "Total training time elapsed 23.659344271818796 minutes\n",
            "L2error = tensor([[0.5274463296]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 113, saving model\n",
            "Iteration number: 114\n",
            "Batch loss: tensor([[0.8986650109]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.48708176612854 seconds\n",
            "Total training time elapsed 23.869163290659586 minutes\n",
            "L2error = tensor([[0.2367767245]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 114, saving model\n",
            "Iteration number: 115\n",
            "Batch loss: tensor([[1.0779601336]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.309751033782959 seconds\n",
            "Total training time elapsed 24.07514799038569 minutes\n",
            "L2error = tensor([[0.4990786016]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 115, saving model\n",
            "Iteration number: 116\n",
            "Batch loss: tensor([[0.9437892437]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.410425901412964 seconds\n",
            "Total training time elapsed 24.282681091626486 minutes\n",
            "L2error = tensor([[0.2460909188]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 116, saving model\n",
            "Iteration number: 117\n",
            "Batch loss: tensor([[0.9968254566]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.367543697357178 seconds\n",
            "Total training time elapsed 24.489619743824004 minutes\n",
            "L2error = tensor([[0.4876893163]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 117, saving model\n",
            "Iteration number: 118\n",
            "Batch loss: tensor([[0.9397807121]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.437251329421997 seconds\n",
            "Total training time elapsed 24.69756999810537 minutes\n",
            "L2error = tensor([[0.2500116527]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 118, saving model\n",
            "Iteration number: 119\n",
            "Batch loss: tensor([[1.0173549652]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.399744272232056 seconds\n",
            "Total training time elapsed 24.905297243595122 minutes\n",
            "L2error = tensor([[0.4649784267]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 119, saving model\n",
            "Iteration number: 120\n",
            "Batch loss: tensor([[0.7533224225]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.403840065002441 seconds\n",
            "Total training time elapsed 25.11281588872274 minutes\n",
            "L2error = tensor([[0.2447844744]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 120, saving model\n",
            "Iteration number: 121\n",
            "Batch loss: tensor([[0.7364140749]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.283052921295166 seconds\n",
            "Total training time elapsed 25.317919234434765 minutes\n",
            "L2error = tensor([[0.4223836958]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 121, saving model\n",
            "Iteration number: 122\n",
            "Batch loss: tensor([[0.6591430902]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.310232400894165 seconds\n",
            "Total training time elapsed 25.523632029692333 minutes\n",
            "L2error = tensor([[0.2510015666]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 122, saving model\n",
            "Iteration number: 123\n",
            "Batch loss: tensor([[0.7336840630]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.417169570922852 seconds\n",
            "Total training time elapsed 25.731712241967518 minutes\n",
            "L2error = tensor([[0.4466814399]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 123, saving model\n",
            "Iteration number: 124\n",
            "Batch loss: tensor([[0.8171218634]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.698679685592651 seconds\n",
            "Total training time elapsed 25.944711899757387 minutes\n",
            "L2error = tensor([[0.2412248999]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 124, saving model\n",
            "Iteration number: 125\n",
            "Batch loss: tensor([[0.9565239549]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.443256855010986 seconds\n",
            "Total training time elapsed 26.153111056486765 minutes\n",
            "L2error = tensor([[0.4254538417]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 125, saving model\n",
            "Iteration number: 126\n",
            "Batch loss: tensor([[0.8088350892]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.4551420211792 seconds\n",
            "Total training time elapsed 26.361228926976523 minutes\n",
            "L2error = tensor([[0.2544468045]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 126, saving model\n",
            "Iteration number: 127\n",
            "Batch loss: tensor([[0.9541666508]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.362524271011353 seconds\n",
            "Total training time elapsed 26.56813231309255 minutes\n",
            "L2error = tensor([[0.4756956100]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 127, saving model\n",
            "Iteration number: 128\n",
            "Batch loss: tensor([[0.9719209671]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.402217626571655 seconds\n",
            "Total training time elapsed 26.77551960150401 minutes\n",
            "L2error = tensor([[0.2438362986]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 128, saving model\n",
            "Iteration number: 129\n",
            "Batch loss: tensor([[1.0615854263]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.450845956802368 seconds\n",
            "Total training time elapsed 26.983793926239013 minutes\n",
            "L2error = tensor([[0.4309878945]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 129, saving model\n",
            "Iteration number: 130\n",
            "Batch loss: tensor([[0.9051052332]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.461382865905762 seconds\n",
            "Total training time elapsed 27.192153159777323 minutes\n",
            "L2error = tensor([[0.2359981984]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 130, saving model\n",
            "Iteration number: 131\n",
            "Batch loss: tensor([[1.0186629295]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.611305475234985 seconds\n",
            "Total training time elapsed 27.403039610385896 minutes\n",
            "L2error = tensor([[0.4243981540]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 131, saving model\n",
            "Iteration number: 132\n",
            "Batch loss: tensor([[0.9436926246]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.47079086303711 seconds\n",
            "Total training time elapsed 27.629198408126832 minutes\n",
            "L2error = tensor([[0.2449673563]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 132, saving model\n",
            "Iteration number: 133\n",
            "Batch loss: tensor([[1.0558087826]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.898391008377075 seconds\n",
            "Total training time elapsed 27.861880485216776 minutes\n",
            "L2error = tensor([[0.4336589575]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 133, saving model\n",
            "Iteration number: 134\n",
            "Batch loss: tensor([[1.1060696840]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.448787450790405 seconds\n",
            "Total training time elapsed 28.070346788565317 minutes\n",
            "L2error = tensor([[0.3036576211]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 134, saving model\n",
            "Iteration number: 135\n",
            "Batch loss: tensor([[1.3085951805]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.394227981567383 seconds\n",
            "Total training time elapsed 28.27758551041285 minutes\n",
            "L2error = tensor([[0.3943418264]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 135, saving model\n",
            "Iteration number: 136\n",
            "Batch loss: tensor([[0.8810922503]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.418878078460693 seconds\n",
            "Total training time elapsed 28.48544064362844 minutes\n",
            "L2error = tensor([[0.2894034088]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 136, saving model\n",
            "Iteration number: 137\n",
            "Batch loss: tensor([[0.9161707163]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.050919771194458 seconds\n",
            "Total training time elapsed 28.703518728415172 minutes\n",
            "L2error = tensor([[0.4045083225]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 137, saving model\n",
            "Iteration number: 138\n",
            "Batch loss: tensor([[0.8253380060]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.400065898895264 seconds\n",
            "Total training time elapsed 28.910834495226542 minutes\n",
            "L2error = tensor([[0.2655123770]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 138, saving model\n",
            "Iteration number: 139\n",
            "Batch loss: tensor([[0.8219755888]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.368531703948975 seconds\n",
            "Total training time elapsed 29.117698947588604 minutes\n",
            "L2error = tensor([[0.3690342307]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 139, saving model\n",
            "Iteration number: 140\n",
            "Batch loss: tensor([[0.6143710613]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.354830741882324 seconds\n",
            "Total training time elapsed 29.32399555047353 minutes\n",
            "L2error = tensor([[0.2678960860]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 140, saving model\n",
            "Iteration number: 141\n",
            "Batch loss: tensor([[0.6229057312]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.614380598068237 seconds\n",
            "Total training time elapsed 29.535233787695567 minutes\n",
            "L2error = tensor([[0.3041267991]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 141, saving model\n",
            "Iteration number: 142\n",
            "Batch loss: tensor([[0.4594900012]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.446603775024414 seconds\n",
            "Total training time elapsed 29.743700897693635 minutes\n",
            "L2error = tensor([[0.2573769689]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 142, saving model\n",
            "Iteration number: 143\n",
            "Batch loss: tensor([[0.4978973269]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.439669609069824 seconds\n",
            "Total training time elapsed 29.951522294680277 minutes\n",
            "L2error = tensor([[0.2824615836]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 143, saving model\n",
            "Iteration number: 144\n",
            "Batch loss: tensor([[0.4754706323]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.36120319366455 seconds\n",
            "Total training time elapsed 30.15817571878433 minutes\n",
            "L2error = tensor([[0.2344636768]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 144, saving model\n",
            "Iteration number: 145\n",
            "Batch loss: tensor([[0.4834573567]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.318397998809814 seconds\n",
            "Total training time elapsed 30.364274223645527 minutes\n",
            "L2error = tensor([[0.3047776222]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 145, saving model\n",
            "Iteration number: 146\n",
            "Batch loss: tensor([[0.4638311863]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.482560634613037 seconds\n",
            "Total training time elapsed 30.573104321956635 minutes\n",
            "L2error = tensor([[0.2307892889]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 146, saving model\n",
            "Iteration number: 147\n",
            "Batch loss: tensor([[0.4924789667]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.47298002243042 seconds\n",
            "Total training time elapsed 30.78188143571218 minutes\n",
            "L2error = tensor([[0.3082102537]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 147, saving model\n",
            "Iteration number: 148\n",
            "Batch loss: tensor([[0.4355155230]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.452727556228638 seconds\n",
            "Total training time elapsed 30.990051078796387 minutes\n",
            "L2error = tensor([[0.2229151428]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 148, saving model\n",
            "Iteration number: 149\n",
            "Batch loss: tensor([[0.4384875596]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.322077512741089 seconds\n",
            "Total training time elapsed 31.19610361258189 minutes\n",
            "L2error = tensor([[0.2741432190]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 149, saving model\n",
            "Iteration number: 150\n",
            "Batch loss: tensor([[0.4285973310]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.337881088256836 seconds\n",
            "Total training time elapsed 31.402149430910747 minutes\n",
            "L2error = tensor([[0.2434980720]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 150, saving model\n",
            "Iteration number: 151\n",
            "Batch loss: tensor([[0.4671477079]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.355711698532104 seconds\n",
            "Total training time elapsed 31.609082706769307 minutes\n",
            "L2error = tensor([[0.2954585552]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 151, saving model\n",
            "Iteration number: 152\n",
            "Batch loss: tensor([[0.4740311205]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.480615377426147 seconds\n",
            "Total training time elapsed 31.81786162853241 minutes\n",
            "L2error = tensor([[0.2364915162]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 152, saving model\n",
            "Iteration number: 153\n",
            "Batch loss: tensor([[0.5099139214]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.256231784820557 seconds\n",
            "Total training time elapsed 32.02310067017873 minutes\n",
            "L2error = tensor([[0.3088881969]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 153, saving model\n",
            "Iteration number: 154\n",
            "Batch loss: tensor([[0.5217377543]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.30118203163147 seconds\n",
            "Total training time elapsed 32.22858457565307 minutes\n",
            "L2error = tensor([[0.2434165776]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 154, saving model\n",
            "Iteration number: 155\n",
            "Batch loss: tensor([[0.6296617389]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.217179298400879 seconds\n",
            "Total training time elapsed 32.43289699554443 minutes\n",
            "L2error = tensor([[0.3394676745]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 155, saving model\n",
            "Iteration number: 156\n",
            "Batch loss: tensor([[0.6411800385]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.602987051010132 seconds\n",
            "Total training time elapsed 32.64352697134018 minutes\n",
            "L2error = tensor([[0.2652049959]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 156, saving model\n",
            "Iteration number: 157\n",
            "Batch loss: tensor([[0.7286083102]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.676039695739746 seconds\n",
            "Total training time elapsed 32.87390588124593 minutes\n",
            "L2error = tensor([[0.3225669861]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 157, saving model\n",
            "Iteration number: 158\n",
            "Batch loss: tensor([[0.6291016936]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.772072315216064 seconds\n",
            "Total training time elapsed 33.08892060915629 minutes\n",
            "L2error = tensor([[0.2589889467]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 158, saving model\n",
            "Iteration number: 159\n",
            "Batch loss: tensor([[0.6505694985]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.40947413444519 seconds\n",
            "Total training time elapsed 33.29617956876755 minutes\n",
            "L2error = tensor([[0.2841002047]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 159, saving model\n",
            "Iteration number: 160\n",
            "Batch loss: tensor([[0.5423122644]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.362456321716309 seconds\n",
            "Total training time elapsed 33.50276117324829 minutes\n",
            "L2error = tensor([[0.2373464853]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 160, saving model\n",
            "Iteration number: 161\n",
            "Batch loss: tensor([[0.5589122176]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.60793685913086 seconds\n",
            "Total training time elapsed 33.71380488872528 minutes\n",
            "L2error = tensor([[0.3220810294]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 161, saving model\n",
            "Iteration number: 162\n",
            "Batch loss: tensor([[0.6331576705]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.911031723022461 seconds\n",
            "Total training time elapsed 33.9317086259524 minutes\n",
            "L2error = tensor([[0.2471894026]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 162, saving model\n",
            "Iteration number: 163\n",
            "Batch loss: tensor([[0.6609588861]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.511553287506104 seconds\n",
            "Total training time elapsed 34.141091124216715 minutes\n",
            "L2error = tensor([[0.3120012879]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 163, saving model\n",
            "Iteration number: 164\n",
            "Batch loss: tensor([[0.6885573268]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.335535764694214 seconds\n",
            "Total training time elapsed 34.34781920512517 minutes\n",
            "L2error = tensor([[0.2898883522]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 164, saving model\n",
            "Iteration number: 165\n",
            "Batch loss: tensor([[0.7668017745]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.297666072845459 seconds\n",
            "Total training time elapsed 34.553453485171 minutes\n",
            "L2error = tensor([[0.3279633224]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 165, saving model\n",
            "Iteration number: 166\n",
            "Batch loss: tensor([[0.7957815528]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.379476547241211 seconds\n",
            "Total training time elapsed 34.76042106548945 minutes\n",
            "L2error = tensor([[0.2939148843]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 166, saving model\n",
            "Iteration number: 167\n",
            "Batch loss: tensor([[0.9402524829]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.421210289001465 seconds\n",
            "Total training time elapsed 34.96788806120555 minutes\n",
            "L2error = tensor([[0.3561326265]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 167, saving model\n",
            "Iteration number: 168\n",
            "Batch loss: tensor([[0.8676815033]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.284230709075928 seconds\n",
            "Total training time elapsed 35.17306352853775 minutes\n",
            "L2error = tensor([[0.3039002419]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 168, saving model\n",
            "Iteration number: 169\n",
            "Batch loss: tensor([[0.9727190137]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.406721591949463 seconds\n",
            "Total training time elapsed 35.381557083129884 minutes\n",
            "L2error = tensor([[0.3334065378]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 169, saving model\n",
            "Iteration number: 170\n",
            "Batch loss: tensor([[0.7825382352]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.267788887023926 seconds\n",
            "Total training time elapsed 35.586784557501474 minutes\n",
            "L2error = tensor([[0.3166485429]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 170, saving model\n",
            "Iteration number: 171\n",
            "Batch loss: tensor([[0.9156091809]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.493350982666016 seconds\n",
            "Total training time elapsed 35.79545507828394 minutes\n",
            "L2error = tensor([[0.3412069082]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 171, saving model\n",
            "Iteration number: 172\n",
            "Batch loss: tensor([[0.8273963928]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.42601490020752 seconds\n",
            "Total training time elapsed 36.003713337580365 minutes\n",
            "L2error = tensor([[0.2945258021]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 172, saving model\n",
            "Iteration number: 173\n",
            "Batch loss: tensor([[0.8996633291]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.482107639312744 seconds\n",
            "Total training time elapsed 36.2127910455068 minutes\n",
            "L2error = tensor([[0.3331228793]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 173, saving model\n",
            "Iteration number: 174\n",
            "Batch loss: tensor([[0.8383660913]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.402162790298462 seconds\n",
            "Total training time elapsed 36.419870511690775 minutes\n",
            "L2error = tensor([[0.2870255113]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 174, saving model\n",
            "Iteration number: 175\n",
            "Batch loss: tensor([[0.9768492579]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.348310947418213 seconds\n",
            "Total training time elapsed 36.62611054182052 minutes\n",
            "L2error = tensor([[0.3373943865]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 175, saving model\n",
            "Iteration number: 176\n",
            "Batch loss: tensor([[0.7848411798]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.48152494430542 seconds\n",
            "Total training time elapsed 36.83458843628566 minutes\n",
            "L2error = tensor([[0.3046437800]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 176, saving model\n",
            "Iteration number: 177\n",
            "Batch loss: tensor([[0.7356963158]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.59643268585205 seconds\n",
            "Total training time elapsed 37.04558655420939 minutes\n",
            "L2error = tensor([[0.2927636206]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 177, saving model\n",
            "Iteration number: 178\n",
            "Batch loss: tensor([[0.6116266251]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.412339210510254 seconds\n",
            "Total training time elapsed 37.25325697660446 minutes\n",
            "L2error = tensor([[0.2788145840]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 178, saving model\n",
            "Iteration number: 179\n",
            "Batch loss: tensor([[0.5607260466]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.450571298599243 seconds\n",
            "Total training time elapsed 37.46176742315292 minutes\n",
            "L2error = tensor([[0.2508822083]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 179, saving model\n",
            "Iteration number: 180\n",
            "Batch loss: tensor([[0.4730812013]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.686703443527222 seconds\n",
            "Total training time elapsed 37.67451265652974 minutes\n",
            "L2error = tensor([[0.2586610019]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 180, saving model\n",
            "Iteration number: 181\n",
            "Batch loss: tensor([[0.4705956280]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.66630482673645 seconds\n",
            "Total training time elapsed 37.88645087480545 minutes\n",
            "L2error = tensor([[0.2790040374]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 181, saving model\n",
            "Iteration number: 182\n",
            "Batch loss: tensor([[0.4502865672]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.379647254943848 seconds\n",
            "Total training time elapsed 38.11055711905161 minutes\n",
            "L2error = tensor([[0.2456569970]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 182, saving model\n",
            "Iteration number: 183\n",
            "Batch loss: tensor([[0.4106431305]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.540903329849243 seconds\n",
            "Total training time elapsed 38.338352262973785 minutes\n",
            "L2error = tensor([[0.2613295913]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 183, saving model\n",
            "Iteration number: 184\n",
            "Batch loss: tensor([[0.4131675959]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.425002574920654 seconds\n",
            "Total training time elapsed 38.54602253437042 minutes\n",
            "L2error = tensor([[0.2309230715]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 184, saving model\n",
            "Iteration number: 185\n",
            "Batch loss: tensor([[0.4096225798]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.400644302368164 seconds\n",
            "Total training time elapsed 38.75333515803019 minutes\n",
            "L2error = tensor([[0.2673282921]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 185, saving model\n",
            "Iteration number: 186\n",
            "Batch loss: tensor([[0.4308952093]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.352098226547241 seconds\n",
            "Total training time elapsed 38.97667253812154 minutes\n",
            "L2error = tensor([[0.2373972684]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 186, saving model\n",
            "Iteration number: 187\n",
            "Batch loss: tensor([[0.3727041781]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.385438680648804 seconds\n",
            "Total training time elapsed 39.18347314596176 minutes\n",
            "L2error = tensor([[0.2457523793]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 187, saving model\n",
            "Iteration number: 188\n",
            "Batch loss: tensor([[0.3713593185]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.282672643661499 seconds\n",
            "Total training time elapsed 39.388661881287895 minutes\n",
            "L2error = tensor([[0.2226801962]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 188, saving model\n",
            "Iteration number: 189\n",
            "Batch loss: tensor([[0.3792597055]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.284184455871582 seconds\n",
            "Total training time elapsed 39.593720177809395 minutes\n",
            "L2error = tensor([[0.2393573374]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 189, saving model\n",
            "Iteration number: 190\n",
            "Batch loss: tensor([[0.3255608678]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.440354824066162 seconds\n",
            "Total training time elapsed 39.80129044850667 minutes\n",
            "L2error = tensor([[0.2283608615]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 190, saving model\n",
            "Iteration number: 191\n",
            "Batch loss: tensor([[0.3239420354]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.522806644439697 seconds\n",
            "Total training time elapsed 40.01106575330098 minutes\n",
            "L2error = tensor([[0.2541378140]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 191, saving model\n",
            "Iteration number: 192\n",
            "Batch loss: tensor([[0.3392575681]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.401546716690063 seconds\n",
            "Total training time elapsed 40.21831734975179 minutes\n",
            "L2error = tensor([[0.2316338569]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 192, saving model\n",
            "Iteration number: 193\n",
            "Batch loss: tensor([[0.3349374831]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.299600601196289 seconds\n",
            "Total training time elapsed 40.42386314868927 minutes\n",
            "L2error = tensor([[0.2283369154]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 193, saving model\n",
            "Iteration number: 194\n",
            "Batch loss: tensor([[0.3241679966]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.355456829071045 seconds\n",
            "Total training time elapsed 40.63038828372955 minutes\n",
            "L2error = tensor([[0.2277256548]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 194, saving model\n",
            "Iteration number: 195\n",
            "Batch loss: tensor([[0.3149271309]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.503828525543213 seconds\n",
            "Total training time elapsed 40.83929776748021 minutes\n",
            "L2error = tensor([[0.2110020667]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 195, saving model\n",
            "Iteration number: 196\n",
            "Batch loss: tensor([[0.3281346858]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.401779174804688 seconds\n",
            "Total training time elapsed 41.04655923048655 minutes\n",
            "L2error = tensor([[0.2382744700]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 196, saving model\n",
            "Iteration number: 197\n",
            "Batch loss: tensor([[0.3140211403]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.439396858215332 seconds\n",
            "Total training time elapsed 41.254906713962555 minutes\n",
            "L2error = tensor([[0.2226213664]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 197, saving model\n",
            "Iteration number: 198\n",
            "Batch loss: tensor([[0.3167115152]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.361425638198853 seconds\n",
            "Total training time elapsed 41.461340538660686 minutes\n",
            "L2error = tensor([[0.2278209776]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 198, saving model\n",
            "Iteration number: 199\n",
            "Batch loss: tensor([[0.3085361719]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.337748527526855 seconds\n",
            "Total training time elapsed 41.66734087864558 minutes\n",
            "L2error = tensor([[0.2282396406]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 199, saving model\n",
            "Iteration number: 200\n",
            "Batch loss: tensor([[0.3504570723]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.429669857025146 seconds\n",
            "Total training time elapsed 41.87547508478165 minutes\n",
            "L2error = tensor([[0.2288154662]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 200, saving model\n",
            "Plotting and saving convergence history\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f01eb733e48>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}