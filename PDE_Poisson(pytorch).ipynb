{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PDE_Poisson(pytorch).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1kyYd30MifFGQzvjxojYVdiMtg5VqYekK",
      "authorship_tag": "ABX9TyOkk2GpZzjtZlFX3qm6g2Bs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duypham01/PDE_Poisson/blob/master/PDE_Poisson(pytorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJPZCBQUtcYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import grad\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "import itertools\n",
        "import time\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ypQtqOa6odI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PDEInputLayer(nn.Module):\n",
        "    def __init__(self, dim, units):\n",
        "        super(PDEInputLayer, self).__init__()\n",
        "        self.W = nn.Parameter(torch.Tensor(units, dim))\n",
        "        self.b = nn.Parameter(torch.Tensor(units, 1))\n",
        "    def init_weights(self):\n",
        "        for p in self.parameters():\n",
        "            nn.init.xavier_uniform_(p.data)\n",
        "    def forward(self, x):\n",
        "        return torch.tanh(torch.mm(self.W, x) + self.b)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8om1UER6qYO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PDELayer(nn.Module):\n",
        "    def __init__(self, dim, units):\n",
        "        super(PDELayer, self).__init__()\n",
        "        self.units = units\n",
        "        self.Uz = torch.nn.Parameter(torch.Tensor(units, dim))\n",
        "        self.Wz = torch.nn.Parameter(torch.Tensor(units, units))\n",
        "        self.bz = torch.nn.Parameter(torch.Tensor(units, 1))\n",
        "\n",
        "        self.Ug = torch.nn.Parameter(torch.Tensor(units, dim))\n",
        "        self.Wg = torch.nn.Parameter(torch.Tensor(units, units))\n",
        "        self.bg = torch.nn.Parameter(torch.Tensor(units, 1))\n",
        "\n",
        "        self.Ur = torch.nn.Parameter(torch.Tensor(units, dim))\n",
        "        self.Wr = torch.nn.Parameter(torch.Tensor(units, units))\n",
        "        self.br = torch.nn.Parameter(torch.Tensor(units, 1))\n",
        "\n",
        "        self.Uh = torch.nn.Parameter(torch.Tensor(units, dim))\n",
        "        self.Wh = torch.nn.Parameter(torch.Tensor(units, units))\n",
        "        self.bh = torch.nn.Parameter(torch.Tensor(units, 1))\n",
        "\n",
        "    def init_weights(self):\n",
        "        for p in self.parameters():\n",
        "            nn.init.xavier_uniform_(p.data)\n",
        "    def forward(self, input):\n",
        "        S = input[0]\n",
        "        x = input[1]\n",
        "        Ones = torch.ones(self.units, 1)\n",
        "        Z = torch.tanh(torch.mm(self.Uz, x) + torch.mm(self.Wz, S) + self.bz)\n",
        "        G = torch.tanh(torch.mm(self.Ug, x) + torch.mm(self.Wg, S) + self.bg)\n",
        "        R = torch.tanh(torch.mm(self.Ur, x) + torch.mm(self.Wr, S) + self.br)\n",
        "        H = torch.tanh(torch.mm(self.Uh, x) + torch.mm(self.Wh, S * R) + self.bh)\n",
        "\n",
        "        return (Ones - G) * H + S * Z"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXXvb3Ly6tUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PDEOutputLayer(nn.Module):\n",
        "    def __init__(self, dim, units):\n",
        "        super(PDEOutputLayer, self).__init__()\n",
        "        self.W = nn.Parameter(torch.Tensor(1, units))\n",
        "        self.b = nn.Parameter(torch.Tensor(1, 1))\n",
        "    def init_weights(self):\n",
        "        for p in self.parameters():\n",
        "            nn.init.xavier_uniform_(p.data)\n",
        "    def forward(self, x):\n",
        "        return torch.mm(self.W, x) + self.b"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF4TW53Y6vJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PDENet(nn.Module):\n",
        "    def __init__(self, dim, units = 10):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.units = units\n",
        "        self.L1 = PDEInputLayer(dim, units)\n",
        "        self.L2 = PDELayer(dim, units)\n",
        "        self.L3 = PDELayer(dim, units)\n",
        "        self.L4 = PDEOutputLayer(dim, units)\n",
        "    def init_weights(self):\n",
        "        for p in self.parameters():\n",
        "            nn.init.xavier_uniform_(p.data)\n",
        "    def forward(self, x):\n",
        "        S = self.L1(x)\n",
        "        S = self.L2([S, x])\n",
        "        S = self.L3([S, x])\n",
        "        S = self.L4(S)\n",
        "        return S"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAtQGKCMtp-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def laplacian(f, input_vector):\n",
        "    gradient = grad(f, input_vector, create_graph=True)[0]\n",
        "    ux = gradient.take(torch.tensor([0]))\n",
        "    uxx = grad(ux, input_vector, create_graph=True)[0].take(torch.tensor([0]))\n",
        "    uy = gradient.take(torch.tensor([1]))\n",
        "    uyy = grad(uy, input_vector, create_graph=True)[0].take(torch.tensor([1]))\n",
        "    return uxx + uyy\n",
        "\n",
        "\n",
        "def boundary_condition(input):\n",
        "    return 0\n",
        "\n",
        "\n",
        "# -laplace(u) = f\n",
        "def right_hand_side(input):\n",
        "    return 2*(math.pi)**2*math.sin(math.pi*input.take(torch.tensor([0])))*math.sin(math.pi*input.take(torch.tensor([1])))\n",
        "\n",
        "\n",
        "def exact_solution(input):\n",
        "    return math.sin(math.pi*input.take(torch.tensor([0])))*math.sin(math.pi*input.take(torch.tensor([1])))\n",
        "\n",
        "\n",
        "def random_data_points(batch_size):\n",
        "    Omegapoints = []\n",
        "    boundary_points = []\n",
        "    for i in range(batch_size):\n",
        "        Omegapoints.append([random.uniform(0, 1), random.uniform(0, 1)])\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        random_point = [random.uniform(0, 1), random.uniform(0, 1)]\n",
        "        random_index = random.randint(0,1)\n",
        "        random_value = random.randint(0,1)\n",
        "        random_point[random_index] = random_value\n",
        "        boundary_points.append(random_point)\n",
        "    return Omegapoints, boundary_points\n",
        "\n",
        "\n",
        "def batch_loss(net, datapoints):\n",
        "    G1 = G2 = 0\n",
        "    Omegapoints, boundary_points = datapoints\n",
        "    for Omegapoint in Omegapoints:\n",
        "        Omegapoint_input = Variable(torch.Tensor(Omegapoint).resize_(2, 1), requires_grad=True)\n",
        "        Omegapoint_output = net(Omegapoint_input)\n",
        "        G1 += (- laplacian(Omegapoint_output, Omegapoint_input) - right_hand_side(Omegapoint_input)) ** 2\n",
        "\n",
        "    for boundary_point in boundary_points:\n",
        "        boundary_point_input = Variable(torch.Tensor(boundary_point).resize_(2, 1), requires_grad=True)\n",
        "        boundary_point_output = net(boundary_point_input)\n",
        "        G2 += (boundary_point_output - boundary_condition(boundary_point_input))**2\n",
        "\n",
        "    G1 = G1 / len(Omegapoints)\n",
        "    G2 = G2 / len(boundary_points)\n",
        "    return G1 + G2\n",
        "\n",
        "\n",
        "def plot_estimation_and_exact_solution(file_name):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    x = y = np.arange(0, 3, 0.05)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    condition = X ** 2 + Y ** 2 <= 6\n",
        "    zs = np.array([net(torch.tensor([x, y]).resize_(2,1)).item() for x, y in zip(np.ravel(X), np.ravel(Y))])\n",
        "    Z = zs.reshape(X.shape)\n",
        "\n",
        "    zs = np.array([exact_solution_scalar_value(x, y) for x, y in zip(np.ravel(X), np.ravel(Y))])\n",
        "    Z1 = zs.reshape(X.shape)\n",
        "\n",
        "    ax.plot_surface(X, Y, Z, color='red')\n",
        "    ax.plot_surface(X, Y, Z1, color='blue')\n",
        "\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_zlabel('z')\n",
        "\n",
        "    # plt.show()\n",
        "    plt.savefig(file_name)\n",
        "    plt.close(\"all\")\n",
        "\n",
        "\n",
        "def random_points_test():\n",
        "    points = []\n",
        "    for i in range(2000):\n",
        "        points.append([random.uniform(0, 1), random.uniform(0, 1)])\n",
        "    return points"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DuAKxfQtwiB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7b9b6eb3-9167-4128-d9b7-0f5ccef9c7d3"
      },
      "source": [
        "net = PDENet(2, 10)\n",
        "net.init_weights()\n",
        "\n",
        "# plot_estimation_and_exact_solution('./result/testnoBatch/Initial.png')\n",
        "l2_errors = []\n",
        "losses = []\n",
        "\n",
        "# net = torch.load('./model3')\n",
        "torch.set_printoptions(precision=10)\n",
        "learning_rate = 0.001\n",
        "iterations_count = 0\n",
        "start_training = time.time()\n",
        "for i in range(200):\n",
        "    print('Iteration number: ' + str(i + 1))\n",
        "    sample = random_data_points(1000)  # sample space time point\n",
        "    start_ite = time.time()\n",
        "    net.zero_grad()\n",
        "    square_error = batch_loss(net, sample)  # calculate square error loss\n",
        "    square_error.backward()  # calculate gradient of square loss w.r.t the parameters\n",
        "    print('Batch loss: ' + str(square_error))\n",
        "    for param in net.parameters():\n",
        "        param.data -= learning_rate * param.grad.data\n",
        "    losses.append(square_error.item())\n",
        "    end_ite = time.time()\n",
        "    ite_time = end_ite - start_ite\n",
        "    total_time = end_ite - start_training\n",
        "    print('This iteration took ' + str(ite_time) + ' seconds')\n",
        "    print('Total training time elapsed ' + str(total_time/60) + ' minutes')\n",
        "    L2_error = 0\n",
        "    points, boundary_points = random_data_points(1000)\n",
        "    for point in points:\n",
        "        point_input = torch.Tensor(point).resize_(2,1)\n",
        "        L2_error += (net(point_input) - exact_solution(point_input))**2\n",
        "    for boundary in boundary_points:\n",
        "        point_input = torch.Tensor(boundary).resize_(2,1)\n",
        "        L2_error += (net(point_input) - exact_solution(point_input))**2\n",
        "    L2_error = torch.sqrt(L2_error)/2000\n",
        "    l2_errors.append(L2_error.item())\n",
        "\n",
        "    print('L2error = ' + str(L2_error))\n",
        "    # print('Loss function = ' + str(square_error))\n",
        "\n",
        "    print('Finished iteration number ' + str(i + 1) + ', saving model')\n",
        "    model_path = './result/testnoBatch/model' + str(i+1) + 'th_ite'\n",
        "   # net.save(model_path)\n",
        "    # fig_path = './result/testnoBatch/' + str(i+1) + 'th_ite.png'\n",
        "    # plot_estimation_and_exact_solution(fig_path)\n",
        "\n",
        "print('Plotting and saving convergence history')\n",
        "epochs = [i + 1 for i in range(200)]\n",
        "plt.xticks(np.arange(0, 201, 20))\n",
        "\n",
        "plt.figure(0)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(epochs, losses, color = 'blue')\n",
        "# plt.savefig('./result/testnoBatch/loss.png')\n",
        "\n",
        "# with open('./result/testnoBatch/batchloss.txt', 'w+') as f:\n",
        "#     for loss in losses:\n",
        "#         f.write(str(loss) + '\\n')\n",
        "\n",
        "plt.figure(1)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('L2error')\n",
        "\n",
        "plt.plot(epochs, l2_errors, color = 'blue')\n",
        "# plt.savefig('./result/testnoBatch/l2error.png')\n",
        "# with open('./result/testnoBatch/L2error.txt', 'w+') as f:\n",
        "#     for l2_error in l2_errors:\n",
        "#         f.write(str(l2_error) + '\\n')\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration number: 1\n",
            "Batch loss: tensor([[92.1592102051]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.327248096466064 seconds\n",
            "Total training time elapsed 0.18886600732803344 minutes\n",
            "L2error = tensor([[0.0163429510]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 1, saving model\n",
            "Iteration number: 2\n",
            "Batch loss: tensor([[60.7864723206]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.606203556060791 seconds\n",
            "Total training time elapsed 0.39913598299026487 minutes\n",
            "L2error = tensor([[0.0217844564]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 2, saving model\n",
            "Iteration number: 3\n",
            "Batch loss: tensor([[38.3665428162]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.047905921936035 seconds\n",
            "Total training time elapsed 0.6007983843485515 minutes\n",
            "L2error = tensor([[0.0263322666]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 3, saving model\n",
            "Iteration number: 4\n",
            "Batch loss: tensor([[24.5950584412]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.8490309715271 seconds\n",
            "Total training time elapsed 0.7995047132174175 minutes\n",
            "L2error = tensor([[0.0281840824]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 4, saving model\n",
            "Iteration number: 5\n",
            "Batch loss: tensor([[16.4229793549]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.790265560150146 seconds\n",
            "Total training time elapsed 0.9973851203918457 minutes\n",
            "L2error = tensor([[0.0277761631]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 5, saving model\n",
            "Iteration number: 6\n",
            "Batch loss: tensor([[13.6299762726]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.96613097190857 seconds\n",
            "Total training time elapsed 1.1993751287460328 minutes\n",
            "L2error = tensor([[0.0263173785]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 6, saving model\n",
            "Iteration number: 7\n",
            "Batch loss: tensor([[11.8165349960]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.896131992340088 seconds\n",
            "Total training time elapsed 1.3990112622578939 minutes\n",
            "L2error = tensor([[0.0253136102]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 7, saving model\n",
            "Iteration number: 8\n",
            "Batch loss: tensor([[10.1381015778]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.938780546188354 seconds\n",
            "Total training time elapsed 1.5991620659828185 minutes\n",
            "L2error = tensor([[0.0240408219]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 8, saving model\n",
            "Iteration number: 9\n",
            "Batch loss: tensor([[8.6142616272]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.904894590377808 seconds\n",
            "Total training time elapsed 1.7989589015642802 minutes\n",
            "L2error = tensor([[0.0233230777]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 9, saving model\n",
            "Iteration number: 10\n",
            "Batch loss: tensor([[8.2297220230]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.328890085220337 seconds\n",
            "Total training time elapsed 2.00551860332489 minutes\n",
            "L2error = tensor([[0.0209678970]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 10, saving model\n",
            "Iteration number: 11\n",
            "Batch loss: tensor([[7.2459878922]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.369195938110352 seconds\n",
            "Total training time elapsed 2.2302246570587156 minutes\n",
            "L2error = tensor([[0.0206162315]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 11, saving model\n",
            "Iteration number: 12\n",
            "Batch loss: tensor([[6.4190783501]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.400845050811768 seconds\n",
            "Total training time elapsed 2.439671206474304 minutes\n",
            "L2error = tensor([[0.0188145489]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 12, saving model\n",
            "Iteration number: 13\n",
            "Batch loss: tensor([[5.3183746338]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.146980047225952 seconds\n",
            "Total training time elapsed 2.6435462633768716 minutes\n",
            "L2error = tensor([[0.0187757984]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 13, saving model\n",
            "Iteration number: 14\n",
            "Batch loss: tensor([[4.6046714783]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.272048711776733 seconds\n",
            "Total training time elapsed 2.8497400005658466 minutes\n",
            "L2error = tensor([[0.0176182333]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 14, saving model\n",
            "Iteration number: 15\n",
            "Batch loss: tensor([[3.6364254951]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.146227598190308 seconds\n",
            "Total training time elapsed 3.0533928076426187 minutes\n",
            "L2error = tensor([[0.0166492797]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 15, saving model\n",
            "Iteration number: 16\n",
            "Batch loss: tensor([[3.4490575790]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.619652509689331 seconds\n",
            "Total training time elapsed 3.2653538902600605 minutes\n",
            "L2error = tensor([[0.0158051718]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 16, saving model\n",
            "Iteration number: 17\n",
            "Batch loss: tensor([[2.8633735180]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.013349771499634 seconds\n",
            "Total training time elapsed 3.4677523175875344 minutes\n",
            "L2error = tensor([[0.0154785234]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 17, saving model\n",
            "Iteration number: 18\n",
            "Batch loss: tensor([[2.6140468121]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.967196702957153 seconds\n",
            "Total training time elapsed 3.668605343500773 minutes\n",
            "L2error = tensor([[0.0146334376]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 18, saving model\n",
            "Iteration number: 19\n",
            "Batch loss: tensor([[2.2243633270]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.007711172103882 seconds\n",
            "Total training time elapsed 3.870097200075785 minutes\n",
            "L2error = tensor([[0.0147629473]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 19, saving model\n",
            "Iteration number: 20\n",
            "Batch loss: tensor([[2.2260310650]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.875901460647583 seconds\n",
            "Total training time elapsed 4.069258352120717 minutes\n",
            "L2error = tensor([[0.0134026445]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 20, saving model\n",
            "Iteration number: 21\n",
            "Batch loss: tensor([[2.1173195839]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.524449348449707 seconds\n",
            "Total training time elapsed 4.280728296438853 minutes\n",
            "L2error = tensor([[0.0137108723]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 21, saving model\n",
            "Iteration number: 22\n",
            "Batch loss: tensor([[2.1475272179]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.277865648269653 seconds\n",
            "Total training time elapsed 4.4866339842478435 minutes\n",
            "L2error = tensor([[0.0123886457]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 22, saving model\n",
            "Iteration number: 23\n",
            "Batch loss: tensor([[2.1106679440]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.006754636764526 seconds\n",
            "Total training time elapsed 4.6880368947982785 minutes\n",
            "L2error = tensor([[0.0133326054]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 23, saving model\n",
            "Iteration number: 24\n",
            "Batch loss: tensor([[2.0279459953]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.169793128967285 seconds\n",
            "Total training time elapsed 4.892824733257294 minutes\n",
            "L2error = tensor([[0.0116542950]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 24, saving model\n",
            "Iteration number: 25\n",
            "Batch loss: tensor([[2.0362892151]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.028647422790527 seconds\n",
            "Total training time elapsed 5.094760795434316 minutes\n",
            "L2error = tensor([[0.0130393449]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 25, saving model\n",
            "Iteration number: 26\n",
            "Batch loss: tensor([[2.1878595352]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.407253503799438 seconds\n",
            "Total training time elapsed 5.302839684486389 minutes\n",
            "L2error = tensor([[0.0109090740]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 26, saving model\n",
            "Iteration number: 27\n",
            "Batch loss: tensor([[2.0597715378]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.531734466552734 seconds\n",
            "Total training time elapsed 5.513083378473918 minutes\n",
            "L2error = tensor([[0.0125626428]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 27, saving model\n",
            "Iteration number: 28\n",
            "Batch loss: tensor([[2.2757630348]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.142454147338867 seconds\n",
            "Total training time elapsed 5.717551740010579 minutes\n",
            "L2error = tensor([[0.0102356160]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 28, saving model\n",
            "Iteration number: 29\n",
            "Batch loss: tensor([[2.1961998940]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.521626472473145 seconds\n",
            "Total training time elapsed 5.927433133125305 minutes\n",
            "L2error = tensor([[0.0129749980]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 29, saving model\n",
            "Iteration number: 30\n",
            "Batch loss: tensor([[2.5454385281]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.940257549285889 seconds\n",
            "Total training time elapsed 6.144515327612559 minutes\n",
            "L2error = tensor([[0.0097776260]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 30, saving model\n",
            "Iteration number: 31\n",
            "Batch loss: tensor([[2.4208397865]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.539581537246704 seconds\n",
            "Total training time elapsed 6.35595357020696 minutes\n",
            "L2error = tensor([[0.0127260713]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 31, saving model\n",
            "Iteration number: 32\n",
            "Batch loss: tensor([[3.1204090118]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.401063203811646 seconds\n",
            "Total training time elapsed 6.563894041379293 minutes\n",
            "L2error = tensor([[0.0093434304]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 32, saving model\n",
            "Iteration number: 33\n",
            "Batch loss: tensor([[2.4719097614]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.154980421066284 seconds\n",
            "Total training time elapsed 6.7682735045750935 minutes\n",
            "L2error = tensor([[0.0125458678]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 33, saving model\n",
            "Iteration number: 34\n",
            "Batch loss: tensor([[2.7328910828]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.33201813697815 seconds\n",
            "Total training time elapsed 6.975696909427643 minutes\n",
            "L2error = tensor([[0.0093873870]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 34, saving model\n",
            "Iteration number: 35\n",
            "Batch loss: tensor([[1.8834145069]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.56903338432312 seconds\n",
            "Total training time elapsed 7.186479600270589 minutes\n",
            "L2error = tensor([[0.0115118269]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 35, saving model\n",
            "Iteration number: 36\n",
            "Batch loss: tensor([[1.8737683296]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.91489553451538 seconds\n",
            "Total training time elapsed 7.402931066354116 minutes\n",
            "L2error = tensor([[0.0093125934]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 36, saving model\n",
            "Iteration number: 37\n",
            "Batch loss: tensor([[1.4278289080]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.909840106964111 seconds\n",
            "Total training time elapsed 7.620094215869903 minutes\n",
            "L2error = tensor([[0.0102749476]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 37, saving model\n",
            "Iteration number: 38\n",
            "Batch loss: tensor([[1.4831228256]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.089953660964966 seconds\n",
            "Total training time elapsed 7.822876787185669 minutes\n",
            "L2error = tensor([[0.0089597190]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 38, saving model\n",
            "Iteration number: 39\n",
            "Batch loss: tensor([[1.3068914413]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.016804695129395 seconds\n",
            "Total training time elapsed 8.025314243634542 minutes\n",
            "L2error = tensor([[0.0095123239]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 39, saving model\n",
            "Iteration number: 40\n",
            "Batch loss: tensor([[1.3079514503]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.40010929107666 seconds\n",
            "Total training time elapsed 8.233122221628825 minutes\n",
            "L2error = tensor([[0.0086240713]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 40, saving model\n",
            "Iteration number: 41\n",
            "Batch loss: tensor([[1.2326562405]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.229676961898804 seconds\n",
            "Total training time elapsed 8.438410758972168 minutes\n",
            "L2error = tensor([[0.0096304445]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 41, saving model\n",
            "Iteration number: 42\n",
            "Batch loss: tensor([[1.1909184456]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.846386909484863 seconds\n",
            "Total training time elapsed 8.637101825078329 minutes\n",
            "L2error = tensor([[0.0085955728]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 42, saving model\n",
            "Iteration number: 43\n",
            "Batch loss: tensor([[1.2693599463]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.906408309936523 seconds\n",
            "Total training time elapsed 8.83668303489685 minutes\n",
            "L2error = tensor([[0.0088875452]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 43, saving model\n",
            "Iteration number: 44\n",
            "Batch loss: tensor([[1.1180707216]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.858854055404663 seconds\n",
            "Total training time elapsed 9.03546881278356 minutes\n",
            "L2error = tensor([[0.0079272119]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 44, saving model\n",
            "Iteration number: 45\n",
            "Batch loss: tensor([[1.1267231703]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.314069271087646 seconds\n",
            "Total training time elapsed 9.242093424002329 minutes\n",
            "L2error = tensor([[0.0085571883]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 45, saving model\n",
            "Iteration number: 46\n",
            "Batch loss: tensor([[1.0671224594]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.172860145568848 seconds\n",
            "Total training time elapsed 9.446278099219004 minutes\n",
            "L2error = tensor([[0.0078611784]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 46, saving model\n",
            "Iteration number: 47\n",
            "Batch loss: tensor([[0.9657024145]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.21560788154602 seconds\n",
            "Total training time elapsed 9.651525608698527 minutes\n",
            "L2error = tensor([[0.0085128844]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 47, saving model\n",
            "Iteration number: 48\n",
            "Batch loss: tensor([[1.1507244110]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.889296531677246 seconds\n",
            "Total training time elapsed 9.85130688349406 minutes\n",
            "L2error = tensor([[0.0072333096]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 48, saving model\n",
            "Iteration number: 49\n",
            "Batch loss: tensor([[1.0252397060]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.846120119094849 seconds\n",
            "Total training time elapsed 10.049703856309256 minutes\n",
            "L2error = tensor([[0.0083207507]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 49, saving model\n",
            "Iteration number: 50\n",
            "Batch loss: tensor([[1.1173421144]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.659692764282227 seconds\n",
            "Total training time elapsed 10.261682693163554 minutes\n",
            "L2error = tensor([[0.0072052064]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 50, saving model\n",
            "Iteration number: 51\n",
            "Batch loss: tensor([[1.0268259048]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.880117654800415 seconds\n",
            "Total training time elapsed 10.46094475587209 minutes\n",
            "L2error = tensor([[0.0078701228]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 51, saving model\n",
            "Iteration number: 52\n",
            "Batch loss: tensor([[1.0388872623]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.102853775024414 seconds\n",
            "Total training time elapsed 10.666785939534504 minutes\n",
            "L2error = tensor([[0.0066588442]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 52, saving model\n",
            "Iteration number: 53\n",
            "Batch loss: tensor([[1.1434497833]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.883102416992188 seconds\n",
            "Total training time elapsed 10.865908519426982 minutes\n",
            "L2error = tensor([[0.0077985516]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 53, saving model\n",
            "Iteration number: 54\n",
            "Batch loss: tensor([[1.0788315535]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.681844234466553 seconds\n",
            "Total training time elapsed 11.078306849797567 minutes\n",
            "L2error = tensor([[0.0063788802]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 54, saving model\n",
            "Iteration number: 55\n",
            "Batch loss: tensor([[1.1073802710]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.648122787475586 seconds\n",
            "Total training time elapsed 11.29031235774358 minutes\n",
            "L2error = tensor([[0.0077042896]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 55, saving model\n",
            "Iteration number: 56\n",
            "Batch loss: tensor([[1.1382696629]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.92763876914978 seconds\n",
            "Total training time elapsed 11.490273892879486 minutes\n",
            "L2error = tensor([[0.0063346052]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 56, saving model\n",
            "Iteration number: 57\n",
            "Batch loss: tensor([[0.9247409105]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.873597383499146 seconds\n",
            "Total training time elapsed 11.689593780040742 minutes\n",
            "L2error = tensor([[0.0071454030]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 57, saving model\n",
            "Iteration number: 58\n",
            "Batch loss: tensor([[0.8944818974]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.85446572303772 seconds\n",
            "Total training time elapsed 11.88829265832901 minutes\n",
            "L2error = tensor([[0.0063236253]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 58, saving model\n",
            "Iteration number: 59\n",
            "Batch loss: tensor([[0.9094101191]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.747306823730469 seconds\n",
            "Total training time elapsed 12.102270106474558 minutes\n",
            "L2error = tensor([[0.0071618804]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 59, saving model\n",
            "Iteration number: 60\n",
            "Batch loss: tensor([[0.9416815042]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.50692081451416 seconds\n",
            "Total training time elapsed 12.312064480781554 minutes\n",
            "L2error = tensor([[0.0058756117]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 60, saving model\n",
            "Iteration number: 61\n",
            "Batch loss: tensor([[0.9079293609]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.000728130340576 seconds\n",
            "Total training time elapsed 12.51316808462143 minutes\n",
            "L2error = tensor([[0.0068782517]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 61, saving model\n",
            "Iteration number: 62\n",
            "Batch loss: tensor([[0.9233533144]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.327195167541504 seconds\n",
            "Total training time elapsed 12.719849487145742 minutes\n",
            "L2error = tensor([[0.0059087672]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 62, saving model\n",
            "Iteration number: 63\n",
            "Batch loss: tensor([[0.8233909607]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.331871747970581 seconds\n",
            "Total training time elapsed 12.927124547958375 minutes\n",
            "L2error = tensor([[0.0064595831]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 63, saving model\n",
            "Iteration number: 64\n",
            "Batch loss: tensor([[0.7523341775]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.362662315368652 seconds\n",
            "Total training time elapsed 13.136151603857677 minutes\n",
            "L2error = tensor([[0.0057903375]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 64, saving model\n",
            "Iteration number: 65\n",
            "Batch loss: tensor([[0.7545343041]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.96222448348999 seconds\n",
            "Total training time elapsed 13.336781919002533 minutes\n",
            "L2error = tensor([[0.0064441808]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 65, saving model\n",
            "Iteration number: 66\n",
            "Batch loss: tensor([[0.8203800917]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.839982986450195 seconds\n",
            "Total training time elapsed 13.551740094025929 minutes\n",
            "L2error = tensor([[0.0055961600]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 66, saving model\n",
            "Iteration number: 67\n",
            "Batch loss: tensor([[0.7962724566]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.213220357894897 seconds\n",
            "Total training time elapsed 13.757547469933828 minutes\n",
            "L2error = tensor([[0.0061290697]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 67, saving model\n",
            "Iteration number: 68\n",
            "Batch loss: tensor([[0.7509228587]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.353505849838257 seconds\n",
            "Total training time elapsed 13.965227822462717 minutes\n",
            "L2error = tensor([[0.0055539850]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 68, saving model\n",
            "Iteration number: 69\n",
            "Batch loss: tensor([[0.6859000325]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.233117818832397 seconds\n",
            "Total training time elapsed 14.170559934775035 minutes\n",
            "L2error = tensor([[0.0059150290]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 69, saving model\n",
            "Iteration number: 70\n",
            "Batch loss: tensor([[0.7240523696]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.14737057685852 seconds\n",
            "Total training time elapsed 14.37487424214681 minutes\n",
            "L2error = tensor([[0.0053195376]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 70, saving model\n",
            "Iteration number: 71\n",
            "Batch loss: tensor([[0.6260894537]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.295096397399902 seconds\n",
            "Total training time elapsed 14.580997570355732 minutes\n",
            "L2error = tensor([[0.0057615805]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 71, saving model\n",
            "Iteration number: 72\n",
            "Batch loss: tensor([[0.6265665889]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.853358268737793 seconds\n",
            "Total training time elapsed 14.780857384204865 minutes\n",
            "L2error = tensor([[0.0051555089]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 72, saving model\n",
            "Iteration number: 73\n",
            "Batch loss: tensor([[0.6016216874]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.162425756454468 seconds\n",
            "Total training time elapsed 14.984847553571065 minutes\n",
            "L2error = tensor([[0.0054420279]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 73, saving model\n",
            "Iteration number: 74\n",
            "Batch loss: tensor([[0.6417749524]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.90546202659607 seconds\n",
            "Total training time elapsed 15.18499337832133 minutes\n",
            "L2error = tensor([[0.0050414791]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 74, saving model\n",
            "Iteration number: 75\n",
            "Batch loss: tensor([[0.6508879662]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.329556465148926 seconds\n",
            "Total training time elapsed 15.39184608856837 minutes\n",
            "L2error = tensor([[0.0055053825]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 75, saving model\n",
            "Iteration number: 76\n",
            "Batch loss: tensor([[0.6201719642]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.022069454193115 seconds\n",
            "Total training time elapsed 15.593405946095784 minutes\n",
            "L2error = tensor([[0.0048496476]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 76, saving model\n",
            "Iteration number: 77\n",
            "Batch loss: tensor([[0.6364849210]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.099474668502808 seconds\n",
            "Total training time elapsed 15.796407608191172 minutes\n",
            "L2error = tensor([[0.0052161007]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 77, saving model\n",
            "Iteration number: 78\n",
            "Batch loss: tensor([[0.6645833254]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.786850929260254 seconds\n",
            "Total training time elapsed 15.993954185644785 minutes\n",
            "L2error = tensor([[0.0048446981]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 78, saving model\n",
            "Iteration number: 79\n",
            "Batch loss: tensor([[0.6523599625]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.090787649154663 seconds\n",
            "Total training time elapsed 16.196823533376058 minutes\n",
            "L2error = tensor([[0.0052832356]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 79, saving model\n",
            "Iteration number: 80\n",
            "Batch loss: tensor([[0.7017460465]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.121244192123413 seconds\n",
            "Total training time elapsed 16.40134193897247 minutes\n",
            "L2error = tensor([[0.0045926468]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 80, saving model\n",
            "Iteration number: 81\n",
            "Batch loss: tensor([[0.6984921694]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.01140022277832 seconds\n",
            "Total training time elapsed 16.602427649497987 minutes\n",
            "L2error = tensor([[0.0052968939]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 81, saving model\n",
            "Iteration number: 82\n",
            "Batch loss: tensor([[0.7108113170]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.867191314697266 seconds\n",
            "Total training time elapsed 16.80114466349284 minutes\n",
            "L2error = tensor([[0.0045007337]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 82, saving model\n",
            "Iteration number: 83\n",
            "Batch loss: tensor([[0.6683801413]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.92098593711853 seconds\n",
            "Total training time elapsed 17.017605165640514 minutes\n",
            "L2error = tensor([[0.0051875575]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 83, saving model\n",
            "Iteration number: 84\n",
            "Batch loss: tensor([[0.5844079256]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.16724419593811 seconds\n",
            "Total training time elapsed 17.239725625514986 minutes\n",
            "L2error = tensor([[0.0044883960]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 84, saving model\n",
            "Iteration number: 85\n",
            "Batch loss: tensor([[0.5610679388]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.370043516159058 seconds\n",
            "Total training time elapsed 17.465419455369315 minutes\n",
            "L2error = tensor([[0.0048230747]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 85, saving model\n",
            "Iteration number: 86\n",
            "Batch loss: tensor([[0.5362896919]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.99595832824707 seconds\n",
            "Total training time elapsed 17.683533505598703 minutes\n",
            "L2error = tensor([[0.0044788038]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 86, saving model\n",
            "Iteration number: 87\n",
            "Batch loss: tensor([[0.5393092632]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.874120950698853 seconds\n",
            "Total training time elapsed 17.899651936690013 minutes\n",
            "L2error = tensor([[0.0046343599]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 87, saving model\n",
            "Iteration number: 88\n",
            "Batch loss: tensor([[0.4886575341]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.265947341918945 seconds\n",
            "Total training time elapsed 18.122072343031565 minutes\n",
            "L2error = tensor([[0.0043804990]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 88, saving model\n",
            "Iteration number: 89\n",
            "Batch loss: tensor([[0.4751976132]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.284629583358765 seconds\n",
            "Total training time elapsed 18.35008721748988 minutes\n",
            "L2error = tensor([[0.0047046952]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 89, saving model\n",
            "Iteration number: 90\n",
            "Batch loss: tensor([[0.4559498131]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.60071873664856 seconds\n",
            "Total training time elapsed 18.56221842368444 minutes\n",
            "L2error = tensor([[0.0043127709]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 90, saving model\n",
            "Iteration number: 91\n",
            "Batch loss: tensor([[0.4896066189]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.555930852890015 seconds\n",
            "Total training time elapsed 18.77318391799927 minutes\n",
            "L2error = tensor([[0.0045828819]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 91, saving model\n",
            "Iteration number: 92\n",
            "Batch loss: tensor([[0.4513044059]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.409045219421387 seconds\n",
            "Total training time elapsed 18.98170357942581 minutes\n",
            "L2error = tensor([[0.0041930033]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 92, saving model\n",
            "Iteration number: 93\n",
            "Batch loss: tensor([[0.4086989760]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.673892498016357 seconds\n",
            "Total training time elapsed 19.194957423210145 minutes\n",
            "L2error = tensor([[0.0043011522]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 93, saving model\n",
            "Iteration number: 94\n",
            "Batch loss: tensor([[0.4230034351]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.107448101043701 seconds\n",
            "Total training time elapsed 19.398428785800935 minutes\n",
            "L2error = tensor([[0.0042098369]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 94, saving model\n",
            "Iteration number: 95\n",
            "Batch loss: tensor([[0.4215091467]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.700087070465088 seconds\n",
            "Total training time elapsed 19.611374974250793 minutes\n",
            "L2error = tensor([[0.0043105260]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 95, saving model\n",
            "Iteration number: 96\n",
            "Batch loss: tensor([[0.3965056241]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.538678884506226 seconds\n",
            "Total training time elapsed 19.82183068593343 minutes\n",
            "L2error = tensor([[0.0041670366]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 96, saving model\n",
            "Iteration number: 97\n",
            "Batch loss: tensor([[0.4291631877]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.113818645477295 seconds\n",
            "Total training time elapsed 20.025392866134645 minutes\n",
            "L2error = tensor([[0.0042016166]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 97, saving model\n",
            "Iteration number: 98\n",
            "Batch loss: tensor([[0.3941800892]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.440019369125366 seconds\n",
            "Total training time elapsed 20.23500203291575 minutes\n",
            "L2error = tensor([[0.0041155992]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 98, saving model\n",
            "Iteration number: 99\n",
            "Batch loss: tensor([[0.4233280420]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.219625234603882 seconds\n",
            "Total training time elapsed 20.441266945997874 minutes\n",
            "L2error = tensor([[0.0040885438]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 99, saving model\n",
            "Iteration number: 100\n",
            "Batch loss: tensor([[0.4127591550]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.656209945678711 seconds\n",
            "Total training time elapsed 20.653799096743267 minutes\n",
            "L2error = tensor([[0.0039802850]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 100, saving model\n",
            "Iteration number: 101\n",
            "Batch loss: tensor([[0.4189653099]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.595020771026611 seconds\n",
            "Total training time elapsed 20.865433112780252 minutes\n",
            "L2error = tensor([[0.0040796646]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 101, saving model\n",
            "Iteration number: 102\n",
            "Batch loss: tensor([[0.3695314527]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.66935658454895 seconds\n",
            "Total training time elapsed 21.078294265270234 minutes\n",
            "L2error = tensor([[0.0040702657]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 102, saving model\n",
            "Iteration number: 103\n",
            "Batch loss: tensor([[0.4043043256]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.150026559829712 seconds\n",
            "Total training time elapsed 21.298959891001385 minutes\n",
            "L2error = tensor([[0.0039369902]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 103, saving model\n",
            "Iteration number: 104\n",
            "Batch loss: tensor([[0.4189559221]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.89725136756897 seconds\n",
            "Total training time elapsed 21.517998524506886 minutes\n",
            "L2error = tensor([[0.0038810873]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 104, saving model\n",
            "Iteration number: 105\n",
            "Batch loss: tensor([[0.3883711100]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.14643144607544 seconds\n",
            "Total training time elapsed 21.72328590154648 minutes\n",
            "L2error = tensor([[0.0039330604]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 105, saving model\n",
            "Iteration number: 106\n",
            "Batch loss: tensor([[0.3801205754]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.32300877571106 seconds\n",
            "Total training time elapsed 21.930333387851714 minutes\n",
            "L2error = tensor([[0.0038498552]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 106, saving model\n",
            "Iteration number: 107\n",
            "Batch loss: tensor([[0.3929090202]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.220295906066895 seconds\n",
            "Total training time elapsed 22.135781804720562 minutes\n",
            "L2error = tensor([[0.0038150565]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 107, saving model\n",
            "Iteration number: 108\n",
            "Batch loss: tensor([[0.3609083593]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.723324298858643 seconds\n",
            "Total training time elapsed 22.349210540453594 minutes\n",
            "L2error = tensor([[0.0038328064]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 108, saving model\n",
            "Iteration number: 109\n",
            "Batch loss: tensor([[0.3288738132]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.455226182937622 seconds\n",
            "Total training time elapsed 22.559706338246663 minutes\n",
            "L2error = tensor([[0.0037987006]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 109, saving model\n",
            "Iteration number: 110\n",
            "Batch loss: tensor([[0.3534185588]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.940080404281616 seconds\n",
            "Total training time elapsed 22.759930984179178 minutes\n",
            "L2error = tensor([[0.0038869423]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 110, saving model\n",
            "Iteration number: 111\n",
            "Batch loss: tensor([[0.3482782543]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.068324089050293 seconds\n",
            "Total training time elapsed 22.96284095446269 minutes\n",
            "L2error = tensor([[0.0037274945]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 111, saving model\n",
            "Iteration number: 112\n",
            "Batch loss: tensor([[0.3655655384]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.285703659057617 seconds\n",
            "Total training time elapsed 23.168925778071085 minutes\n",
            "L2error = tensor([[0.0037493550]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 112, saving model\n",
            "Iteration number: 113\n",
            "Batch loss: tensor([[0.3540980816]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.41817021369934 seconds\n",
            "Total training time elapsed 23.37811360359192 minutes\n",
            "L2error = tensor([[0.0037598310]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 113, saving model\n",
            "Iteration number: 114\n",
            "Batch loss: tensor([[0.3508871198]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.4071786403656 seconds\n",
            "Total training time elapsed 23.606367909908293 minutes\n",
            "L2error = tensor([[0.0037282575]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 114, saving model\n",
            "Iteration number: 115\n",
            "Batch loss: tensor([[0.3571430743]], grad_fn=<AddBackward0>)\n",
            "This iteration took 13.52701449394226 seconds\n",
            "Total training time elapsed 23.850814525286356 minutes\n",
            "L2error = tensor([[0.0037298878]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 115, saving model\n",
            "Iteration number: 116\n",
            "Batch loss: tensor([[0.3615413606]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.426838636398315 seconds\n",
            "Total training time elapsed 24.05934135913849 minutes\n",
            "L2error = tensor([[0.0035963410]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 116, saving model\n",
            "Iteration number: 117\n",
            "Batch loss: tensor([[0.3571903408]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.74969482421875 seconds\n",
            "Total training time elapsed 24.273235241572063 minutes\n",
            "L2error = tensor([[0.0038183066]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 117, saving model\n",
            "Iteration number: 118\n",
            "Batch loss: tensor([[0.3321757615]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.391058921813965 seconds\n",
            "Total training time elapsed 24.481558565298716 minutes\n",
            "L2error = tensor([[0.0036073686]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 118, saving model\n",
            "Iteration number: 119\n",
            "Batch loss: tensor([[0.3522865772]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.697794675827026 seconds\n",
            "Total training time elapsed 24.694878017902376 minutes\n",
            "L2error = tensor([[0.0036140396]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 119, saving model\n",
            "Iteration number: 120\n",
            "Batch loss: tensor([[0.3329303265]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.137651920318604 seconds\n",
            "Total training time elapsed 24.899042220910392 minutes\n",
            "L2error = tensor([[0.0035639470]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 120, saving model\n",
            "Iteration number: 121\n",
            "Batch loss: tensor([[0.3240228295]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.479416608810425 seconds\n",
            "Total training time elapsed 25.108315340677898 minutes\n",
            "L2error = tensor([[0.0036556066]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 121, saving model\n",
            "Iteration number: 122\n",
            "Batch loss: tensor([[0.3077089190]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.763283252716064 seconds\n",
            "Total training time elapsed 25.32238171497981 minutes\n",
            "L2error = tensor([[0.0035902325]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 122, saving model\n",
            "Iteration number: 123\n",
            "Batch loss: tensor([[0.3080098033]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.084401369094849 seconds\n",
            "Total training time elapsed 25.52485426266988 minutes\n",
            "L2error = tensor([[0.0036621906]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 123, saving model\n",
            "Iteration number: 124\n",
            "Batch loss: tensor([[0.3450399637]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.129525661468506 seconds\n",
            "Total training time elapsed 25.728731679916383 minutes\n",
            "L2error = tensor([[0.0035604581]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 124, saving model\n",
            "Iteration number: 125\n",
            "Batch loss: tensor([[0.3193458915]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.911688804626465 seconds\n",
            "Total training time elapsed 25.92853082815806 minutes\n",
            "L2error = tensor([[0.0035679967]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 125, saving model\n",
            "Iteration number: 126\n",
            "Batch loss: tensor([[0.3160911202]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.053457498550415 seconds\n",
            "Total training time elapsed 26.130472966035207 minutes\n",
            "L2error = tensor([[0.0034118122]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 126, saving model\n",
            "Iteration number: 127\n",
            "Batch loss: tensor([[0.3137863278]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.681303262710571 seconds\n",
            "Total training time elapsed 26.343034080664317 minutes\n",
            "L2error = tensor([[0.0035006108]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 127, saving model\n",
            "Iteration number: 128\n",
            "Batch loss: tensor([[0.2962116897]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.520466089248657 seconds\n",
            "Total training time elapsed 26.553127177556355 minutes\n",
            "L2error = tensor([[0.0035102277]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 128, saving model\n",
            "Iteration number: 129\n",
            "Batch loss: tensor([[0.2826591730]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.00228214263916 seconds\n",
            "Total training time elapsed 26.756899563471475 minutes\n",
            "L2error = tensor([[0.0035017417]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 129, saving model\n",
            "Iteration number: 130\n",
            "Batch loss: tensor([[0.3069656491]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.022958517074585 seconds\n",
            "Total training time elapsed 26.95887270371119 minutes\n",
            "L2error = tensor([[0.0034126798]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 130, saving model\n",
            "Iteration number: 131\n",
            "Batch loss: tensor([[0.2969028950]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.29716420173645 seconds\n",
            "Total training time elapsed 27.165217689673106 minutes\n",
            "L2error = tensor([[0.0034163520]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 131, saving model\n",
            "Iteration number: 132\n",
            "Batch loss: tensor([[0.2706447840]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.099203109741211 seconds\n",
            "Total training time elapsed 27.370235788822175 minutes\n",
            "L2error = tensor([[0.0033810979]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 132, saving model\n",
            "Iteration number: 133\n",
            "Batch loss: tensor([[0.2780935466]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.487371921539307 seconds\n",
            "Total training time elapsed 27.579667615890504 minutes\n",
            "L2error = tensor([[0.0033501938]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 133, saving model\n",
            "Iteration number: 134\n",
            "Batch loss: tensor([[0.2901180983]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.922085762023926 seconds\n",
            "Total training time elapsed 27.780241362253825 minutes\n",
            "L2error = tensor([[0.0033818015]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 134, saving model\n",
            "Iteration number: 135\n",
            "Batch loss: tensor([[0.2566431165]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.96892499923706 seconds\n",
            "Total training time elapsed 27.980834213892617 minutes\n",
            "L2error = tensor([[0.0033968166]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 135, saving model\n",
            "Iteration number: 136\n",
            "Batch loss: tensor([[0.2617462277]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.952998399734497 seconds\n",
            "Total training time elapsed 28.18136918147405 minutes\n",
            "L2error = tensor([[0.0034477997]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 136, saving model\n",
            "Iteration number: 137\n",
            "Batch loss: tensor([[0.2743592858]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.588376998901367 seconds\n",
            "Total training time elapsed 28.392183260122934 minutes\n",
            "L2error = tensor([[0.0032889568]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 137, saving model\n",
            "Iteration number: 138\n",
            "Batch loss: tensor([[0.2753351331]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.932893991470337 seconds\n",
            "Total training time elapsed 28.592090833187104 minutes\n",
            "L2error = tensor([[0.0033961062]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 138, saving model\n",
            "Iteration number: 139\n",
            "Batch loss: tensor([[0.2641766071]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.934214115142822 seconds\n",
            "Total training time elapsed 28.793163712819418 minutes\n",
            "L2error = tensor([[0.0033290386]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 139, saving model\n",
            "Iteration number: 140\n",
            "Batch loss: tensor([[0.2547067404]], grad_fn=<AddBackward0>)\n",
            "This iteration took 13.3873450756073 seconds\n",
            "Total training time elapsed 29.034672244389853 minutes\n",
            "L2error = tensor([[0.0033168742]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 140, saving model\n",
            "Iteration number: 141\n",
            "Batch loss: tensor([[0.2660900652]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.73213005065918 seconds\n",
            "Total training time elapsed 29.248435711860658 minutes\n",
            "L2error = tensor([[0.0033568540]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 141, saving model\n",
            "Iteration number: 142\n",
            "Batch loss: tensor([[0.2560237646]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.268595695495605 seconds\n",
            "Total training time elapsed 29.454818109671276 minutes\n",
            "L2error = tensor([[0.0033174697]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 142, saving model\n",
            "Iteration number: 143\n",
            "Batch loss: tensor([[0.2491319180]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.955970525741577 seconds\n",
            "Total training time elapsed 29.655931333700817 minutes\n",
            "L2error = tensor([[0.0032781838]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 143, saving model\n",
            "Iteration number: 144\n",
            "Batch loss: tensor([[0.2498371005]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.154930353164673 seconds\n",
            "Total training time elapsed 29.85999132792155 minutes\n",
            "L2error = tensor([[0.0032962442]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 144, saving model\n",
            "Iteration number: 145\n",
            "Batch loss: tensor([[0.2487706542]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.963659763336182 seconds\n",
            "Total training time elapsed 30.06040454308192 minutes\n",
            "L2error = tensor([[0.0032429753]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 145, saving model\n",
            "Iteration number: 146\n",
            "Batch loss: tensor([[0.2289413959]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.279868602752686 seconds\n",
            "Total training time elapsed 30.266280420621236 minutes\n",
            "L2error = tensor([[0.0033188404]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 146, saving model\n",
            "Iteration number: 147\n",
            "Batch loss: tensor([[0.2271165699]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.153326034545898 seconds\n",
            "Total training time elapsed 30.469974176088968 minutes\n",
            "L2error = tensor([[0.0032163593]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 147, saving model\n",
            "Iteration number: 148\n",
            "Batch loss: tensor([[0.2602638006]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.260271549224854 seconds\n",
            "Total training time elapsed 30.6759840965271 minutes\n",
            "L2error = tensor([[0.0033107665]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 148, saving model\n",
            "Iteration number: 149\n",
            "Batch loss: tensor([[0.2440166473]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.554996967315674 seconds\n",
            "Total training time elapsed 30.886907406648 minutes\n",
            "L2error = tensor([[0.0032245445]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 149, saving model\n",
            "Iteration number: 150\n",
            "Batch loss: tensor([[0.2438037992]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.12555456161499 seconds\n",
            "Total training time elapsed 31.107531170050304 minutes\n",
            "L2error = tensor([[0.0031878853]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 150, saving model\n",
            "Iteration number: 151\n",
            "Batch loss: tensor([[0.2428737879]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.085968255996704 seconds\n",
            "Total training time elapsed 31.327104910214743 minutes\n",
            "L2error = tensor([[0.0032207654]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 151, saving model\n",
            "Iteration number: 152\n",
            "Batch loss: tensor([[0.2457377017]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.434017896652222 seconds\n",
            "Total training time elapsed 31.53650483687719 minutes\n",
            "L2error = tensor([[0.0032217740]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 152, saving model\n",
            "Iteration number: 153\n",
            "Batch loss: tensor([[0.2414520532]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.171854257583618 seconds\n",
            "Total training time elapsed 31.740866621335346 minutes\n",
            "L2error = tensor([[0.0032952048]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 153, saving model\n",
            "Iteration number: 154\n",
            "Batch loss: tensor([[0.2307612151]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.266534090042114 seconds\n",
            "Total training time elapsed 31.947160522143047 minutes\n",
            "L2error = tensor([[0.0032921960]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 154, saving model\n",
            "Iteration number: 155\n",
            "Batch loss: tensor([[0.2432133257]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.363522052764893 seconds\n",
            "Total training time elapsed 32.155421968301134 minutes\n",
            "L2error = tensor([[0.0032299189]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 155, saving model\n",
            "Iteration number: 156\n",
            "Batch loss: tensor([[0.2452993691]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.423628330230713 seconds\n",
            "Total training time elapsed 32.3644277215004 minutes\n",
            "L2error = tensor([[0.0032683401]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 156, saving model\n",
            "Iteration number: 157\n",
            "Batch loss: tensor([[0.2312990427]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.106328964233398 seconds\n",
            "Total training time elapsed 32.584159755706786 minutes\n",
            "L2error = tensor([[0.0031839409]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 157, saving model\n",
            "Iteration number: 158\n",
            "Batch loss: tensor([[0.2209369242]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.251076936721802 seconds\n",
            "Total training time elapsed 32.79011949300766 minutes\n",
            "L2error = tensor([[0.0031748582]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 158, saving model\n",
            "Iteration number: 159\n",
            "Batch loss: tensor([[0.2246746570]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.337107419967651 seconds\n",
            "Total training time elapsed 32.99778584639231 minutes\n",
            "L2error = tensor([[0.0031711254]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 159, saving model\n",
            "Iteration number: 160\n",
            "Batch loss: tensor([[0.2098592967]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.448319435119629 seconds\n",
            "Total training time elapsed 33.2078795949618 minutes\n",
            "L2error = tensor([[0.0032544418]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 160, saving model\n",
            "Iteration number: 161\n",
            "Batch loss: tensor([[0.2103086114]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.42249321937561 seconds\n",
            "Total training time elapsed 33.416866318384805 minutes\n",
            "L2error = tensor([[0.0030442532]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 161, saving model\n",
            "Iteration number: 162\n",
            "Batch loss: tensor([[0.2249976695]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.503064632415771 seconds\n",
            "Total training time elapsed 33.62663830518723 minutes\n",
            "L2error = tensor([[0.0031346392]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 162, saving model\n",
            "Iteration number: 163\n",
            "Batch loss: tensor([[0.2220345438]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.175254344940186 seconds\n",
            "Total training time elapsed 33.83272206783295 minutes\n",
            "L2error = tensor([[0.0031042460]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 163, saving model\n",
            "Iteration number: 164\n",
            "Batch loss: tensor([[0.2266831100]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.16611623764038 seconds\n",
            "Total training time elapsed 34.037943589687345 minutes\n",
            "L2error = tensor([[0.0031457283]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 164, saving model\n",
            "Iteration number: 165\n",
            "Batch loss: tensor([[0.1879757941]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.807064056396484 seconds\n",
            "Total training time elapsed 34.270422315597536 minutes\n",
            "L2error = tensor([[0.0031921002]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 165, saving model\n",
            "Iteration number: 166\n",
            "Batch loss: tensor([[0.2207400799]], grad_fn=<AddBackward0>)\n",
            "This iteration took 12.252865314483643 seconds\n",
            "Total training time elapsed 34.49457750320435 minutes\n",
            "L2error = tensor([[0.0031879942]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 166, saving model\n",
            "Iteration number: 167\n",
            "Batch loss: tensor([[0.2292156816]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.495123147964478 seconds\n",
            "Total training time elapsed 34.708578117688496 minutes\n",
            "L2error = tensor([[0.0031505837]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 167, saving model\n",
            "Iteration number: 168\n",
            "Batch loss: tensor([[0.2128925174]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.919793844223022 seconds\n",
            "Total training time elapsed 34.90967061122259 minutes\n",
            "L2error = tensor([[0.0030947332]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 168, saving model\n",
            "Iteration number: 169\n",
            "Batch loss: tensor([[0.2241509110]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.509514093399048 seconds\n",
            "Total training time elapsed 35.11923619508743 minutes\n",
            "L2error = tensor([[0.0031352739]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 169, saving model\n",
            "Iteration number: 170\n",
            "Batch loss: tensor([[0.2066792548]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.645774126052856 seconds\n",
            "Total training time elapsed 35.33091343243917 minutes\n",
            "L2error = tensor([[0.0030854573]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 170, saving model\n",
            "Iteration number: 171\n",
            "Batch loss: tensor([[0.2213685960]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.224756956100464 seconds\n",
            "Total training time elapsed 35.53572450876236 minutes\n",
            "L2error = tensor([[0.0031294876]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 171, saving model\n",
            "Iteration number: 172\n",
            "Batch loss: tensor([[0.2052546740]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.328331708908081 seconds\n",
            "Total training time elapsed 35.747225081920625 minutes\n",
            "L2error = tensor([[0.0030681349]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 172, saving model\n",
            "Iteration number: 173\n",
            "Batch loss: tensor([[0.2071272135]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.946593046188354 seconds\n",
            "Total training time elapsed 35.947457285722095 minutes\n",
            "L2error = tensor([[0.0031288820]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 173, saving model\n",
            "Iteration number: 174\n",
            "Batch loss: tensor([[0.1977222115]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.271309614181519 seconds\n",
            "Total training time elapsed 36.15315287907918 minutes\n",
            "L2error = tensor([[0.0031393864]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 174, saving model\n",
            "Iteration number: 175\n",
            "Batch loss: tensor([[0.1983449012]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.396362781524658 seconds\n",
            "Total training time elapsed 36.362236026922865 minutes\n",
            "L2error = tensor([[0.0030914859]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 175, saving model\n",
            "Iteration number: 176\n",
            "Batch loss: tensor([[0.2063486874]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.636220932006836 seconds\n",
            "Total training time elapsed 36.57405553261439 minutes\n",
            "L2error = tensor([[0.0030760639]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 176, saving model\n",
            "Iteration number: 177\n",
            "Batch loss: tensor([[0.1787616909]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.218887090682983 seconds\n",
            "Total training time elapsed 36.779418007532755 minutes\n",
            "L2error = tensor([[0.0030843683]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 177, saving model\n",
            "Iteration number: 178\n",
            "Batch loss: tensor([[0.2043464631]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.155704736709595 seconds\n",
            "Total training time elapsed 36.983616816997525 minutes\n",
            "L2error = tensor([[0.0030975884]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 178, saving model\n",
            "Iteration number: 179\n",
            "Batch loss: tensor([[0.2022387385]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.134243249893188 seconds\n",
            "Total training time elapsed 37.18725173473358 minutes\n",
            "L2error = tensor([[0.0031150875]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 179, saving model\n",
            "Iteration number: 180\n",
            "Batch loss: tensor([[0.1964158267]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.490025520324707 seconds\n",
            "Total training time elapsed 37.39685987234115 minutes\n",
            "L2error = tensor([[0.0030196218]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 180, saving model\n",
            "Iteration number: 181\n",
            "Batch loss: tensor([[0.2079859376]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.318872213363647 seconds\n",
            "Total training time elapsed 37.60346134106318 minutes\n",
            "L2error = tensor([[0.0030384499]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 181, saving model\n",
            "Iteration number: 182\n",
            "Batch loss: tensor([[0.1987304986]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.006620645523071 seconds\n",
            "Total training time elapsed 37.805213828881584 minutes\n",
            "L2error = tensor([[0.0030197981]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 182, saving model\n",
            "Iteration number: 183\n",
            "Batch loss: tensor([[0.1970545053]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.068118333816528 seconds\n",
            "Total training time elapsed 38.00746134519577 minutes\n",
            "L2error = tensor([[0.0030429983]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 183, saving model\n",
            "Iteration number: 184\n",
            "Batch loss: tensor([[0.1764730811]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.33602499961853 seconds\n",
            "Total training time elapsed 38.214471419652305 minutes\n",
            "L2error = tensor([[0.0030944045]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 184, saving model\n",
            "Iteration number: 185\n",
            "Batch loss: tensor([[0.1960545927]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.364670515060425 seconds\n",
            "Total training time elapsed 38.42279260555903 minutes\n",
            "L2error = tensor([[0.0030378189]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 185, saving model\n",
            "Iteration number: 186\n",
            "Batch loss: tensor([[0.1947603673]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.132111549377441 seconds\n",
            "Total training time elapsed 38.62624944448471 minutes\n",
            "L2error = tensor([[0.0030348392]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 186, saving model\n",
            "Iteration number: 187\n",
            "Batch loss: tensor([[0.1904987395]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.001485109329224 seconds\n",
            "Total training time elapsed 38.827412164211275 minutes\n",
            "L2error = tensor([[0.0030039987]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 187, saving model\n",
            "Iteration number: 188\n",
            "Batch loss: tensor([[0.1989833713]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.088541030883789 seconds\n",
            "Total training time elapsed 39.030521428585054 minutes\n",
            "L2error = tensor([[0.0030265718]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 188, saving model\n",
            "Iteration number: 189\n",
            "Batch loss: tensor([[0.1798064411]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.698839664459229 seconds\n",
            "Total training time elapsed 39.245804250240326 minutes\n",
            "L2error = tensor([[0.0030924978]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 189, saving model\n",
            "Iteration number: 190\n",
            "Batch loss: tensor([[0.1898078918]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.40661883354187 seconds\n",
            "Total training time elapsed 39.455218088626864 minutes\n",
            "L2error = tensor([[0.0030310489]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 190, saving model\n",
            "Iteration number: 191\n",
            "Batch loss: tensor([[0.1747027785]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.967301607131958 seconds\n",
            "Total training time elapsed 39.65599264701208 minutes\n",
            "L2error = tensor([[0.0030609060]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 191, saving model\n",
            "Iteration number: 192\n",
            "Batch loss: tensor([[0.1853986382]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.778896570205688 seconds\n",
            "Total training time elapsed 39.8707923690478 minutes\n",
            "L2error = tensor([[0.0029782003]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 192, saving model\n",
            "Iteration number: 193\n",
            "Batch loss: tensor([[0.1869911104]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.753764152526855 seconds\n",
            "Total training time elapsed 40.08576339880626 minutes\n",
            "L2error = tensor([[0.0030203757]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 193, saving model\n",
            "Iteration number: 194\n",
            "Batch loss: tensor([[0.1947056055]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.614419221878052 seconds\n",
            "Total training time elapsed 40.2971807718277 minutes\n",
            "L2error = tensor([[0.0029616689]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 194, saving model\n",
            "Iteration number: 195\n",
            "Batch loss: tensor([[0.1984830350]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.025638103485107 seconds\n",
            "Total training time elapsed 40.49877571662267 minutes\n",
            "L2error = tensor([[0.0029900526]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 195, saving model\n",
            "Iteration number: 196\n",
            "Batch loss: tensor([[0.1929699481]], grad_fn=<AddBackward0>)\n",
            "This iteration took 10.983782291412354 seconds\n",
            "Total training time elapsed 40.699970233440396 minutes\n",
            "L2error = tensor([[0.0029400794]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 196, saving model\n",
            "Iteration number: 197\n",
            "Batch loss: tensor([[0.1902341098]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.052577018737793 seconds\n",
            "Total training time elapsed 40.903427040576936 minutes\n",
            "L2error = tensor([[0.0030396474]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 197, saving model\n",
            "Iteration number: 198\n",
            "Batch loss: tensor([[0.1778705120]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.704300880432129 seconds\n",
            "Total training time elapsed 41.11747092008591 minutes\n",
            "L2error = tensor([[0.0029279124]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 198, saving model\n",
            "Iteration number: 199\n",
            "Batch loss: tensor([[0.2011357695]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.33220386505127 seconds\n",
            "Total training time elapsed 41.324305597941084 minutes\n",
            "L2error = tensor([[0.0030150963]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 199, saving model\n",
            "Iteration number: 200\n",
            "Batch loss: tensor([[0.1885473579]], grad_fn=<AddBackward0>)\n",
            "This iteration took 11.58707308769226 seconds\n",
            "Total training time elapsed 41.53515648444493 minutes\n",
            "L2error = tensor([[0.0029962673]], grad_fn=<DivBackward0>)\n",
            "Finished iteration number 200, saving model\n",
            "Plotting and saving convergence history\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff2a1500a20>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4QNZkBCV_PE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(net.state_dict(), 'PDEnet_parameters.pt')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdi7OUegWJwf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c5d4705-2789-4dec-ec51-adbc79c69602"
      },
      "source": [
        "model = PDENet(2, 10)\n",
        "model.load_state_dict(torch.load('PDEnet_parameters.pt'))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0s8K6E-l8Dh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5c6b552a-ea32-4fa6-ed7c-f10bb9288ad3"
      },
      "source": [
        "L2_error = 0\n",
        "points, boundary_points = random_data_points(1000)\n",
        "for point in points:\n",
        "    point_input = torch.Tensor(point).resize_(2,1)\n",
        "    L2_error += (model(point_input) - exact_solution(point_input))**2\n",
        "for boundary in boundary_points:\n",
        "    point_input = torch.Tensor(boundary).resize_(2,1)\n",
        "    L2_error += (net(point_input) - exact_solution(point_input))**2\n",
        "L2_error = torch.sqrt(L2_error)/2000\n",
        "l2_errors.append(L2_error.item())\n",
        "\n",
        "print('L2error = ' + str(L2_error))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L2error = tensor([[0.0028988209]], grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}